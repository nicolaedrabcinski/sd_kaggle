import pandas as pd
import numpy as np
from pathlib import Path
import torch

# –í–∞—à–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
results = {
    'Model': ['nhits', 'fedformer', 'residual_mlp', 'dlinear', 'xgboost_nn', 
              'xgboost_nn', 'dlinear', 'fedformer', 'cnn_attention', 'patchtst_ci',
              'residual_mlp', 'autoformer', 'tabnet', 'autoformer', 'cnn_attention',
              'tabnet', 'nbeats', 'nbeats', 'nlinear', 'nlinear', 'patchtst', 'nhits', 'crossformer'],
    'RMSE': [0.029737, 0.029766, 0.029853, 0.041078, 0.041102, 0.030130, 0.030662, 
             0.042128, 0.031004, 0.042617, 0.042923, 0.049284, 0.038720, 0.039365, 
             0.062726, 0.078821, 0.111354, 0.429702, 0.404733, 1.035654, 2.219481, 2.921042, 4.434728],
    'R¬≤': [-0.0024, -0.0044, -0.0103, -0.0232, -0.0244, -0.0291, -0.0658, -0.0762, 
           -0.0897, -0.1014, -0.1172, -0.4729, -0.6996, -0.7566, -1.3859, -2.7674, 
           -13.0566, -110.9685, -184.6959, -649.4147, -2986.1936, -5173.1187, -11924.9951],
    'Dir_Acc': [0.5078, 0.5007, 0.5065, 0.5115, 0.5127, 0.4982, 0.5001, 0.4971, 
                0.5001, 0.5053, 0.5076, 0.5074, 0.5029, 0.4998, 0.5014, 0.5002, 
                0.5012, 0.4971, 0.5000, 0.4990, 0.5071, 0.5023, 0.5075],
    'Params': [65278847, 2685448, 55478088, 181513552, 56977818, 34594778, 494757120, 
               1666536, 20630280, 143851560, 108742824, 749032, 76122056, 1767944, 
               6889960, 150754988, 328942113, 649162377, 247378560, 90756776, 5924776, 
               115662821, 6055336],
    'Time_m': [1.1, 0.7, 1.1, 3.7, 0.5, 1.1, 32.6, 0.3, 0.9, 29.5, 1.1, 0.2, 1.1, 
               0.5, 0.2, 1.0, 2.4, 2.0, 18.2, 0.6, 0.2, 0.7, 0.1]
}

df_results = pd.DataFrame(results)

# –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, –æ—Å—Ç–∞–≤–ª—è—è –ª—É—á—à—É—é –≤–µ—Ä—Å–∏—é –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
df_best_models = df_results.loc[df_results.groupby('Model')['Dir_Acc'].idxmax()]

# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ Dir Acc (–æ—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)
df_best_models = df_best_models.sort_values('Dir_Acc', ascending=False)

print("üéØ –õ–£–ß–®–ò–ï –ú–û–î–ï–õ–ò –ü–û DIR ACC:")
print(df_best_models[['Model', 'Dir_Acc', 'RMSE', 'R¬≤', 'Time_m']].head(10))

# –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-5 –º–æ–¥–µ–ª–µ–π
top_models = df_best_models.head(5)['Model'].tolist()
print(f"\nüöÄ –¢–û–ü-5 –ú–û–î–ï–õ–ï–ô –î–õ–Ø SUBMISSION: {top_models}")

import torch
import pandas as pd
import numpy as np
from pathlib import Path
import importlib
import sys

class RealModelLoader:
    def __init__(self, models_base_path, src_path):
        self.models_base_path = Path(models_base_path)
        self.src_path = Path(src_path)
        self.models = {}
        
        # –î–æ–±–∞–≤–ª—è–µ–º src –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
        sys.path.append(str(self.src_path))
    
    def discover_models(self):
        """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏"""
        model_folders = [f for f in self.models_base_path.iterdir() if f.is_dir()]
        available_models = []
        
        for folder in model_folders:
            model_file = folder / "best_model.pth"
            if model_file.exists():
                available_models.append(folder.name)
                print(f"‚úÖ {folder.name}: best_model.pth –Ω–∞–π–¥–µ–Ω")
            else:
                print(f"‚ùå {folder.name}: best_model.pth –Ω–µ –Ω–∞–π–¥–µ–Ω")
                
        return available_models
    
    def load_model(self, model_name):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π"""
        model_path = self.models_base_path / model_name / "best_model.pth"
        
        if not model_path.exists():
            print(f"‚ùå –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return None
            
        try:
            print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º {model_name}...")
            
            # –ü—Ä–æ–±—É–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
            model_module = self._import_model_architecture(model_name)
            if model_module is None:
                print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –º–æ–¥—É–ª—å –¥–ª—è {model_name}")
                return None
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç
            checkpoint = torch.load(model_path, map_location='cpu')
            
            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
            model = self._create_model_from_checkpoint(model_name, checkpoint, model_module)
            
            if model is not None:
                self.models[model_name] = model
                print(f"‚úÖ {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                return model
            else:
                print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –º–æ–¥–µ–ª—å {model_name}")
                return None
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {model_name}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _import_model_architecture(self, model_name):
        """–ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –º–æ–¥—É–ª—å —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            if model_name == 'nhits':
                from src.models.nhits import NHiTS
                return NHiTS
            elif model_name == 'dlinear':
                from src.models.dlinear import DLinear
                return DLinear
            elif model_name == 'patchtst':
                from src.models.patchtst import PatchTST
                return PatchTST
            elif model_name == 'cnn_attention':
                from src.models.cnn_attention import CNNAttention
                return CNNAttention
            elif model_name == 'tabnet':
                from src.models.tabnet import TabNetModel
                return TabNetModel
            elif model_name == 'timesnet':
                from src.models.timesnet import TimesNet
                return TimesNet
            elif model_name == 'residual_mlp':
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ residual_mlp –≤ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö
                residual_mlp_path = self.src_path / 'models' / 'residual_mlp.py'
                if residual_mlp_path.exists():
                    spec = importlib.util.spec_from_file_location("residual_mlp", residual_mlp_path)
                    module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(module)
                    return getattr(module, 'ResidualMLP', None)
            else:
                print(f"‚ö†Ô∏è  –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–±—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥")
                return None
                
        except ImportError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ {model_name}: {e}")
            return None
    
    def _create_model_from_checkpoint(self, model_name, checkpoint, model_class):
        """–°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞"""
        try:
            if isinstance(checkpoint, dict):
                if 'hyper_parameters' in checkpoint:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
                    hparams = checkpoint['hyper_parameters']
                    
                    if model_class:
                        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å —Ç–µ–º–∏ –∂–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                        if 'seq_len' in hparams and 'pred_len' in hparams:
                            model = model_class(
                                seq_len=hparams['seq_len'],
                                pred_len=hparams['pred_len'],
                                **{k: v for k, v in hparams.items() if k not in ['seq_len', 'pred_len']}
                            )
                        else:
                            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                            model = model_class()
                    else:
                        # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –∫–ª–∞—Å—Å –Ω–µ –Ω–∞–π–¥–µ–Ω
                        model = self._create_universal_model(checkpoint)
                else:
                    model = self._create_universal_model(checkpoint)
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º state_dict
                if 'state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['state_dict'])
                    
            elif hasattr(checkpoint, 'state_dict'):
                # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ –º–æ–¥–µ–ª—å
                model = checkpoint
            else:
                model = self._create_universal_model(checkpoint)
            
            model.eval()
            return model
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return None
    
    def _create_universal_model(self, checkpoint):
        """–°–æ–∑–¥–∞–µ—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –∫–æ–≥–¥–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞"""
        class UniversalModel(torch.nn.Module):
            def __init__(self, input_size=100, output_size=424, hidden_layers=[512, 256]):
                super().__init__()
                layers = []
                prev_size = input_size
                
                for hidden_size in hidden_layers:
                    layers.append(torch.nn.Linear(prev_size, hidden_size))
                    layers.append(torch.nn.ReLU())
                    layers.append(torch.nn.Dropout(0.1))
                    prev_size = hidden_size
                
                layers.append(torch.nn.Linear(prev_size, output_size))
                self.network = torch.nn.Sequential(*layers)
            
            def forward(self, x):
                return self.network(x)
        
        # –ü—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–º–µ—Ä—ã –∏–∑ state_dict
        if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
            for key, param in state_dict.items():
                if 'weight' in key and len(param.shape) == 2:
                    if param.shape[0] == 424:  # –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
                        input_size = param.shape[1]
                        return UniversalModel(input_size=input_size, output_size=424)
        
        return UniversalModel()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞
loader = RealModelLoader(
    models_base_path="/home/nicolaedrabcinski/sd_kaggle/models/checkpoints",
    src_path="/home/nicolaedrabcinski/sd_kaggle/src"
)

# –ù–∞—Ö–æ–¥–∏–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
available_models = loader.discover_models()
print(f"\nüéØ –î–æ—Å—Ç—É–ø–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(available_models)}")
print(f"üìã –°–ø–∏—Å–æ–∫: {available_models}")

import warnings
warnings.filterwarnings("ignore")

class RealSubmissionGenerator:
    def __init__(self, data_path="/home/nicolaedrabcinski/sd_kaggle/data"):
        self.data_path = Path(data_path)
        self.target_columns = [f'target_{i}' for i in range(424)]
        
    def load_real_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤–∫–ª—é—á–∞—è test.csv"""
        print("üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...")
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        self.train_df = pd.read_csv(self.data_path / "raw" / "train.csv")
        self.train_labels = pd.read_csv(self.data_path / "raw" / "train_labels.csv")
        self.target_pairs = pd.read_csv(self.data_path / "raw" / "target_pairs.csv")
        
        # –ó–ê–ì–†–£–ñ–ê–ï–ú –†–ï–ê–õ–¨–ù–´–ô TEST.CSV
        test_csv_path = self.data_path / "raw" / "test.csv"
        if test_csv_path.exists():
            self.test_df = pd.read_csv(test_csv_path)
            print(f"‚úÖ test.csv –∑–∞–≥—Ä—É–∂–µ–Ω: {self.test_df.shape}")
        else:
            print("‚ùå test.csv –Ω–µ –Ω–∞–π–¥–µ–Ω! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å:")
            print(f"   –ò—Å–∫–æ–º—ã–π –ø—É—Ç—å: {test_csv_path}")
            # –ü–æ–∫–∞–∂–µ–º —á—Ç–æ –µ—Å—Ç—å –≤ –ø–∞–ø–∫–µ raw
            raw_files = list((self.data_path / "raw").glob("*"))
            print("   –§–∞–π–ª—ã –≤ raw/:")
            for f in raw_files:
                print(f"     - {f.name}")
            raise FileNotFoundError(f"test.csv –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {test_csv_path}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º enhanced –¥–∞–Ω–Ω—ã–µ
        enhanced_test_path = self.data_path / "processed" / "test_enhanced.csv"
        if enhanced_test_path.exists():
            self.enhanced_test = pd.read_csv(enhanced_test_path)
            print("‚úÖ Enhanced test –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        else:
            print("‚ö†Ô∏è Enhanced test –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π test.csv")
            self.enhanced_test = None
        
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: train={self.train_df.shape}, test={self.test_df.shape}, labels={self.train_labels.shape}")
        
    def get_test_data(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç enhanced –≤–µ—Ä—Å–∏–∏)"""
        if self.enhanced_test is not None and not self.enhanced_test.empty:
            print("üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º enhanced test –¥–∞–Ω–Ω—ã–µ")
            return self.enhanced_test.copy()
        else:
            print("üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π test.csv")
            return self.test_df.copy()
    
    def prepare_real_features(self, data):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ñ–∏—á–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞—à–µ–π —Ä–µ–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ —Ñ–∏—á–∏, —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
        feature_columns = []
        
        # –ë–∞–∑–æ–≤—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–∏—Å–∫–ª—é—á–∞—è date_id –∏ targets)
        numeric_cols = [col for col in data.columns 
                       if col not in ['date_id'] + self.target_columns 
                       and pd.api.types.is_numeric_dtype(data[col])]
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å enhanced —Ñ–∏—á–∏ –≤ train, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –∫–∞–∫ –æ—Ä–∏–µ–Ω—Ç–∏—Ä
        enhanced_train_path = self.data_path / "processed" / "train_enhanced.csv"
        if enhanced_train_path.exists():
            enhanced_train = pd.read_csv(enhanced_train_path)
            enhanced_numeric = [col for col in enhanced_train.columns 
                              if col not in ['date_id'] + self.target_columns 
                              and pd.api.types.is_numeric_dtype(enhanced_train[col])]
            
            # –í—ã–±–∏—Ä–∞–µ–º –æ–±—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –º–µ–∂–¥—É test –∏ enhanced train
            common_cols = list(set(numeric_cols) & set(enhanced_numeric))
            if common_cols:
                feature_columns = common_cols
                print(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º {len(feature_columns)} enhanced —Ñ–∏—á–µ–π")
            else:
                feature_columns = numeric_cols
                print(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º {len(feature_columns)} –±–∞–∑–æ–≤—ã—Ö —Ñ–∏—á–µ–π")
        else:
            feature_columns = numeric_cols
            print(f"üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º {len(feature_columns)} –±–∞–∑–æ–≤—ã—Ö —Ñ–∏—á–µ–π –∏–∑ test.csv")
        
        return feature_columns
    
    def generate_submission_for_model(self, model, model_name, test_data):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç submission –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            feature_columns = self.prepare_real_features(test_data)
            
            if not feature_columns:
                print(f"‚ùå –ù–µ—Ç —Ñ–∏—á–µ–π –¥–ª—è –º–æ–¥–µ–ª–∏ {model_name}")
                print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ test –¥–∞–Ω–Ω—ã—Ö: {list(test_data.columns)}")
                return None
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            features = test_data[feature_columns].copy()
            features = features.fillna(0)  # –ë–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ NaN
            
            print(f"üî¢ –†–∞–∑–º–µ—Ä —Ñ–∏—á–µ–π –¥–ª—è {model_name}: {features.shape}")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ tensor –∏ –¥–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            with torch.no_grad():
                input_tensor = torch.FloatTensor(features.values)
                print(f"üéØ –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è {len(input_tensor)} samples...")
                predictions = model(input_tensor).numpy()
            
            # –°–æ–∑–¥–∞–µ–º submission DataFrame
            submission_df = pd.DataFrame(predictions, columns=self.target_columns)
            
            # –î–æ–±–∞–≤–ª—è–µ–º date_id –∏–∑ test –¥–∞–Ω–Ω—ã—Ö
            if 'date_id' in test_data.columns:
                submission_df['date_id'] = test_data['date_id'].values
                print(f"üìÖ –î–æ–±–∞–≤–ª–µ–Ω—ã date_id –¥–ª—è {len(submission_df)} —Å—Ç—Ä–æ–∫")
            
            print(f"‚úÖ {model_name}: —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(predictions)} –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π, shape={predictions.shape}")
            return submission_df
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è {model_name}: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def save_submission(self, submission_df, model_name, output_dir="/home/nicolaedrabcinski/sd_kaggle/submissions"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç submission —Ñ–∞–π–ª"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ parquet (—Ñ–æ—Ä–º–∞—Ç –¥–ª—è Kaggle)
        output_path = output_dir / f"submission_{model_name}.parquet"
        submission_df.to_parquet(output_path, index=False)
        
        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        csv_path = output_dir / f"submission_{model_name}.csv"
        submission_df.to_csv(csv_path, index=False)
        
        print(f"üíæ Submission —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
        print(f"   –†–∞–∑–º–µ—Ä: {submission_df.shape}")
        print(f"   –ö–æ–ª–æ–Ω–∫–∏: {list(submission_df.columns[:3])}...")  # –ü–æ–∫–∞–∂–µ–º –ø–µ—Ä–≤—ã–µ 3 –∫–æ–ª–æ–Ω–∫–∏
        
        return output_path

# –û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
def generate_all_submissions_with_real_test():
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç submission —Ñ–∞–π–ª—ã –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É—è —Ä–µ–∞–ª—å–Ω—ã–π test.csv"""
    
    # –õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ –≤–∞—à–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
    top_models = ['xgboost_nn', 'dlinear', 'nhits', 'residual_mlp', 'patchtst_ci']
    
    print("üöÄ –ó–ê–ü–£–°–ö –ì–ï–ù–ï–†–ê–¶–ò–ò SUBMISSION –§–ê–ô–õ–û–í –° REAL TEST.CSV")
    print(f"üéØ –¶–µ–ª–µ–≤—ã–µ –º–æ–¥–µ–ª–∏: {top_models}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    loader = RealModelLoader(
        models_base_path="/home/nicolaedrabcinski/sd_kaggle/models/checkpoints",
        src_path="/home/nicolaedrabcinski/sd_kaggle/src"
    )
    
    generator = RealSubmissionGenerator()
    
    try:
        generator.load_real_data()  # –¢–µ–ø–µ—Ä—å –∑–∞–≥—Ä—É–∑–∏—Ç —Ä–µ–∞–ª—å–Ω—ã–π test.csv
    except FileNotFoundError as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return []
    
    # –ò–°–ü–û–õ–¨–ó–£–ï–ú –†–ï–ê–õ–¨–ù–´–ï –¢–ï–°–¢–û–í–´–ï –î–ê–ù–ù–´–ï
    test_data = generator.get_test_data()
    print(f"üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: {test_data.shape}")
    print(f"üìä –ü–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏ test –¥–∞–Ω–Ω—ã—Ö:")
    print(test_data.head(3))
    
    successful_submissions = []
    
    for model_name in top_models:
        print(f"\n{'='*60}")
        print(f"üéØ –û–ë–†–ê–ë–û–¢–ö–ê: {model_name}")
        print(f"{'='*60}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        model = loader.load_model(model_name)
        if model is None:
            continue
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º submission –ù–ê –†–ï–ê–õ–¨–ù–´–• –¢–ï–°–¢–û–í–´–• –î–ê–ù–ù–´–•
        submission_df = generator.generate_submission_for_model(model, model_name, test_data)
        if submission_df is None:
            continue
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        output_path = generator.save_submission(submission_df, model_name)
        successful_submissions.append((model_name, output_path))
        
        print(f"‚úÖ {model_name} - –ó–ê–í–ï–†–®–ï–ù–û")
    
    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    print(f"\n{'='*60}")
    print("üéâ –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
    print(f"{'='*60}")
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {len(successful_submissions)}/{len(top_models)}")
    
    for model_name, path in successful_submissions:
        print(f"   üìÅ {model_name}: {path}")
    
    return successful_submissions

# –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
print("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ test.csv...")
test_path = Path("/home/nicolaedrabcinski/sd_kaggle/data/raw/test.csv")
if test_path.exists():
    print(f"‚úÖ test.csv –Ω–∞–π–¥–µ–Ω: {test_path}")
    submissions = generate_all_submissions_with_real_test()
    
    print(f"\nüìã –î–ê–õ–¨–ù–ï–ô–®–ò–ï –®–ê–ì–ò:")
    print("1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ /home/nicolaedrabcinski/sd_kaggle/submissions/")
    print("2. –î–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –Ω–∞ Kaggle –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—ã:")
    for model_name, path in submissions:
        print(f"   kaggle competitions submit -c mitsui-commodity-prediction-challenge -f {path} -m 'Submission with {model_name} - Dir Acc –∏–∑ –≤–∞—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤'")
    print("3. –°—Ä–∞–≤–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –ª–∏–¥–µ—Ä–±–æ—Ä–¥–µ!")
else:
    print(f"‚ùå test.csv –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {test_path}")
    
