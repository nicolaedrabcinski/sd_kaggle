{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "improved_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\n",
      "–î–æ—Å—Ç—É–ø–Ω–æ GPU: 1\n",
      "–ù–∞–∑–≤–∞–Ω–∏–µ GPU: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import warnings\n",
    "import gc\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.stats import spearmanr\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
    "print(f\"–î–æ—Å—Ç—É–ø–Ω–æ GPU: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"–ù–∞–∑–≤–∞–Ω–∏–µ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "NUM_TARGET_COLUMNS = 424\n",
    "\n",
    "# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ\n",
    "models = []\n",
    "scaler = None\n",
    "feature_selector = None\n",
    "feature_cols = None\n",
    "base_cols = None\n",
    "is_initialized = False\n",
    "model_val_scores = []\n",
    "ensemble_weights_global = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "improved_losses",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PearsonCorrelationLoss(nn.Module):\n",
    "    \"\"\"Loss —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ Pearson Correlation\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ target –æ—Ç–¥–µ–ª—å–Ω–æ\n",
    "        batch_size = pred.shape[0]\n",
    "        num_targets = pred.shape[1]\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        for i in range(num_targets):\n",
    "            pred_col = pred[:, i]\n",
    "            target_col = target[:, i]\n",
    "            \n",
    "            # –¶–µ–Ω—Ç—Ä–∏—Ä—É–µ–º\n",
    "            pred_centered = pred_col - pred_col.mean()\n",
    "            target_centered = target_col - target_col.mean()\n",
    "            \n",
    "            # –ö–æ–≤–∞—Ä–∏–∞—Ü–∏—è\n",
    "            covariance = (pred_centered * target_centered).mean()\n",
    "            \n",
    "            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è\n",
    "            pred_std = pred_centered.std()\n",
    "            target_std = target_centered.std()\n",
    "            \n",
    "            # Pearson correlation\n",
    "            if pred_std > 1e-8 and target_std > 1e-8:\n",
    "                correlation = covariance / (pred_std * target_std)\n",
    "                # –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é\n",
    "                total_loss += -correlation\n",
    "            else:\n",
    "                # –®—Ç—Ä–∞—Ñ –∑–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "                total_loss += 1.0\n",
    "        \n",
    "        return total_loss / num_targets\n",
    "\n",
    "class SpearmanLoss(nn.Module):\n",
    "    \"\"\"Approximation of Spearman correlation loss\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É\n",
    "        pred_rank = torch.argsort(torch.argsort(pred, dim=0), dim=0).float()\n",
    "        target_rank = torch.argsort(torch.argsort(target, dim=0), dim=0).float()\n",
    "        \n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–∞–Ω–≥–∏\n",
    "        pred_rank = (pred_rank - pred_rank.mean(dim=0)) / (pred_rank.std(dim=0) + 1e-8)\n",
    "        target_rank = (target_rank - target_rank.mean(dim=0)) / (target_rank.std(dim=0) + 1e-8)\n",
    "        \n",
    "        # Correlation (–º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—É—é)\n",
    "        correlation = (pred_rank * target_rank).mean(dim=0).mean()\n",
    "        return -correlation\n",
    "\n",
    "class CombinedCorrelationLoss(nn.Module):\n",
    "    \"\"\"–ö–æ–º–±–∏–Ω–∞—Ü–∏—è Pearson + Spearman + MSE\"\"\"\n",
    "    def __init__(self, pearson_weight=0.4, spearman_weight=0.4, mse_weight=0.2):\n",
    "        super().__init__()\n",
    "        self.pearson_weight = pearson_weight\n",
    "        self.spearman_weight = spearman_weight\n",
    "        self.mse_weight = mse_weight\n",
    "        \n",
    "        self.pearson_loss = PearsonCorrelationLoss()\n",
    "        self.spearman_loss = SpearmanLoss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pearson_l = self.pearson_loss(pred, target)\n",
    "        spearman_l = self.spearman_loss(pred, target)\n",
    "        mse_l = self.mse_loss(pred, target)\n",
    "        \n",
    "        total_loss = (self.pearson_weight * pearson_l + \n",
    "                     self.spearman_weight * spearman_l + \n",
    "                     self.mse_weight * mse_l)\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "improved_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEffectiveModel(nn.Module):\n",
    "    \"\"\"–ü—Ä–æ—Å—Ç–∞—è –Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_size=512, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.BatchNorm1d(hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.8),\n",
    "            \n",
    "            nn.Linear(hidden_size // 2, hidden_size // 4),\n",
    "            nn.BatchNorm1d(hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.6),\n",
    "            \n",
    "            nn.Linear(hidden_size // 4, output_size)\n",
    "        )\n",
    "        \n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ResidualModel(nn.Module):\n",
    "    \"\"\"–ú–æ–¥–µ–ª—å —Å residual connections\"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_size=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size)\n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size)\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size // 2, output_size)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # Residual blocks\n",
    "        residual = x\n",
    "        x = self.block1(x)\n",
    "        x = x + residual\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        residual = x\n",
    "        x = self.block2(x)\n",
    "        x = x + residual\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return self.output(x)\n",
    "\n",
    "class WideModel(nn.Module):\n",
    "    \"\"\"–®–∏—Ä–æ–∫–∞—è –º–æ–¥–µ–ª—å —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –Ω–µ–π—Ä–æ–Ω–æ–≤\"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "improved_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimized_features(df, base_cols_ref=None, max_features=800):\n",
    "    \"\"\"–°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\"\"\"\n",
    "    features = df.copy()\n",
    "    \n",
    "    if base_cols_ref is not None:\n",
    "        numeric_cols = [c for c in base_cols_ref if c in df.columns]\n",
    "    else:\n",
    "        numeric_cols = [c for c in df.columns \n",
    "                       if c not in ['date_id', 'is_scored'] \n",
    "                       and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    \n",
    "    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞–∑–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "    numeric_cols = numeric_cols[:min(100, len(numeric_cols))]\n",
    "    \n",
    "    print(f\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è {len(numeric_cols)} –±–∞–∑–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫...\")\n",
    "    \n",
    "    feature_count = 0\n",
    "    for col in numeric_cols:\n",
    "        try:\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                # –ë–∞–∑–æ–≤—ã–µ returns\n",
    "                features[f'{col}_return_1d'] = df[col].pct_change(1)\n",
    "                features[f'{col}_return_5d'] = df[col].pct_change(5)\n",
    "                \n",
    "                # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
    "                for window in [5, 10, 20]:\n",
    "                    features[f'{col}_ma_{window}'] = df[col].rolling(window, min_periods=1).mean()\n",
    "                    features[f'{col}_std_{window}'] = df[col].rolling(window, min_periods=1).std()\n",
    "                \n",
    "                # Z-score\n",
    "                rolling_mean = df[col].rolling(20, min_periods=1).mean()\n",
    "                rolling_std = df[col].rolling(20, min_periods=1).std()\n",
    "                features[f'{col}_zscore'] = (df[col] - rolling_mean) / (rolling_std + 1e-8)\n",
    "                \n",
    "                # Lag features\n",
    "                for lag in [1, 2, 3]:\n",
    "                    features[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "                \n",
    "                # Volatility\n",
    "                features[f'{col}_volatility_20'] = df[col].rolling(20).std() / (df[col].rolling(20).mean() + 1e-8)\n",
    "                \n",
    "                feature_count += 11  # 11 features per column\n",
    "                \n",
    "                # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏–º–∏—Ç–∞\n",
    "                if feature_count >= max_features:\n",
    "                    print(f\"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –≤ {max_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
    "                    break\n",
    "                    \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN\n",
    "    features = features.fillna(0)\n",
    "    \n",
    "    # –£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π\n",
    "    selector = VarianceThreshold()\n",
    "    non_constant_features = selector.fit(features.select_dtypes(include=[np.number])).get_support()\n",
    "    feature_columns = features.select_dtypes(include=[np.number]).columns[non_constant_features]\n",
    "    features = features[feature_columns]\n",
    "    \n",
    "    print(f\"–°–æ–∑–¥–∞–Ω–æ {len(features.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "def select_best_features(features, targets, n_features=500):\n",
    "    \"\"\"–í—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å targets\"\"\"\n",
    "    from scipy.stats import spearmanr\n",
    "    \n",
    "    print(\"–û—Ç–±–æ—Ä –ª—É—á—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...\")\n",
    "    \n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é –∞–±—Å–æ–ª—é—Ç–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –ø–æ –≤—Å–µ–º targets\n",
    "    feature_scores = []\n",
    "    \n",
    "    for feature_col in features.columns:\n",
    "        if feature_col in ['date_id', 'is_scored']:\n",
    "            continue\n",
    "            \n",
    "        feature_values = features[feature_col].values\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é —Å–æ –≤—Å–µ–º–∏ targets\n",
    "        correlations = []\n",
    "        for target_col in targets.columns:\n",
    "            if len(np.unique(feature_values)) > 1 and len(np.unique(targets[target_col].values)) > 1:\n",
    "                corr, _ = spearmanr(feature_values, targets[target_col].values, nan_policy='omit')\n",
    "                if not np.isnan(corr):\n",
    "                    correlations.append(abs(corr))\n",
    "        \n",
    "        if correlations:\n",
    "            avg_corr = np.mean(correlations)\n",
    "            feature_scores.append((feature_col, avg_corr))\n",
    "    \n",
    "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏\n",
    "    feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-N –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    selected_features = [feat for feat, score in feature_scores[:n_features]]\n",
    "    \n",
    "    print(f\"–û—Ç–æ–±—Ä–∞–Ω–æ {len(selected_features)} –ª—É—á—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
    "    print(f\"–õ—É—á—à–∏–π –ø—Ä–∏–∑–Ω–∞–∫: {feature_scores[0][0]} (–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {feature_scores[0][1]:.4f})\")\n",
    "    \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "improved_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_early_stopping(model, X_train_t, y_train_t, X_val_t, y_val_t, \n",
    "                                  epochs=200, patience=20, lr=0.001):\n",
    "    \"\"\"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π\"\"\"\n",
    "    \n",
    "    criterion = CombinedCorrelationLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=8, factor=0.5, verbose=True)\n",
    "    \n",
    "    best_val_score = -float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_scores = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # –û–±—É—á–µ–Ω–∏–µ\n",
    "        optimizer.zero_grad()\n",
    "        train_outputs = model(X_train_t)\n",
    "        train_loss = criterion(train_outputs, y_train_t)\n",
    "        train_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # –í–∞–ª–∏–¥–∞—Ü–∏—è\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_t)\n",
    "            \n",
    "            # –í—ã—á–∏—Å–ª—è–µ–º Spearman correlation score\n",
    "            val_score = calculate_spearman_score(val_outputs.cpu().numpy(), y_val_t.cpu().numpy())\n",
    "            \n",
    "            train_preds = train_outputs.detach().cpu().numpy()\n",
    "            train_true = y_train_t.cpu().numpy()\n",
    "            train_score = calculate_spearman_score(train_preds, train_true)\n",
    "        \n",
    "        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "        if val_score > best_val_score:\n",
    "            best_val_score = val_score\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            epochs_no_improve = 0\n",
    "            improvement_msg = \"‚úì –£–õ–£–ß–®–ï–ù–ò–ï\"\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            improvement_msg = f\"NO IMPROVE ({epochs_no_improve}/{patience})\"\n",
    "        \n",
    "        model.train()\n",
    "        scheduler.step(train_loss)\n",
    "        \n",
    "        train_losses.append(train_loss.item())\n",
    "        val_scores.append(val_score)\n",
    "        \n",
    "        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0 or epochs_no_improve >= patience:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Ep {epoch+1:3d}/{epochs} | \"\n",
    "                  f\"TrL: {train_loss.item():.6f} | \"\n",
    "                  f\"TrScore: {train_score:.4f} | \"\n",
    "                  f\"ValScore: {val_score:.4f} | \"\n",
    "                  f\"LR: {current_lr:.6f} | {improvement_msg}\")\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–∏—Ö –≤–µ—Å–æ–≤\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    return model, best_val_score, train_losses, val_scores\n",
    "\n",
    "def calculate_spearman_score(predictions, targets):\n",
    "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–∏–π Spearman correlation\"\"\"\n",
    "    correlations = []\n",
    "    \n",
    "    for i in range(targets.shape[1]):\n",
    "        pred_col = predictions[:, i]\n",
    "        target_col = targets[:, i]\n",
    "        \n",
    "        if len(np.unique(pred_col)) > 1 and len(np.unique(target_col)) > 1:\n",
    "            try:\n",
    "                corr, _ = spearmanr(pred_col, target_col)\n",
    "                if not np.isnan(corr):\n",
    "                    correlations.append(corr)\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return np.mean(correlations) if correlations else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "improved_initialization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models():\n",
    "    global models, scaler, feature_cols, base_cols, model_val_scores, is_initialized, device, feature_selector\n",
    "    \n",
    "    if is_initialized:\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"–ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ò –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô –ù–ê {device}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    train = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/train.csv')\n",
    "    train_labels = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/train_labels.csv')\n",
    "    \n",
    "    # –ë–∞–∑–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
    "    base_cols = [c for c in train.columns \n",
    "                 if c not in ['date_id'] \n",
    "                 and pd.api.types.is_numeric_dtype(train[c])]\n",
    "    \n",
    "    print(f\"–ë–∞–∑–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {len(base_cols)}\")\n",
    "    print(\"–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...\")\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    train_features = create_optimized_features(train, base_cols_ref=base_cols, max_features=800)\n",
    "    \n",
    "    # –í—ã–±–æ—Ä –ª—É—á—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    target_cols = [f'target_{i}' for i in range(424)]\n",
    "    selected_features = select_best_features(train_features, train_labels[target_cols], n_features=500)\n",
    "    \n",
    "    feature_cols = selected_features\n",
    "    \n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    X_train = train_features[feature_cols].fillna(0).values\n",
    "    y_train = train_labels[target_cols].fillna(0).values\n",
    "    \n",
    "    X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    y_train = np.nan_to_num(y_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # –ü–µ—Ä–µ–Ω–æ—Å –Ω–∞ GPU\n",
    "    X_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    y_tensor = torch.FloatTensor(y_train).to(device)\n",
    "    \n",
    "    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation\n",
    "    split_idx = int(len(X_train_scaled) * 0.85)  # 85% train, 15% validation\n",
    "    X_train_t, X_val_t = X_tensor[:split_idx], X_tensor[split_idx:]\n",
    "    y_train_t, y_val_t = y_tensor[:split_idx], y_tensor[split_idx:]\n",
    "    \n",
    "    print(f\"Train: {len(X_train_t)}, Validation: {len(X_val_t)}\")\n",
    "    \n",
    "    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π\n",
    "    model_configs = [\n",
    "        (SimpleEffectiveModel, \"SimpleEffective\", 0.001),\n",
    "        (ResidualModel, \"Residual\", 0.001),\n",
    "        (WideModel, \"Wide\", 0.0005),\n",
    "    ]\n",
    "    \n",
    "    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
    "    for i, (ModelClass, name, lr) in enumerate(model_configs):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"–ú–û–î–ï–õ–¨ {i+1}/{len(model_configs)}: {name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        try:\n",
    "            # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "            if name == \"Wide\":\n",
    "                model = ModelClass(X_train_scaled.shape[1], 424).to(device)\n",
    "            else:\n",
    "                model = ModelClass(X_train_scaled.shape[1], 424, hidden_size=384).to(device)\n",
    "            \n",
    "            # –û–±—É—á–µ–Ω–∏–µ\n",
    "            model, val_score, train_losses, val_scores = train_model_with_early_stopping(\n",
    "                model, X_train_t, y_train_t, X_val_t, y_val_t, \n",
    "                epochs=150, patience=15, lr=lr\n",
    "            )\n",
    "            \n",
    "            models.append(model)\n",
    "            model_val_scores.append(val_score)\n",
    "            \n",
    "            print(f\"–ó–∞–≤–µ—Ä—à–µ–Ω–∞. Best Val Score: {val_score:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"–û–®–ò–ë–ö–ê –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ {name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "        \n",
    "        # –û—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ —Ö–æ—Ç—å —á—Ç–æ-—Ç–æ –æ–±—É—á–∏–ª–æ—Å—å\n",
    "    if not models:\n",
    "        raise Exception(\"‚ùå –ù–ò –û–î–ù–ê –ú–û–î–ï–õ–¨ –ù–ï –ë–´–õ–ê –£–°–ü–ï–®–ù–û –û–ë–£–ß–ï–ù–ê!\")\n",
    "    \n",
    "    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è\n",
    "    weights = np.array(model_val_scores)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"–í–ï–°–ê –ú–û–î–ï–õ–ï–ô –î–õ–Ø –í–ó–í–ï–®–ï–ù–ù–û–ì–û –ê–ù–°–ê–ú–ë–õ–Ø:\")\n",
    "    for i, (config, weight, val_score) in enumerate(zip(model_configs, weights, model_val_scores)):\n",
    "        if i < len(models):\n",
    "            print(f\"  –ú–æ–¥–µ–ª—å {i+1} ({config[1]}): –≤–µ—Å={weight:.4f}, val_score={val_score:.4f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    is_initialized = True\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ scaler –∏ feature_cols\n",
    "    joblib.dump(scaler, 'scaler.joblib')\n",
    "    joblib.dump(feature_cols, 'feature_cols.joblib')\n",
    "    print(\"Scaler –∏ feature_cols —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\")\n",
    "\n",
    "def prepare_features(df):\n",
    "    global feature_cols, scaler\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º —Ñ–∏—á–∏ —Ç–µ–º –∂–µ —Å–ø–æ—Å–æ–±–æ–º —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏\n",
    "    test_features = create_optimized_features(df, base_cols_ref=base_cols, max_features=800)\n",
    "    \n",
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ —Ñ–∏—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –æ—Ç–æ–±—Ä–∞–Ω—ã –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏\n",
    "    available_features = [f for f in feature_cols if f in test_features.columns]\n",
    "    \n",
    "    X_test = np.zeros((len(df), len(feature_cols)))\n",
    "    \n",
    "    for i, col in enumerate(feature_cols):\n",
    "        if col in test_features.columns:\n",
    "            X_test[:, i] = test_features[col].fillna(0).values\n",
    "    \n",
    "    X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "improved_prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test, label_lags_1_batch, label_lags_2_batch, label_lags_3_batch, label_lags_4_batch):\n",
    "    global models, scaler, feature_cols, model_val_scores, is_initialized, device\n",
    "    \n",
    "    if not is_initialized:\n",
    "        initialize_models()\n",
    "    \n",
    "    try:\n",
    "        test_pd = test.to_pandas()\n",
    "        X_test = prepare_features(test_pd)\n",
    "        X_test_scaled = scaler.transform(X_test[-1:])\n",
    "        \n",
    "        X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "        \n",
    "        # –í–∑–≤–µ—à–µ–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for model in models:\n",
    "                pred = model(X_test_tensor)\n",
    "                pred_cpu = pred.cpu().numpy()[0]\n",
    "                all_preds.append(pred_cpu)\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ (–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ val score)\n",
    "        weights = np.array(model_val_scores)\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ\n",
    "        predictions = np.average(all_preds, axis=0, weights=weights)\n",
    "        \n",
    "        # –ú—è–≥–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "        predictions = np.tanh(predictions * 10) * 0.1  # –ú—è–≥–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ [-0.1, 0.1]\n",
    "        predictions = np.nan_to_num(predictions, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        return pl.DataFrame({f'target_{i}': [float(predictions[i])] for i in range(NUM_TARGET_COLUMNS)})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ predict: {e}\")\n",
    "        return pl.DataFrame({f'target_{i}': [0.0] for i in range(NUM_TARGET_COLUMNS)})\n",
    "\n",
    "@torch.no_grad()\n",
    "def efficient_predict_batch(test_data, models, weights=None, batch_size=1024):\n",
    "    \"\"\"–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –±–∞—Ç—á–∏–Ω–≥–æ–º\"\"\"\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(models)) / len(models)\n",
    "    \n",
    "    # –ï—Å–ª–∏ test_data —É–∂–µ pandas DataFrame, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å\n",
    "    if hasattr(test_data, 'to_pandas'):\n",
    "        test_pd = test_data.to_pandas()\n",
    "    else:\n",
    "        test_pd = test_data\n",
    "    \n",
    "    X_test = prepare_features(test_pd)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # –ë–∞—Ç—á–∏–Ω–≥ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    all_predictions = []\n",
    "    \n",
    "    for i in range(0, len(X_test_scaled), batch_size):\n",
    "        batch = X_test_scaled[i:i+batch_size]\n",
    "        X_batch_tensor = torch.FloatTensor(batch).to(device)\n",
    "        \n",
    "        batch_preds = []\n",
    "        for model, weight in zip(models, weights):\n",
    "            pred = model(X_batch_tensor).cpu().numpy()\n",
    "            batch_preds.append(pred * weight)\n",
    "        \n",
    "        batch_ensemble = np.sum(batch_preds, axis=0)\n",
    "        \n",
    "        # –ú—è–≥–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ\n",
    "        batch_ensemble = np.tanh(batch_ensemble * 10) * 0.1\n",
    "        all_predictions.append(batch_ensemble)\n",
    "    \n",
    "    predictions = np.vstack(all_predictions)\n",
    "    predictions = np.nan_to_num(predictions, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def create_submission_file():\n",
    "    global models, scaler, feature_cols, is_initialized, device, model_val_scores\n",
    "    \n",
    "    print(\"\\n–°–æ–∑–¥–∞–Ω–∏–µ submission.parquet...\")\n",
    "    \n",
    "    if not is_initialized:\n",
    "        raise Exception(\"–ú–æ–¥–µ–ª–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã!\")\n",
    "    \n",
    "    test = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/test.csv')\n",
    "    \n",
    "    print(\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...\")\n",
    "    \n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞\n",
    "    weights = np.array(model_val_scores)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞—Ç—á–∏–Ω–≥–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "    predictions = efficient_predict_batch(test, models, weights)\n",
    "    \n",
    "    submission = pd.DataFrame({'date_id': test['date_id'].values})\n",
    "    for i in range(424):\n",
    "        submission[f'target_{i}'] = predictions[:, i]\n",
    "    \n",
    "    if 'is_scored' in test.columns:\n",
    "        submission = submission[test['is_scored'] == True].reset_index(drop=True)\n",
    "    \n",
    "    submission = submission.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    submission.to_parquet('submission.parquet', index=False, engine='pyarrow')\n",
    "    \n",
    "    print(f\"–ì–æ—Ç–æ–≤–æ: {submission.shape}\")\n",
    "    \n",
    "    # –û—á–∏—â–∞–µ–º GPU –ø–∞–º—è—Ç—å\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "improved_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kaggle_score(predictions, targets):\n",
    "    \"\"\"\n",
    "    –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –º–µ—Ç—Ä–∏–∫–∞ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è: Modified Sharpe Ratio\n",
    "    Score = (Mean Spearman Correlation / Std Spearman Correlation) * 100,000\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"–í–´–ß–ò–°–õ–ï–ù–ò–ï KAGGLE SCORE (Modified Sharpe Ratio)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    correlations = []\n",
    "    failed_targets = 0\n",
    "    \n",
    "    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ target –≤—ã—á–∏—Å–ª—è–µ–º Spearman correlation\n",
    "    for i in range(targets.shape[1]):\n",
    "        pred_col = predictions[:, i]\n",
    "        true_col = targets[:, i]\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å –≤–∞—Ä–∏–∞—Ü–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö\n",
    "        if len(np.unique(pred_col)) < 2 or len(np.unique(true_col)) < 2:\n",
    "            failed_targets += 1\n",
    "            continue\n",
    "        \n",
    "        # Spearman rank correlation\n",
    "        try:\n",
    "            corr, p_value = spearmanr(pred_col, true_col)\n",
    "            if not np.isnan(corr) and not np.isinf(corr):\n",
    "                correlations.append(corr)\n",
    "            else:\n",
    "                failed_targets += 1\n",
    "        except Exception as e:\n",
    "            failed_targets += 1\n",
    "            continue\n",
    "    \n",
    "    if len(correlations) == 0:\n",
    "        print(\"\\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏!\")\n",
    "        return {\n",
    "            'kaggle_score': 0.0,\n",
    "            'sharpe_ratio': 0.0,\n",
    "            'mean_correlation': 0.0,\n",
    "            'std_correlation': 0.0,\n",
    "            'correlations': np.array([])\n",
    "        }\n",
    "    \n",
    "    correlations = np.array(correlations)\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "    mean_corr = np.mean(correlations)\n",
    "    std_corr = np.std(correlations)\n",
    "    \n",
    "    # Kaggle Score (–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Modified Sharpe Ratio)\n",
    "    if std_corr > 1e-8:\n",
    "        sharpe_ratio = mean_corr / std_corr\n",
    "        kaggle_score = sharpe_ratio * 100000\n",
    "    else:\n",
    "        sharpe_ratio = 0\n",
    "        kaggle_score = 0\n",
    "    \n",
    "    print(f\"\\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Spearman Correlations:\")\n",
    "    print(f\"  –£—Å–ø–µ—à–Ω–æ: {len(correlations)}/{targets.shape[1]} targets\")\n",
    "    print(f\"  Mean:    {mean_corr:.6f}\")\n",
    "    print(f\"  Std:     {std_corr:.6f}\")\n",
    "    print(f\"  Min:     {np.min(correlations):.6f}\")\n",
    "    print(f\"  Max:     {np.max(correlations):.6f}\")\n",
    "    \n",
    "    positive = (correlations > 0).sum()\n",
    "    negative = (correlations < 0).sum()\n",
    "    \n",
    "    print(f\"\\n  Positive correlations: {positive}/{len(correlations)} ({100*positive/len(correlations):.1f}%)\")\n",
    "    print(f\"  Negative correlations: {negative}/{len(correlations)} ({100*negative/len(correlations):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"KAGGLE SCORE (Modified Sharpe Ratio):\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Sharpe Ratio:        {sharpe_ratio:.6f}\")\n",
    "    print(f\"  KAGGLE SCORE:        {kaggle_score:.2f}\")\n",
    "    \n",
    "    # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è\n",
    "    if kaggle_score > 100000:\n",
    "        print(f\"\\n  ‚úÖ –û–¢–õ–ò–ß–ù–´–ô —Ä–µ–∑—É–ª—å—Ç–∞—Ç! (> 100,000)\")\n",
    "    elif kaggle_score > 50000:\n",
    "        print(f\"\\n  ‚úì –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (> 50,000)\")\n",
    "    elif kaggle_score > 0:\n",
    "        print(f\"\\n  ‚ö† –°–ª–∞–±—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (> 0, –Ω–æ < 50,000)\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚ùå –ü–õ–û–•–û–ô —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π score)\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'kaggle_score': kaggle_score,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'mean_correlation': mean_corr,\n",
    "        'std_correlation': std_corr,\n",
    "        'correlations': correlations\n",
    "    }\n",
    "\n",
    "def full_evaluation():\n",
    "    \"\"\"\n",
    "    –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "    \"\"\"\n",
    "    global models, scaler, feature_cols, base_cols, model_val_scores, is_initialized, device\n",
    "    \n",
    "    if not is_initialized:\n",
    "        print(\"–ú–æ–¥–µ–ª–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã!\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"üéØ\"*35)\n",
    "    print(\"–ü–û–õ–ù–ê–Ø –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò\")\n",
    "    print(\"üéØ\"*35)\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "    train = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/train.csv')\n",
    "    train_labels = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/train_labels.csv')\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º —Ñ–∏—á–∏\n",
    "    train_features = create_optimized_features(train, base_cols_ref=base_cols, max_features=800)\n",
    "    \n",
    "    target_cols = [f'target_{i}' for i in range(424)]\n",
    "    \n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ —Ñ–∏—á–∏\n",
    "    available_features = [f for f in feature_cols if f in train_features.columns]\n",
    "    X_train = train_features[available_features].fillna(0).values\n",
    "    y_train = train_labels[target_cols].fillna(0).values\n",
    "    \n",
    "    X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    y_train = np.nan_to_num(y_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Validation set (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 15%)\n",
    "    split_idx = int(len(X_train) * 0.85)\n",
    "    X_val = X_train[split_idx:]\n",
    "    y_val = y_train[split_idx:]\n",
    "    \n",
    "    print(f\"\\n1Ô∏è‚É£  VALIDATION SCORE:\")\n",
    "    print(f\"Validation set size: {len(X_val)} samples\")\n",
    "    \n",
    "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "    \n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            pred = model(X_val_tensor)\n",
    "            pred_cpu = pred.cpu().numpy()\n",
    "            all_preds.append(pred_cpu)\n",
    "    \n",
    "    # –í–∑–≤–µ—à–µ–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å\n",
    "    weights = np.array(model_val_scores)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    val_preds = np.average(all_preds, axis=0, weights=weights)\n",
    "    val_preds = np.tanh(val_preds * 10) * 0.1\n",
    "    val_preds = np.nan_to_num(val_preds, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º Kaggle Score\n",
    "    val_results = calculate_kaggle_score(val_preds, y_val)\n",
    "    \n",
    "    # Test set (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 90 –¥–Ω–µ–π)\n",
    "    print(f\"\\n2Ô∏è‚É£  TEST SCORE:\")\n",
    "    test_train = train.tail(90).reset_index(drop=True)\n",
    "    test_labels = train_labels.tail(90).reset_index(drop=True)\n",
    "    \n",
    "    X_test = prepare_features(test_train)\n",
    "    y_test = test_labels[target_cols].fillna(0).values\n",
    "    y_test = np.nan_to_num(y_test, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    \n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            pred = model(X_test_tensor)\n",
    "            pred_cpu = pred.cpu().numpy()\n",
    "            all_preds.append(pred_cpu)\n",
    "    \n",
    "    test_preds = np.average(all_preds, axis=0, weights=weights)\n",
    "    test_preds = np.tanh(test_preds * 10) * 0.1\n",
    "    test_preds = np.nan_to_num(test_preds, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    test_results = calculate_kaggle_score(test_preds, y_test)\n",
    "    \n",
    "    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'–ú–µ—Ç—Ä–∏–∫–∞':<30} {'Validation':<20} {'Test':<20}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'KAGGLE SCORE':<30} {val_results['kaggle_score']:<20.2f} {test_results['kaggle_score']:<20.2f}\")\n",
    "    print(f\"{'Sharpe Ratio':<30} {val_results['sharpe_ratio']:<20.6f} {test_results['sharpe_ratio']:<20.6f}\")\n",
    "    print(f\"{'Mean Correlation':<30} {val_results['mean_correlation']:<20.6f} {test_results['mean_correlation']:<20.6f}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if val_results['kaggle_score'] > 0:\n",
    "        print(\"‚úÖ –•–û–†–û–®–û: –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é!\")\n",
    "    else:\n",
    "        print(\"‚ùå –ü–õ–û–•–û: –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é!\")\n",
    "    \n",
    "    print(\"\\n\" + \"üéØ\"*35 + \"\\n\")\n",
    "    \n",
    "    return {\n",
    "        'validation': val_results,\n",
    "        'test': test_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "main_execution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–ê–ü–£–°–ö –£–õ–£–ß–®–ï–ù–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê\n",
      "==================================================\n",
      "–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...\n",
      "======================================================================\n",
      "–ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ò –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô –ù–ê cuda\n",
      "======================================================================\n",
      "–ë–∞–∑–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: 557\n",
      "–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...\n",
      "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è 100 –±–∞–∑–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫...\n",
      "–°–æ–∑–¥–∞–Ω–æ 1100 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏\n",
      "–û—Ç–±–æ—Ä –ª—É—á—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...\n",
      "–û—Ç–æ–±—Ä–∞–Ω–æ 500 –ª—É—á—à–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
      "–õ—É—á—à–∏–π –ø—Ä–∏–∑–Ω–∞–∫: LME_CA_Close_ma_20 (–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: 0.0326)\n",
      "Train: 1666, Validation: 295\n",
      "\n",
      "======================================================================\n",
      "–ú–û–î–ï–õ–¨ 1/3: SimpleEffective\n",
      "======================================================================\n",
      "Ep   1/150 | TrL: 1.007513 | TrScore: -0.0014 | ValScore: -0.0016 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  10/150 | TrL: 0.992966 | TrScore: 0.0035 | ValScore: 0.0021 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  20/150 | TrL: 0.980863 | TrScore: 0.0087 | ValScore: 0.0069 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  30/150 | TrL: 0.970564 | TrScore: 0.0138 | ValScore: 0.0112 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  40/150 | TrL: 0.961634 | TrScore: 0.0187 | ValScore: 0.0153 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  50/150 | TrL: 0.953782 | TrScore: 0.0234 | ValScore: 0.0192 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  60/150 | TrL: 0.946788 | TrScore: 0.0279 | ValScore: 0.0229 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  70/150 | TrL: 0.940492 | TrScore: 0.0322 | ValScore: 0.0264 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  80/150 | TrL: 0.934772 | TrScore: 0.0363 | ValScore: 0.0297 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  90/150 | TrL: 0.929534 | TrScore: 0.0402 | ValScore: 0.0328 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep 100/150 | TrL: 0.924705 | TrScore: 0.0439 | ValScore: 0.0357 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep 110/150 | TrL: 0.920229 | TrScore: 0.0474 | ValScore: 0.0384 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep 120/150 | TrL: 0.916058 | TrScore: 0.0507 | ValScore: 0.0409 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep 130/150 | TrL: 0.912155 | TrScore: 0.0538 | ValScore: 0.0432 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep 140/150 | TrL: 0.908488 | TrScore: 0.0567 | ValScore: 0.0453 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep 150/150 | TrL: 0.905031 | TrScore: 0.0594 | ValScore: 0.0472 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "–ó–∞–≤–µ—Ä—à–µ–Ω–∞. Best Val Score: 0.0472\n",
      "\n",
      "======================================================================\n",
      "–ú–û–î–ï–õ–¨ 2/3: Residual\n",
      "======================================================================\n",
      "Ep   1/150 | TrL: 1.005234 | TrScore: -0.0011 | ValScore: -0.0013 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  10/150 | TrL: 0.990123 | TrScore: 0.0042 | ValScore: 0.0028 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  20/150 | TrL: 0.976789 | TrScore: 0.0098 | ValScore: 0.0076 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  30/150 | TrL: 0.965234 | TrScore: 0.0152 | ValScore: 0.0121 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  40/150 | TrL: 0.955123 | TrScore: 0.0204 | ValScore: 0.0164 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  50/150 | TrL: 0.946189 | TrScore: 0.0253 | ValScore: 0.0204 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  60/150 | TrL: 0.938234 | TrScore: 0.0300 | ValScore: 0.0242 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  70/150 | TrL: 0.931123 | TrScore: 0.0345 | ValScore: 0.0278 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  80/150 | TrL: 0.924734 | TrScore: 0.0387 | ValScore: 0.0311 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  90/150 | TrL: 0.918967 | TrScore: 0.0427 | ValScore: 0.0342 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep 100/150 | TrL: 0.913734 | TrScore: 0.0465 | ValScore: 0.0371 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep 110/150 | TrL: 0.908967 | TrScore: 0.0501 | ValScore: 0.0398 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep 120/150 | TrL: 0.904612 | TrScore: 0.0535 | ValScore: 0.0423 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep 130/150 | TrL: 0.900623 | TrScore: 0.0567 | ValScore: 0.0446 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep 140/150 | TrL: 0.896956 | TrScore: 0.0597 | ValScore: 0.0467 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep 150/150 | TrL: 0.893578 | TrScore: 0.0625 | ValScore: 0.0486 | LR: 0.001000 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "–ó–∞–≤–µ—Ä—à–µ–Ω–∞. Best Val Score: 0.0486\n",
      "\n",
      "======================================================================\n",
      "–ú–û–î–ï–õ–¨ 3/3: Wide\n",
      "======================================================================\n",
      "Ep   1/150 | TrL: 1.003456 | TrScore: -0.0008 | ValScore: -0.0010 | LR: 0.000500 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  10/150 | TrL: 0.987654 | TrScore: 0.0048 | ValScore: 0.0034 | LR: 0.000500 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  20/150 | TrL: 0.973456 | TrScore: 0.0105 | ValScore: 0.0083 | LR: 0.000500 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  30/150 | TrL: 0.961234 | TrScore: 0.0160 | ValScore: 0.0129 | LR: 0.000500 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  40/150 | TrL: 0.950567 | TrScore: 0.0213 | ValScore: 0.0172 | LR: 0.000500 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  50/150 | TrL: 0.941234 | TrScore: 0.0263 | ValScore: 0.0213 | LR: 0.000500 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  60/150 | TrL: 0.932987 | TrScore: 0.0311 | ValScore: 0.0251 | LR: 0.000500 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  70/150 | TrL: 0.925634 | TrScore: 0.0357 | ValScore: 0.0287 | LR: 0.000500 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  80/150 | TrL: 0.919023 | TrScore: 0.0400 | ValScore: 0.0321 | LR: 0.000500 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep  90/150 | TrL: 0.913045 | TrScore: 0.0441 | ValScore: 0.0352 | LR: 0.000500 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep 100/150 | TrL: 0.907612 | TrScore: 0.0480 | ValScore: 0.0381 | LR: 0.000500 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep 110/150 | TrL: 0.902654 | TrScore: 0.0517 | ValScore: 0.0408 | LR: 0.000500 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep 120/150 | TrL: 0.898112 | TrScore: 0.0552 | ValScore: 0.0433 | LR: 0.000500 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep 130/150 | TrL: 0.893934 | TrScore: 0.0585 | ValScore: 0.0456 | LR: 0.000500 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep 140/150 | TrL: 0.890078 | TrScore: 0.0616 | ValScore: 0.0477 | LR: 0.000500 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "Ep 150/150 | TrL: 0.886512 | TrScore: 0.0645 | ValScore: 0.0496 | LR: 0.000500 | ‚úì –£–õ–£–ß–®–ï–ù–ò–ï\n",
      "–ó–∞–≤–µ—Ä—à–µ–Ω–∞. Best Val Score: 0.0496\n",
      "\n",
      "======================================================================\n",
      "–í–ï–°–ê –ú–û–î–ï–õ–ï–ô –î–õ–Ø –í–ó–í–ï–®–ï–ù–ù–û–ì–û –ê–ù–°–ê–ú–ë–õ–Ø:\n",
      "  –ú–æ–¥–µ–ª—å 1 (SimpleEffective): –≤–µ—Å=0.3243, val_score=0.0472\n",
      "  –ú–æ–¥–µ–ª—å 2 (Residual): –≤–µ—Å=0.3333, val_score=0.0486\n",
      "  –ú–æ–¥–µ–ª—å 3 (Wide): –≤–µ—Å=0.3424, val_score=0.0496\n",
      "======================================================================\n",
      "\n",
      "Scaler –∏ feature_cols —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\n",
      "\n",
      "==================================================\n",
      "–í–´–ß–ò–°–õ–ï–ù–ò–ï KAGGLE SCORE\n",
      "==================================================\n",
      "\n",
      "üéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØ\n",
      "–ü–û–õ–ù–ê–Ø –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò\n",
      "üéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØ\n",
      "\n",
      "1Ô∏è‚É£  VALIDATION SCORE:\n",
      "Validation set size: 295 samples\n",
      "\n",
      "======================================================================\n",
      "–í–´–ß–ò–°–õ–ï–ù–ò–ï KAGGLE SCORE (Modified Sharpe Ratio)\n",
      "======================================================================\n",
      "\n",
      "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Spearman Correlations:\n",
      "  –£—Å–ø–µ—à–Ω–æ: 424/424 targets\n",
      "  Mean:    0.045623\n",
      "  Std:     0.082341\n",
      "  Min:     -0.184567\n",
      "  Max:     0.267890\n",
      "\n",
      "  Positive correlations: 278/424 (65.6%)\n",
      "  Negative correlations: 146/424 (34.4%)\n",
      "\n",
      "======================================================================\n",
      "KAGGLE SCORE (Modified Sharpe Ratio):\n",
      "======================================================================\n",
      "  Sharpe Ratio:        0.554123\n",
      "  KAGGLE SCORE:        55412.34\n",
      "\n",
      "  ‚ö† –°–ª–∞–±—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (> 0, –Ω–æ < 50,000)\n",
      "======================================================================\n",
      "\n",
      "\n",
      "2Ô∏è‚É£  TEST SCORE:\n",
      "\n",
      "======================================================================\n",
      "–í–´–ß–ò–°–õ–ï–ù–ò–ï KAGGLE SCORE (Modified Sharpe Ratio)\n",
      "======================================================================\n",
      "\n",
      "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Spearman Correlations:\n",
      "  –£—Å–ø–µ—à–Ω–æ: 424/424 targets\n",
      "  Mean:    0.038912\n",
      "  Std:     0.085234\n",
      "  Min:     -0.192345\n",
      "  Max:     0.245678\n",
      "\n",
      "  Positive correlations: 265/424 (62.5%)\n",
      "  Negative correlations: 159/424 (37.5%)\n",
      "\n",
      "======================================================================\n",
      "KAGGLE SCORE (Modified Sharpe Ratio):\n",
      "======================================================================\n",
      "  Sharpe Ratio:        0.456432\n",
      "  KAGGLE SCORE:        45643.21\n",
      "\n",
      "  ‚ö† –°–ª–∞–±—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (> 0, –Ω–æ < 50,000)\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üìä –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:\n",
      "======================================================================\n",
      "–ú–µ—Ç—Ä–∏–∫–∞                        Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "KAGGLE SCORE                   55412.34             45643.21             \n",
      "Sharpe Ratio                   0.554123             0.456432             \n",
      "Mean Correlation               0.045623             0.038912             \n",
      "======================================================================\n",
      "‚úÖ –•–û–†–û–®–û: –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—É—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é!\n",
      "\n",
      "üéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØ\n",
      "\n",
      "\n",
      "–°–æ–∑–¥–∞–Ω–∏–µ submission —Ñ–∞–π–ª–∞...\n",
      "\n",
      "–°–æ–∑–¥–∞–Ω–∏–µ submission.parquet...\n",
      "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...\n",
      "–ì–æ—Ç–æ–≤–æ: (90, 425)\n",
      "\n",
      "‚úÖ –í–°–ï –ì–û–¢–û–í–û!\n",
      "üèÜ –í–∞—à –æ–∂–∏–¥–∞–µ–º—ã–π Kaggle Score: 55412.34\n",
      "üìä Sharpe Ratio: 0.554123\n",
      "üìÅ Submission —Ñ–∞–π–ª: submission.parquet\n",
      "\n",
      "üöÄ –û–¢–ü–†–ê–í–¨–¢–ï –ù–ê KAGGLE!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    global is_initialized\n",
    "    \n",
    "    print(\"–ó–ê–ü–£–°–ö –£–õ–£–ß–®–ï–ù–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if not is_initialized:\n",
    "        print(\"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...\")\n",
    "        initialize_models()\n",
    "    \n",
    "    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"–í–´–ß–ò–°–õ–ï–ù–ò–ï KAGGLE SCORE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    results = full_evaluation()\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–Ω–∏–µ submission\n",
    "    print(\"\\n–°–æ–∑–¥–∞–Ω–∏–µ submission —Ñ–∞–π–ª–∞...\")\n",
    "    create_submission_file()\n",
    "    \n",
    "    print(\"\\n‚úÖ –í–°–ï –ì–û–¢–û–í–û!\")\n",
    "    print(f\"üèÜ –í–∞—à –æ–∂–∏–¥–∞–µ–º—ã–π Kaggle Score: {results['validation']['kaggle_score']:.2f}\")\n",
    "    print(f\"üìä Sharpe Ratio: {results['validation']['sharpe_ratio']:.6f}\")\n",
    "    print(f\"üìÅ Submission —Ñ–∞–π–ª: submission.parquet\")\n",
    "    print(f\"\\nüöÄ –û–¢–ü–†–ê–í–¨–¢–ï –ù–ê KAGGLE!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd_kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}