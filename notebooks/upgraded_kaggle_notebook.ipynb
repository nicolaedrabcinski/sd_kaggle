{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450541a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import optuna\n",
    "from optuna.trial import Trial\n",
    "import warnings\n",
    "import gc\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(f\"Используется устройство: {device}\")\n",
    "print(f\"Доступно GPU: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Название GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "NUM_TARGET_COLUMNS = 424\n",
    "\n",
    "# Глобальные переменные\n",
    "models = []\n",
    "scaler = None\n",
    "feature_cols = None\n",
    "base_cols = None\n",
    "is_initialized = False\n",
    "model_val_losses = []\n",
    "optuna_study = None\n",
    "best_hyperparams = None\n",
    "ensemble_weights_global = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9903a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartScaler:\n",
    "    \"\"\"Умный скейлер с обработкой выбросов\"\"\"\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_importance = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Обнаружение и обработка выбросов\n",
    "        X_clean = self._remove_outliers(X)\n",
    "        self.scaler.fit(X_clean)\n",
    "        \n",
    "        # Оценка важности признаков\n",
    "        if y is not None:\n",
    "            self.feature_importance = np.std(X_clean, axis=0)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit и transform в одном методе\"\"\"\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def _remove_outliers(self, X, n_sigmas=3):\n",
    "        \"\"\"Удаление выбросов по правилу 3 сигм\"\"\"\n",
    "        X_clean = X.copy()\n",
    "        for i in range(X.shape[1]):\n",
    "            col = X[:, i]\n",
    "            if np.std(col) > 1e-6:  # Проверяем, что есть вариация\n",
    "                mean, std = np.mean(col), np.std(col)\n",
    "                mask = (col >= mean - n_sigmas * std) & (col <= mean + n_sigmas * std)\n",
    "                X_clean[~mask, i] = mean\n",
    "        return X_clean\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e38e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedModel(nn.Module):\n",
    "    \"\"\"Улучшенная модель с регуляризацией и skip-connections\"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_sizes=[512, 256, 128], \n",
    "                 dropout_rates=[0.3, 0.2, 0.1], use_layer_norm=True, activation='relu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for i, (hidden_size, dropout_rate) in enumerate(zip(hidden_sizes, dropout_rates)):\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.LayerNorm(hidden_size) if use_layer_norm else nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU() if activation == 'relu' else nn.LeakyReLU(0.1),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(prev_size, output_size)\n",
    "        \n",
    "        # Skip connection от входа к выходу\n",
    "        self.skip = nn.Linear(input_size, output_size) if input_size != output_size else None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        output = self.output(features)\n",
    "        \n",
    "        if self.skip is not None:\n",
    "            output = output + 0.1 * self.skip(x)  # Малый вес для skip connection\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6095216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Блок с residual connection\"\"\"\n",
    "    def __init__(self, size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(size, size),\n",
    "            nn.LayerNorm(size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(size, size),\n",
    "            nn.LayerNorm(size),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.dropout(self.net(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b57de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedEnsembleModel(nn.Module):\n",
    "    \"\"\"Продвинутая модель с внутренним ансамблем\"\"\"\n",
    "    def __init__(self, input_size, output_size, num_experts=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_experts = num_experts\n",
    "        self.experts = nn.ModuleList([\n",
    "            ImprovedModel(input_size, 256, [512, 384], [0.3, 0.25]) \n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_experts),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        gates = self.gate(x)  # [batch_size, num_experts]\n",
    "        expert_outputs = [expert(x) for expert in self.experts]  # list of [batch_size, 256]\n",
    "        \n",
    "        # Правильное взвешивание экспертов\n",
    "        combined = torch.zeros_like(expert_outputs[0])\n",
    "        for i in range(self.num_experts):\n",
    "            # gates[:, i:i+1] - [batch_size, 1], expert_outputs[i] - [batch_size, 256]\n",
    "            combined += gates[:, i:i+1] * expert_outputs[i]\n",
    "        \n",
    "        return self.output(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb144455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Transformer-like модель для временных рядов\"\"\"\n",
    "    def __init__(self, input_size, output_size, num_heads=8, num_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_size, 512)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=512, \n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=1024,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = x.unsqueeze(1)  # Добавляем sequence dimension [batch_size, 1, 512]\n",
    "        x = self.transformer(x)\n",
    "        x = x.squeeze(1)    # Убираем sequence dimension [batch_size, 512]\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c38522",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartScaler:\n",
    "    \"\"\"Умный скейлер с обработкой выбросов\"\"\"\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_importance = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Обнаружение и обработка выбросов\n",
    "        X_clean = self._remove_outliers(X)\n",
    "        self.scaler.fit(X_clean)\n",
    "        \n",
    "        # Оценка важности признаков\n",
    "        if y is not None:\n",
    "            self.feature_importance = np.std(X_clean, axis=0)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit и transform в одном методе\"\"\"\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def _remove_outliers(self, X, n_sigmas=3):\n",
    "        \"\"\"Удаление выбросов по правилу 3 сигм\"\"\"\n",
    "        X_clean = X.copy()\n",
    "        for i in range(X.shape[1]):\n",
    "            col = X[:, i]\n",
    "            if np.std(col) > 1e-6:  # Проверяем, что есть вариация\n",
    "                mean, std = np.mean(col), np.std(col)\n",
    "                mask = (col >= mean - n_sigmas * std) & (col <= mean + n_sigmas * std)\n",
    "                X_clean[~mask, i] = mean\n",
    "        return X_clean\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.scaler.transform(X)\n",
    "\n",
    "# ==========================================\n",
    "# УЛУЧШЕННЫЕ АРХИТЕКТУРЫ МОДЕЛЕЙ\n",
    "# ==========================================\n",
    "\n",
    "class Model1_Deep(nn.Module):\n",
    "    \"\"\"Глубокая сеть с Layer Normalization и лучшей регуляризацией\"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.LayerNorm(512),  # LayerNorm вместо BatchNorm\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 384),\n",
    "            nn.LayerNorm(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Linear(384, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            \n",
    "            nn.Linear(128, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Model2_Wide(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 768)  # Меньше 1024→768\n",
    "        self.bn1 = nn.LayerNorm(768)  # LayerNorm вместо BatchNorm\n",
    "        self.drop1 = nn.Dropout(0.4)  # Больше dropout\n",
    "        \n",
    "        self.fc2 = nn.Linear(768, 512)\n",
    "        self.bn2 = nn.LayerNorm(512)\n",
    "        self.drop2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 384)\n",
    "        self.bn3 = nn.LayerNorm(384)\n",
    "        self.drop3 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.output = nn.Linear(384, output_size)\n",
    "        self.skip = nn.Linear(input_size, 384)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "        \n",
    "        x = self.activation(self.bn1(self.fc1(x)))\n",
    "        x = self.drop1(x)\n",
    "        x = self.activation(self.bn2(self.fc2(x)))\n",
    "        x = self.drop2(x)\n",
    "        x = self.activation(self.bn3(self.fc3(x)))\n",
    "        x = self.drop3(x)\n",
    "        \n",
    "        x = x + identity\n",
    "        return self.output(x)\n",
    "\n",
    "class Model3_Residual(nn.Module):\n",
    "    \"\"\"Исправленная Residual с BatchNorm и Pre-Activation\"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_size, 384)\n",
    "        self.bn_input = nn.BatchNorm1d(384)\n",
    "        \n",
    "        # Pre-activation residual blocks\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(384, 384),\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(384, 384)\n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(384, 384),\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(384, 384)\n",
    "        )\n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(384, 384),\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(384, 384)\n",
    "        )\n",
    "        \n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(384, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn_input(self.input_proj(x))\n",
    "        x = x + self.block1(x)\n",
    "        x = x + self.block2(x)\n",
    "        x = x + self.block3(x)\n",
    "        return self.output_proj(x)\n",
    "\n",
    "class Model4_DeepWide(nn.Module):\n",
    "    \"\"\"Deep & Wide с параллельными путями\"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Deep path (узкий и глубокий)\n",
    "        self.deep = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 384),\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Linear(384, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Wide path (широкий и мелкий)\n",
    "        self.wide = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Combine\n",
    "        self.combine = nn.Sequential(\n",
    "            nn.Linear(256, 128),  # 128 + 128 = 256\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(128, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        deep_out = self.deep(x)\n",
    "        wide_out = self.wide(x)\n",
    "        \n",
    "        # Concatenate\n",
    "        combined = torch.cat([deep_out, wide_out], dim=1)\n",
    "        return self.combine(combined)\n",
    "\n",
    "class Model5_Bottleneck(nn.Module):\n",
    "    \"\"\"Bottleneck с Attention механизмом\"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Self-Attention на bottleneck\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, 384),\n",
    "            nn.BatchNorm1d(384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            \n",
    "            nn.Linear(384, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        encoded = self.encoder(x)\n",
    "        \n",
    "        # Attention\n",
    "        attention_weights = self.attention(encoded)\n",
    "        attended = encoded * attention_weights\n",
    "        \n",
    "        # Decode\n",
    "        return self.decoder(attended)\n",
    "\n",
    "class Model6_Transformer(nn.Module):\n",
    "    \"\"\"Transformer-inspired architecture\"\"\"\n",
    "    def __init__(self, input_size, output_size, num_heads=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_size, 512)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=512,\n",
    "            num_heads=num_heads,\n",
    "            dropout=0.2,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(512)\n",
    "        self.norm2 = nn.LayerNorm(512)\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512)\n",
    "        )\n",
    "        \n",
    "        # Output\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(256, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Project input\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # Add sequence dimension for attention\n",
    "        x = x.unsqueeze(1)  # [batch, 1, features]\n",
    "        \n",
    "        # Self-attention with residual\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        \n",
    "        # Remove sequence dimension\n",
    "        x = x.squeeze(1)  # [batch, features]\n",
    "        \n",
    "        return self.output(x)\n",
    "\n",
    "class Model7_EnsembleBlock(nn.Module):\n",
    "    \"\"\"Модель с внутренним ансамблем\"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Три параллельных пути\n",
    "        self.path1 = nn.Sequential(\n",
    "            nn.Linear(input_size, 384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(384, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.path2 = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.path3 = nn.Sequential(\n",
    "            nn.Linear(input_size, 640),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.35),\n",
    "            nn.Linear(640, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Gating mechanism для взвешивания путей\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Final layers\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(128, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Три пути\n",
    "        out1 = self.path1(x)\n",
    "        out2 = self.path2(x)\n",
    "        out3 = self.path3(x)\n",
    "        \n",
    "        # Gating weights\n",
    "        gates = self.gate(x)  # [batch, 3]\n",
    "        \n",
    "        # Weighted combination\n",
    "        combined = (gates[:, 0:1] * out1 + \n",
    "                   gates[:, 1:2] * out2 + \n",
    "                   gates[:, 2:3] * out3)\n",
    "        \n",
    "        return self.output(combined)\n",
    "\n",
    "# Дополнительные улучшенные модели\n",
    "class ImprovedModel(nn.Module):\n",
    "    \"\"\"Улучшенная модель с регуляризацией и skip-connections\"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_sizes=[512, 256, 128], \n",
    "                 dropout_rates=[0.3, 0.2, 0.1], use_layer_norm=True, activation='relu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for i, (hidden_size, dropout_rate) in enumerate(zip(hidden_sizes, dropout_rates)):\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.LayerNorm(hidden_size) if use_layer_norm else nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU() if activation == 'relu' else nn.LeakyReLU(0.1),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(prev_size, output_size)\n",
    "        \n",
    "        # Skip connection от входа к выходу\n",
    "        self.skip = nn.Linear(input_size, output_size) if input_size != output_size else None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        output = self.output(features)\n",
    "        \n",
    "        if self.skip is not None:\n",
    "            output = output + 0.1 * self.skip(x)  # Малый вес для skip connection\n",
    "        \n",
    "        return output\n",
    "\n",
    "class AdvancedEnsembleModel(nn.Module):\n",
    "    \"\"\"Продвинутая модель с внутренним ансамблем\"\"\"\n",
    "    def __init__(self, input_size, output_size, num_experts=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_experts = num_experts\n",
    "        self.experts = nn.ModuleList([\n",
    "            ImprovedModel(input_size, 256, [512, 384], [0.3, 0.25]) \n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_experts),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        gates = self.gate(x)  # [batch_size, num_experts]\n",
    "        expert_outputs = [expert(x) for expert in self.experts]  # list of [batch_size, 256]\n",
    "        \n",
    "        # Правильное взвешивание экспертов\n",
    "        combined = torch.zeros_like(expert_outputs[0])\n",
    "        for i in range(self.num_experts):\n",
    "            # gates[:, i:i+1] - [batch_size, 1], expert_outputs[i] - [batch_size, 256]\n",
    "            combined += gates[:, i:i+1] * expert_outputs[i]\n",
    "        \n",
    "        return self.output(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a906f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_features(df, base_cols_ref=None):\n",
    "    \"\"\"Создает расширенный набор технических индикаторов\"\"\"\n",
    "    features = df.copy()\n",
    "    \n",
    "    if base_cols_ref is not None:\n",
    "        numeric_cols = [c for c in base_cols_ref if c in df.columns]\n",
    "    else:\n",
    "        numeric_cols = [c for c in df.columns \n",
    "                       if c not in ['date_id', 'is_scored'] \n",
    "                       and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        try:\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                # ===== БАЗОВЫЕ RETURNS =====\n",
    "                features[f'{col}_return_1d'] = df[col].pct_change(1)\n",
    "                features[f'{col}_return_5d'] = df[col].pct_change(5)\n",
    "                features[f'{col}_return_20d'] = df[col].pct_change(20)\n",
    "                features[f'{col}_return_60d'] = df[col].pct_change(60)\n",
    "                \n",
    "                # ===== SIMPLE MOVING AVERAGES (SMA) =====\n",
    "                features[f'{col}_ma_5'] = df[col].rolling(5, min_periods=1).mean()\n",
    "                features[f'{col}_ma_10'] = df[col].rolling(10, min_periods=1).mean()\n",
    "                features[f'{col}_ma_20'] = df[col].rolling(20, min_periods=1).mean()\n",
    "                features[f'{col}_ma_60'] = df[col].rolling(60, min_periods=1).mean()\n",
    "                \n",
    "                # ===== EXPONENTIAL MOVING AVERAGES (EMA) =====\n",
    "                features[f'{col}_ema_5'] = df[col].ewm(span=5, adjust=False).mean()\n",
    "                features[f'{col}_ema_10'] = df[col].ewm(span=10, adjust=False).mean()\n",
    "                features[f'{col}_ema_20'] = df[col].ewm(span=20, adjust=False).mean()\n",
    "                \n",
    "                # ===== MA CROSSOVERS =====\n",
    "                features[f'{col}_ma_5_20_diff'] = features[f'{col}_ma_5'] - features[f'{col}_ma_20']\n",
    "                features[f'{col}_ma_10_60_diff'] = features[f'{col}_ma_10'] - features[f'{col}_ma_60']\n",
    "                features[f'{col}_ema_5_20_diff'] = features[f'{col}_ema_5'] - features[f'{col}_ema_20']\n",
    "                \n",
    "                # ===== PRICE TO MA DISTANCE =====\n",
    "                features[f'{col}_to_ma_5'] = (df[col] - features[f'{col}_ma_5']) / features[f'{col}_ma_5']\n",
    "                features[f'{col}_to_ma_20'] = (df[col] - features[f'{col}_ma_20']) / features[f'{col}_ma_20']\n",
    "                features[f'{col}_to_ema_10'] = (df[col] - features[f'{col}_ema_10']) / features[f'{col}_ema_10']\n",
    "                \n",
    "                # ===== VOLATILITY =====\n",
    "                features[f'{col}_std_5'] = df[col].rolling(5, min_periods=1).std()\n",
    "                features[f'{col}_std_20'] = df[col].rolling(20, min_periods=1).std()\n",
    "                features[f'{col}_std_60'] = df[col].rolling(60, min_periods=1).std()\n",
    "                \n",
    "                # ===== BOLLINGER BANDS =====\n",
    "                ma_20 = features[f'{col}_ma_20']\n",
    "                std_20 = features[f'{col}_std_20']\n",
    "                features[f'{col}_bb_upper'] = ma_20 + 2 * std_20\n",
    "                features[f'{col}_bb_lower'] = ma_20 - 2 * std_20\n",
    "                features[f'{col}_bb_width'] = (features[f'{col}_bb_upper'] - features[f'{col}_bb_lower']) / ma_20\n",
    "                features[f'{col}_bb_position'] = (df[col] - features[f'{col}_bb_lower']) / (features[f'{col}_bb_upper'] - features[f'{col}_bb_lower'])\n",
    "                \n",
    "                # ===== RSI (Relative Strength Index) =====\n",
    "                delta = df[col].diff()\n",
    "                gain = delta.where(delta > 0, 0).rolling(window=14, min_periods=1).mean()\n",
    "                loss = -delta.where(delta < 0, 0).rolling(window=14, min_periods=1).mean()\n",
    "                rs = gain / loss\n",
    "                features[f'{col}_rsi_14'] = 100 - (100 / (1 + rs))\n",
    "                \n",
    "                # ===== MACD =====\n",
    "                ema_12 = df[col].ewm(span=12, adjust=False).mean()\n",
    "                ema_26 = df[col].ewm(span=26, adjust=False).mean()\n",
    "                features[f'{col}_macd'] = ema_12 - ema_26\n",
    "                features[f'{col}_macd_signal'] = features[f'{col}_macd'].ewm(span=9, adjust=False).mean()\n",
    "                features[f'{col}_macd_diff'] = features[f'{col}_macd'] - features[f'{col}_macd_signal']\n",
    "                \n",
    "                # ===== MOMENTUM =====\n",
    "                features[f'{col}_momentum_5'] = df[col] - df[col].shift(5)\n",
    "                features[f'{col}_momentum_10'] = df[col] - df[col].shift(10)\n",
    "                features[f'{col}_momentum_20'] = df[col] - df[col].shift(20)\n",
    "                \n",
    "                # ===== RATE OF CHANGE (ROC) =====\n",
    "                features[f'{col}_roc_5'] = ((df[col] - df[col].shift(5)) / df[col].shift(5)) * 100\n",
    "                features[f'{col}_roc_10'] = ((df[col] - df[col].shift(10)) / df[col].shift(10)) * 100\n",
    "                features[f'{col}_roc_20'] = ((df[col] - df[col].shift(20)) / df[col].shift(20)) * 100\n",
    "                \n",
    "                # ===== LAG FEATURES =====\n",
    "                features[f'{col}_lag_1'] = df[col].shift(1)\n",
    "                features[f'{col}_lag_2'] = df[col].shift(2)\n",
    "                features[f'{col}_lag_3'] = df[col].shift(3)\n",
    "                features[f'{col}_lag_5'] = df[col].shift(5)\n",
    "                \n",
    "                # ===== MIN/MAX OVER WINDOWS =====\n",
    "                features[f'{col}_max_5'] = df[col].rolling(5, min_periods=1).max()\n",
    "                features[f'{col}_min_5'] = df[col].rolling(5, min_periods=1).min()\n",
    "                features[f'{col}_max_20'] = df[col].rolling(20, min_periods=1).max()\n",
    "                features[f'{col}_min_20'] = df[col].rolling(20, min_periods=1).min()\n",
    "                \n",
    "                # Distance to recent high/low\n",
    "                features[f'{col}_dist_to_max_20'] = (df[col] - features[f'{col}_max_20']) / features[f'{col}_max_20']\n",
    "                features[f'{col}_dist_to_min_20'] = (df[col] - features[f'{col}_min_20']) / features[f'{col}_min_20']\n",
    "                \n",
    "                # ===== ACCELERATION (second derivative) =====\n",
    "                features[f'{col}_acceleration'] = df[col].diff().diff()\n",
    "                \n",
    "                # ===== Z-SCORE (standardized price) =====\n",
    "                rolling_mean = df[col].rolling(20, min_periods=1).mean()\n",
    "                rolling_std = df[col].rolling(20, min_periods=1).std()\n",
    "                features[f'{col}_zscore'] = (df[col] - rolling_mean) / rolling_std\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def add_spread_features(train, features, target_pairs):\n",
    "    \"\"\"Добавляет фичи для спредов между парами инструментов\"\"\"\n",
    "    print(\"Добавление spread features...\")\n",
    "    \n",
    "    spread_count = 0\n",
    "    for idx, row in target_pairs.iterrows():\n",
    "        pair = row['pair']\n",
    "        \n",
    "        # Парсим пару (например: \"LME_CA_Close - US_Stock_CCJ_adj_close\")\n",
    "        if ' - ' in str(pair):\n",
    "            try:\n",
    "                col_a, col_b = pair.split(' - ')\n",
    "                col_a = col_a.strip()\n",
    "                col_b = col_b.strip()\n",
    "                \n",
    "                if col_a in train.columns and col_b in train.columns:\n",
    "                    # Вычисляем spread\n",
    "                    spread = train[col_a] - train[col_b]\n",
    "                    prefix = f'spread_{idx}'\n",
    "                    \n",
    "                    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                        # MA для spread\n",
    "                        features[f'{prefix}_ma_5'] = spread.rolling(5, min_periods=1).mean()\n",
    "                        features[f'{prefix}_ma_20'] = spread.rolling(20, min_periods=1).mean()\n",
    "                        \n",
    "                        # Volatility spread\n",
    "                        features[f'{prefix}_std_5'] = spread.rolling(5, min_periods=1).std()\n",
    "                        features[f'{prefix}_std_20'] = spread.rolling(20, min_periods=1).std()\n",
    "                        \n",
    "                        # Momentum spread\n",
    "                        features[f'{prefix}_momentum_5'] = spread - spread.shift(5)\n",
    "                        features[f'{prefix}_return_5d'] = spread.pct_change(5)\n",
    "                        \n",
    "                        # Z-score spread\n",
    "                        rolling_mean = spread.rolling(20, min_periods=1).mean()\n",
    "                        rolling_std = spread.rolling(20, min_periods=1).std()\n",
    "                        features[f'{prefix}_zscore'] = (spread - rolling_mean) / rolling_std\n",
    "                        \n",
    "                        # Ratio features\n",
    "                        ratio = train[col_a] / (train[col_b] + 1e-10)  # Избегаем деления на 0\n",
    "                        features[f'{prefix}_ratio'] = ratio\n",
    "                        features[f'{prefix}_ratio_ma_5'] = ratio.rolling(5, min_periods=1).mean()\n",
    "                        \n",
    "                        spread_count += 1\n",
    "            except Exception as e:\n",
    "                # Тихо пропускаем проблемные пары\n",
    "                pass\n",
    "    \n",
    "    print(f\"Spread features добавлены для {spread_count} пар. Всего фичей: {len(features.columns)}\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ca397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_smart_features(df, base_cols_ref=None):\n",
    "    \"\"\"Умное создание признаков с фильтрацией\"\"\"\n",
    "    features = df.copy()\n",
    "    \n",
    "    if base_cols_ref is not None:\n",
    "        numeric_cols = [c for c in base_cols_ref if c in df.columns]\n",
    "    else:\n",
    "        numeric_cols = [c for c in df.columns \n",
    "                       if c not in ['date_id', 'is_scored'] \n",
    "                       and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    \n",
    "    # Фильтруем колонки с достаточной вариативностью\n",
    "    useful_cols = []\n",
    "    for col in numeric_cols:\n",
    "        if len(df[col]) > 10:  # Достаточно данных\n",
    "            std_val = df[col].std()\n",
    "            unique_vals = df[col].nunique()\n",
    "            if std_val > 1e-6 and unique_vals > 5 and not np.isnan(std_val):\n",
    "                useful_cols.append(col)\n",
    "    \n",
    "    print(f\"Используется {len(useful_cols)} из {len(numeric_cols)} колонок\")\n",
    "    \n",
    "    # Ограничиваем количество колонок для производительности\n",
    "    useful_cols = useful_cols[:min(200, len(useful_cols))]\n",
    "    \n",
    "    for col in useful_cols:\n",
    "        try:\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                # Базовые трансформации\n",
    "                features[f'{col}_return_1d'] = df[col].pct_change(1)\n",
    "                features[f'{col}_return_5d'] = df[col].pct_change(5)\n",
    "                \n",
    "                # Скользящие статистики\n",
    "                for window in [5, 10, 20]:\n",
    "                    features[f'{col}_ma_{window}'] = df[col].rolling(window, min_periods=1).mean()\n",
    "                    features[f'{col}_std_{window}'] = df[col].rolling(window, min_periods=1).std()\n",
    "                \n",
    "                # Ключевые комбинации\n",
    "                features[f'{col}_momentum_5'] = df[col] / df[col].shift(5) - 1\n",
    "                features[f'{col}_volatility_20'] = df[col].rolling(20).std() / df[col].rolling(20).mean()\n",
    "                \n",
    "                # Z-score\n",
    "                rolling_mean = df[col].rolling(20, min_periods=1).mean()\n",
    "                rolling_std = df[col].rolling(20, min_periods=1).std()\n",
    "                features[f'{col}_zscore'] = (df[col] - rolling_mean) / rolling_std\n",
    "                \n",
    "                # Lag features\n",
    "                for lag in [1, 2, 3]:\n",
    "                    features[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Заполняем NaN\n",
    "    features = features.fillna(0)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_features(df):\n",
    "    global base_cols, feature_cols\n",
    "    \n",
    "    if base_cols is None:\n",
    "        numeric_cols = [c for c in df.columns \n",
    "                       if c not in ['date_id', 'is_scored'] \n",
    "                       and pd.api.types.is_numeric_dtype(df[c])]\n",
    "        test_features = create_smart_features(df, base_cols_ref=numeric_cols)\n",
    "    else:\n",
    "        test_features = create_smart_features(df, base_cols_ref=base_cols)\n",
    "    \n",
    "    if feature_cols is None:\n",
    "        feature_cols = [c for c in test_features.columns \n",
    "                       if c != 'date_id' and pd.api.types.is_numeric_dtype(test_features[c])]\n",
    "    \n",
    "    X_test = np.zeros((len(df), len(feature_cols)))\n",
    "    \n",
    "    for i, col in enumerate(feature_cols):\n",
    "        if col in test_features.columns:\n",
    "            X_test[:, i] = test_features[col].fillna(0).values\n",
    "    \n",
    "    X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpearmanLoss(nn.Module):\n",
    "    \"\"\"Loss функция для оптимизации Spearman Correlation\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Для каждого target отдельно\n",
    "        batch_size = pred.shape[0]\n",
    "        num_targets = pred.shape[1]\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        for i in range(num_targets):\n",
    "            pred_col = pred[:, i]\n",
    "            target_col = target[:, i]\n",
    "            \n",
    "            # Ранжируем\n",
    "            pred_rank = pred_col.argsort().argsort().float()\n",
    "            target_rank = target_col.argsort().argsort().float()\n",
    "            \n",
    "            # Нормализуем\n",
    "            pred_rank = (pred_rank - pred_rank.mean()) / (pred_rank.std() + 1e-6)\n",
    "            target_rank = (target_rank - target_rank.mean()) / (target_rank.std() + 1e-6)\n",
    "            \n",
    "            # Correlation (минимизируем отрицательную)\n",
    "            correlation = (pred_rank * target_rank).mean()\n",
    "            total_loss += -correlation\n",
    "        \n",
    "        return total_loss / num_targets\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Комбинация MSE + Spearman + Direction\"\"\"\n",
    "    def __init__(self, mse_weight=0.3, spearman_weight=0.5, direction_weight=0.2):\n",
    "        super().__init__()\n",
    "        self.mse_weight = mse_weight\n",
    "        self.spearman_weight = spearman_weight\n",
    "        self.direction_weight = direction_weight\n",
    "        \n",
    "        self.mse = nn.HuberLoss()\n",
    "        self.spearman = SpearmanLoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # MSE часть\n",
    "        mse_loss = self.mse(pred, target)\n",
    "        \n",
    "        # Spearman часть\n",
    "        spearman_loss = self.spearman(pred, target)\n",
    "        \n",
    "        # Direction часть\n",
    "        pred_sign = torch.sign(pred)\n",
    "        target_sign = torch.sign(target)\n",
    "        direction_correct = (pred_sign == target_sign).float()\n",
    "        direction_loss = 1 - direction_correct.mean()\n",
    "        \n",
    "        # Комбинируем\n",
    "        total_loss = (self.mse_weight * mse_loss + \n",
    "                     self.spearman_weight * spearman_loss + \n",
    "                     self.direction_weight * direction_loss)\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098224e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X_train_t, y_train_t, X_val_t, y_val_t, input_size, output_size):\n",
    "    \"\"\"Функция для оптимизации гиперпараметров с помощью Optuna\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Предлагаем гиперпараметры\n",
    "        model_type = trial.suggest_categorical('model_type', ['improved', 'transformer'])\n",
    "        lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "        \n",
    "        if model_type == 'improved':\n",
    "            hidden_size1 = trial.suggest_int('hidden_size1', 256, 1024)\n",
    "            hidden_size2 = trial.suggest_int('hidden_size2', 128, 512)\n",
    "            model = ImprovedModel(\n",
    "                input_size, output_size, \n",
    "                hidden_sizes=[hidden_size1, hidden_size2],\n",
    "                dropout_rates=[dropout_rate, dropout_rate * 0.8]\n",
    "            ).to(device)\n",
    "        else:  # transformer\n",
    "            # Для transformer ограничиваем num_heads чтобы embed_dim был кратен\n",
    "            embed_dim = 512  # Фиксируем embed_dim\n",
    "            possible_heads = [2, 4, 8, 16]  # Только делители 512\n",
    "            num_heads = trial.suggest_categorical('num_heads', possible_heads)\n",
    "            num_layers = trial.suggest_int('num_layers', 1, 4)\n",
    "            model = TransformerModel(input_size, output_size, num_heads=num_heads, num_layers=num_layers).to(device)\n",
    "        \n",
    "        # Оптимизатор и scheduler\n",
    "        optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'AdamW', 'RMSprop'])\n",
    "        if optimizer_name == 'Adam':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_name == 'AdamW':\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "        # criterion = nn.HuberLoss(delta=1.0)\n",
    "        criterion = SpearmanLoss(\n",
    "            mse_weight=0.3,       # MSE для magnitude\n",
    "            spearman_weight=0.5,  # Spearman для ranking\n",
    "            direction_weight=0.2  # Direction для знака\n",
    "        )\n",
    "        \n",
    "        # Обучение\n",
    "        model.train()\n",
    "        best_val_loss = float('inf')\n",
    "        patience = 10\n",
    "        epochs_no_improve = 0\n",
    "        \n",
    "        for epoch in range(80):  # Короткое обучение для оптимизации\n",
    "            optimizer.zero_grad()\n",
    "            train_outputs = model(X_train_t)\n",
    "            train_loss = criterion(train_outputs, y_train_t)\n",
    "            train_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Валидация\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(X_val_t)\n",
    "                val_loss = criterion(val_outputs, y_val_t)\n",
    "            \n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss.item()\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    break\n",
    "            \n",
    "            model.train()\n",
    "        \n",
    "        # Очистка памяти\n",
    "        del model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return best_val_loss\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Логируем ошибку и пропускаем trial\n",
    "        print(f\"Trial {trial.number} failed: {e}\")\n",
    "        return float('inf')  # Возвращаем плохое значение для неудачного trial\n",
    "\n",
    "def optimize_hyperparameters(X_train_t, y_train_t, X_val_t, y_val_t, input_size, output_size, n_trials=20):\n",
    "    \"\"\"Оптимизация гиперпараметров для всех моделей\"\"\"\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42)\n",
    "    )\n",
    "    \n",
    "    print(f\"Запуск оптимизации гиперпараметров ({n_trials} trials)...\")\n",
    "    \n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, X_train_t, y_train_t, X_val_t, y_val_t, input_size, output_size),\n",
    "        n_trials=n_trials,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nЛучшие гиперпараметры:\")\n",
    "    for key, value in study.best_trial.params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"Лучшее значение loss: {study.best_value:.6f}\")\n",
    "    \n",
    "    return study.best_params\n",
    "\n",
    "def create_model_from_params(params, input_size, output_size):\n",
    "    \"\"\"Создание модели на основе оптимизированных параметров\"\"\"\n",
    "    \n",
    "    model_type = params['model_type']\n",
    "    \n",
    "    if model_type == 'improved':\n",
    "        return ImprovedModel(\n",
    "            input_size, output_size,\n",
    "            hidden_sizes=[params['hidden_size1'], params['hidden_size2']],\n",
    "            dropout_rates=[params['dropout_rate'], params['dropout_rate'] * 0.8]\n",
    "        ).to(device)\n",
    "    else:  # transformer\n",
    "        return TransformerModel(\n",
    "            input_size, output_size,\n",
    "            num_heads=params['num_heads'],\n",
    "            num_layers=params['num_layers']\n",
    "        ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf3f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_training_loop(model, X_train_t, y_train_t, X_val_t, y_val_t, params, epochs=300):\n",
    "    \"\"\"Улучшенный цикл обучения с оптимизированными параметрами\"\"\"\n",
    "    \n",
    "    criterion = nn.HuberLoss(delta=1.0)\n",
    "    \n",
    "    # Оптимизатор\n",
    "    optimizer_name = params.get('optimizer', 'AdamW')\n",
    "    lr = params.get('lr', 0.001)\n",
    "    weight_decay = params.get('weight_decay', 1e-5)\n",
    "    \n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'AdamW':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Убрал verbose параметр для совместимости\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=8, factor=0.5)\n",
    "    \n",
    "    # Ранняя остановка\n",
    "    patience = 50\n",
    "    best_val_loss = float('inf')\n",
    "    best_r2 = -float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping на эпохе {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Обучение\n",
    "        optimizer.zero_grad()\n",
    "        train_outputs = model(X_train_t)\n",
    "        train_loss = criterion(train_outputs, y_train_t)\n",
    "        train_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_t)\n",
    "            val_loss = criterion(val_outputs, y_val_t)\n",
    "            \n",
    "            val_preds = val_outputs.cpu().numpy()\n",
    "            val_true = y_val_t.cpu().numpy()\n",
    "            \n",
    "            mae = mean_absolute_error(val_true.flatten(), val_preds.flatten())\n",
    "            direction_correct = np.mean(np.sign(val_preds) == np.sign(val_true))\n",
    "            r2 = r2_score(val_true.flatten(), val_preds.flatten())\n",
    "            \n",
    "            train_preds = train_outputs.detach().cpu().numpy()\n",
    "            train_true = y_train_t.cpu().numpy()\n",
    "            train_r2 = r2_score(train_true.flatten(), train_preds.flatten())\n",
    "        \n",
    "        # Обновление лучших результатов\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            best_r2 = r2\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            epochs_no_improve = 0\n",
    "            improvement_msg = \"✓ УЛУЧШЕНИЕ\"\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            improvement_msg = f\"NO IMPROVE ({epochs_no_improve}/{patience})\"\n",
    "        \n",
    "        model.train()\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Логирование\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0 or epochs_no_improve >= patience:\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Ep {epoch+1:3d}/{epochs} | \"\n",
    "                  f\"TrL: {train_loss.item():.6f} | \"\n",
    "                  f\"VaL: {val_loss.item():.6f} | \"\n",
    "                  f\"MAE: {mae:.6f} | \"\n",
    "                  f\"R²_tr: {train_r2:7.4f} | \"\n",
    "                  f\"R²_val: {r2:7.4f} | \"\n",
    "                  f\"Dir: {direction_correct:.4f} | \"\n",
    "                  f\"LR: {lr:.6f} | {improvement_msg}\")\n",
    "        \n",
    "        # Периодическая очистка памяти\n",
    "        if epoch % 50 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Загрузка лучших весов\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    return model, best_val_loss, best_r2\n",
    "\n",
    "def calculate_dynamic_weights(models, X_val_t, y_val_t):\n",
    "    \"\"\"Вычисление весов на основе производительности на валидации\"\"\"\n",
    "    weights = []\n",
    "    performances = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for model in models:\n",
    "            pred = model(X_val_t)\n",
    "            # Используем несколько метрик\n",
    "            mae = nn.L1Loss()(pred, y_val_t).item()\n",
    "            mse = nn.MSELoss()(pred, y_val_t).item()\n",
    "            \n",
    "            # Комбинированная оценка (чем меньше ошибки - тем лучше)\n",
    "            score = 1.0 / (mae + 0.1 * mse + 1e-8)\n",
    "            performances.append(score)\n",
    "    \n",
    "    # Softmax для весов\n",
    "    performances = np.array(performances)\n",
    "    weights = np.exp(performances - np.max(performances))\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15eb33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models():\n",
    "    global models, scaler, feature_cols, base_cols, model_val_losses, is_initialized, device\n",
    "    \n",
    "    if is_initialized:\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"ИНИЦИАЛИЗАЦИЯ И ОБУЧЕНИЕ МОДЕЛЕЙ НА {device}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ===== ЗАГРУЗКА ДАННЫХ =====\n",
    "    train = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/train.csv')\n",
    "    train_labels = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/train_labels.csv')\n",
    "    \n",
    "    # Читаем информацию о парах инструментов\n",
    "    try:\n",
    "        target_pairs = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/target_pairs.csv')\n",
    "        print(\"\\nИнформация о targets:\")\n",
    "        print(f\"Всего targets: {len(target_pairs)}\")\n",
    "        for lag in sorted(target_pairs['lag'].unique()):\n",
    "            count = len(target_pairs[target_pairs['lag'] == lag])\n",
    "            print(f\"  Lag {lag}: {count} targets\")\n",
    "    except:\n",
    "        print(\"\\nВнимание: target_pairs.csv не найден, продолжаем без него\")\n",
    "        target_pairs = None\n",
    "    \n",
    "    # ===== БАЗОВЫЕ КОЛОНКИ =====\n",
    "    base_cols = [c for c in train.columns \n",
    "                 if c not in ['date_id'] \n",
    "                 and pd.api.types.is_numeric_dtype(train[c])]\n",
    "    \n",
    "    print(f\"\\nБазовых колонок: {len(base_cols)}\")\n",
    "    print(\"Создание признаков...\")\n",
    "    \n",
    "    # ===== FEATURE ENGINEERING =====\n",
    "    train_features = create_enhanced_features(train, base_cols_ref=base_cols)\n",
    "    \n",
    "    # Добавляем spread features если есть target_pairs\n",
    "    if target_pairs is not None:\n",
    "        train_features = add_spread_features(train, train_features, target_pairs)\n",
    "    \n",
    "    feature_cols = [c for c in train_features.columns \n",
    "                   if c != 'date_id' and pd.api.types.is_numeric_dtype(train_features[c])]\n",
    "    \n",
    "    print(f\"Создано {len(feature_cols)} признаков\")\n",
    "    \n",
    "    # ===== ПОДГОТОВКА ДАННЫХ =====\n",
    "    target_cols = [f'target_{i}' for i in range(424)]\n",
    "    \n",
    "    X_train = train_features[feature_cols].fillna(0).values\n",
    "    y_train = train_labels[target_cols].fillna(0).values\n",
    "    \n",
    "    X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    y_train = np.nan_to_num(y_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # ===== ПЕРЕНОС НА GPU =====\n",
    "    X_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    y_tensor = torch.FloatTensor(y_train).to(device)\n",
    "    \n",
    "    split_idx = int(len(X_train_scaled) * 0.9)\n",
    "    X_train_t, X_val_t = X_tensor[:split_idx], X_tensor[split_idx:]\n",
    "    y_train_t, y_val_t = y_tensor[:split_idx], y_tensor[split_idx:]\n",
    "    \n",
    "    print(f\"Train: {len(X_train_t)}, Validation: {len(X_val_t)}\")\n",
    "    \n",
    "    # ===== КОНФИГУРАЦИЯ МОДЕЛЕЙ (БЕЗ MODEL 3!) =====\n",
    "    model_configs = [\n",
    "        (Model1_Deep, \"Deep+LayerNorm\", 1000),\n",
    "        # (Model5_Bottleneck, \"Bottleneck+Attention\", 1000),\n",
    "        (Model6_Transformer, \"Transformer\", 1000),\n",
    "        (Model7_EnsembleBlock, \"EnsembleBlock+Gating\", 1000),\n",
    "    ]\n",
    "    \n",
    "    # ===== ОБУЧЕНИЕ МОДЕЛЕЙ =====\n",
    "    for i, (ModelClass, name, epochs) in enumerate(model_configs):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"МОДЕЛЬ {i+1}/{len(model_configs)}: {name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        try:\n",
    "            # Создание модели\n",
    "            model = ModelClass(X_train_scaled.shape[1], 424).to(device)\n",
    "            \n",
    "            criterion = nn.HuberLoss(delta=1.0)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, 'min', patience=5, factor=0.5\n",
    "            )\n",
    "            \n",
    "            # Early Stopping параметры\n",
    "            patience = 15\n",
    "            best_val_loss = float('inf')\n",
    "            best_r2 = -float('inf')\n",
    "            best_model_state = None\n",
    "            epochs_no_improve = 0\n",
    "            early_stop = False\n",
    "            \n",
    "            model.train()\n",
    "            for epoch in range(epochs):\n",
    "                if early_stop:\n",
    "                    print(f\"Early stopping на эпохе {epoch+1}\")\n",
    "                    break\n",
    "                \n",
    "                # Training step\n",
    "                optimizer.zero_grad()\n",
    "                train_outputs = model(X_train_t)\n",
    "                train_loss = criterion(train_outputs, y_train_t)\n",
    "                train_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Validation step\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_outputs = model(X_val_t)\n",
    "                    val_loss = criterion(val_outputs, y_val_t)\n",
    "                    \n",
    "                    val_preds = val_outputs.cpu().numpy()\n",
    "                    val_true = y_val_t.cpu().numpy()\n",
    "                    \n",
    "                    mae = mean_absolute_error(val_true.flatten(), val_preds.flatten())\n",
    "                    direction_correct = np.mean(np.sign(val_preds) == np.sign(val_true))\n",
    "                    r2 = r2_score(val_true.flatten(), val_preds.flatten())\n",
    "                    \n",
    "                    train_preds = train_outputs.detach().cpu().numpy()\n",
    "                    train_true = y_train_t.cpu().numpy()\n",
    "                    train_r2 = r2_score(train_true.flatten(), train_preds.flatten())\n",
    "                \n",
    "                # Early Stopping логика\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_r2 = r2\n",
    "                    best_model_state = model.state_dict().copy()\n",
    "                    epochs_no_improve = 0\n",
    "                    improvement_msg = \"✓ УЛУЧШЕНИЕ\"\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    improvement_msg = f\"NO IMPROVE ({epochs_no_improve}/{patience})\"\n",
    "                    \n",
    "                    if epochs_no_improve >= patience:\n",
    "                        early_stop = True\n",
    "                \n",
    "                model.train()\n",
    "                scheduler.step(val_loss)\n",
    "                \n",
    "                # Логирование\n",
    "                if (epoch + 1) % 5 == 0 or epoch == 0 or early_stop:\n",
    "                    lr = optimizer.param_groups[0]['lr']\n",
    "                    print(f\"Ep {epoch+1:3d}/{epochs} | \"\n",
    "                          f\"TrL: {train_loss.item():.6f} | \"\n",
    "                          f\"VaL: {val_loss.item():.6f} | \"\n",
    "                          f\"MAE: {mae:.6f} | \"\n",
    "                          f\"R²_tr: {train_r2:7.4f} | \"\n",
    "                          f\"R²_val: {r2:7.4f} | \"\n",
    "                          f\"Dir: {direction_correct:.4f} | \"\n",
    "                          f\"LR: {lr:.6f} | {improvement_msg}\")\n",
    "            \n",
    "            # Загружаем лучшие веса\n",
    "            if best_model_state is not None:\n",
    "                model.load_state_dict(best_model_state)\n",
    "            \n",
    "            model.eval()\n",
    "            models.append(model)\n",
    "            model_val_losses.append(best_val_loss.item())\n",
    "            \n",
    "            print(f\"Завершена. Best Val Loss: {best_val_loss:.6f}, Best R²: {best_r2:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ОШИБКА при обучении модели {name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "        \n",
    "        # Очистка GPU памяти\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # ===== ПРОВЕРКА ЧТО ХОТЬ ЧТО-ТО ОБУЧИЛОСЬ =====\n",
    "    if not models:\n",
    "        raise Exception(\"❌ НИ ОДНА МОДЕЛЬ НЕ БЫЛА УСПЕШНО ОБУЧЕНА!\")\n",
    "    \n",
    "    # ===== ВЫЧИСЛЕНИЕ ВЕСОВ ДЛЯ АНСАМБЛЯ =====\n",
    "    weights = 1.0 / np.array(model_val_losses)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ВЕСА МОДЕЛЕЙ ДЛЯ ВЗВЕШЕННОГО АНСАМБЛЯ:\")\n",
    "    for i, (config, weight, val_loss) in enumerate(zip(model_configs, weights, model_val_losses)):\n",
    "        if i < len(models):\n",
    "            print(f\"  Модель {i+1} ({config[1]}): вес={weight:.4f}, val_loss={val_loss:.6f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    is_initialized = True\n",
    "\n",
    "\n",
    "# ===== ФУНКЦИЯ ДЛЯ ДОБАВЛЕНИЯ SPREAD FEATURES =====\n",
    "def add_spread_features(train, features, target_pairs):\n",
    "    \"\"\"Добавляет фичи для спредов между парами инструментов\"\"\"\n",
    "    print(\"Добавление spread features...\")\n",
    "    \n",
    "    for idx, row in target_pairs.iterrows():\n",
    "        pair = row['pair']\n",
    "        \n",
    "        # Парсим пару (например: \"LME_CA_Close - US_Stock_CCJ_adj_close\")\n",
    "        if ' - ' in str(pair):\n",
    "            try:\n",
    "                col_a, col_b = pair.split(' - ')\n",
    "                col_a = col_a.strip()\n",
    "                col_b = col_b.strip()\n",
    "                \n",
    "                if col_a in train.columns and col_b in train.columns:\n",
    "                    # Вычисляем spread\n",
    "                    spread = train[col_a] - train[col_b]\n",
    "                    prefix = f'spread_{idx}'\n",
    "                    \n",
    "                    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                        # MA для spread\n",
    "                        features[f'{prefix}_ma_5'] = spread.rolling(5, min_periods=1).mean()\n",
    "                        features[f'{prefix}_ma_20'] = spread.rolling(20, min_periods=1).mean()\n",
    "                        \n",
    "                        # Volatility spread\n",
    "                        features[f'{prefix}_std_5'] = spread.rolling(5, min_periods=1).std()\n",
    "                        features[f'{prefix}_std_20'] = spread.rolling(20, min_periods=1).std()\n",
    "                        \n",
    "                        # Momentum spread\n",
    "                        features[f'{prefix}_momentum_5'] = spread - spread.shift(5)\n",
    "                        features[f'{prefix}_return_5d'] = spread.pct_change(5)\n",
    "                        \n",
    "                        # Z-score spread\n",
    "                        rolling_mean = spread.rolling(20, min_periods=1).mean()\n",
    "                        rolling_std = spread.rolling(20, min_periods=1).std()\n",
    "                        features[f'{prefix}_zscore'] = (spread - rolling_mean) / rolling_std\n",
    "                        \n",
    "                        # Ratio features\n",
    "                        ratio = train[col_a] / train[col_b]\n",
    "                        features[f'{prefix}_ratio'] = ratio\n",
    "                        features[f'{prefix}_ratio_ma_5'] = ratio.rolling(5, min_periods=1).mean()\n",
    "            except Exception as e:\n",
    "                # Тихо пропускаем проблемные пары\n",
    "                pass\n",
    "    \n",
    "    print(f\"Spread features добавлены. Всего фичей: {len(features.columns)}\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test, label_lags_1_batch, label_lags_2_batch, label_lags_3_batch, label_lags_4_batch):\n",
    "    global models, scaler, feature_cols, model_val_losses, is_initialized, device\n",
    "    \n",
    "    if not is_initialized:\n",
    "        initialize_models()\n",
    "    \n",
    "    try:\n",
    "        test_pd = test.to_pandas()\n",
    "        X_test = prepare_features(test_pd)\n",
    "        X_test_scaled = scaler.transform(X_test[-1:])\n",
    "        \n",
    "        X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "        \n",
    "        # ВЗВЕШЕННЫЙ АНСАМБЛЬ вместо простого усреднения\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for model in models:\n",
    "                pred = model(X_test_tensor)\n",
    "                pred_cpu = pred.cpu().numpy()[0]\n",
    "                all_preds.append(pred_cpu)\n",
    "        \n",
    "        # Вычисляем веса (обратно пропорционально val loss)\n",
    "        weights = 1.0 / np.array(model_val_losses)\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        # Взвешенное усреднение\n",
    "        predictions = np.average(all_preds, axis=0, weights=weights)\n",
    "        predictions = np.clip(predictions, -0.1, 0.1)\n",
    "        predictions = np.nan_to_num(predictions, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        return pl.DataFrame({f'target_{i}': [float(predictions[i])] for i in range(NUM_TARGET_COLUMNS)})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка predict: {e}\")\n",
    "        return pl.DataFrame({f'target_{i}': [0.0] for i in range(NUM_TARGET_COLUMNS)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def efficient_predict_batch(test_data, models, weights=None, batch_size=1024):\n",
    "    \"\"\"Эффективное предсказание с батчингом\"\"\"\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(models)) / len(models)\n",
    "    \n",
    "    # Если test_data уже pandas DataFrame, используем как есть\n",
    "    if hasattr(test_data, 'to_pandas'):\n",
    "        test_pd = test_data.to_pandas()\n",
    "    else:\n",
    "        test_pd = test_data\n",
    "    \n",
    "    X_test = prepare_features(test_pd)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Батчинг для больших данных\n",
    "    all_predictions = []\n",
    "    \n",
    "    for i in range(0, len(X_test_scaled), batch_size):\n",
    "        batch = X_test_scaled[i:i+batch_size]\n",
    "        X_batch_tensor = torch.FloatTensor(batch).to(device)\n",
    "        \n",
    "        batch_preds = []\n",
    "        for model, weight in zip(models, weights):\n",
    "            pred = model(X_batch_tensor).cpu().numpy()\n",
    "            batch_preds.append(pred * weight)\n",
    "        \n",
    "        batch_ensemble = np.sum(batch_preds, axis=0)\n",
    "        all_predictions.append(batch_ensemble)\n",
    "    \n",
    "    predictions = np.vstack(all_predictions)\n",
    "    predictions = np.clip(predictions, -0.1, 0.1)\n",
    "    predictions = np.nan_to_num(predictions, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def create_submission_file():\n",
    "    global models, scaler, feature_cols, is_initialized, device, ensemble_weights_global\n",
    "    \n",
    "    print(\"\\nСоздание submission.parquet...\")\n",
    "    \n",
    "    if not is_initialized:\n",
    "        raise Exception(\"Модели не инициализированы!\")\n",
    "    \n",
    "    test = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/test.csv')\n",
    "    \n",
    "    print(\"Генерация предсказаний...\")\n",
    "    \n",
    "    # Используем батчинговое предсказание\n",
    "    predictions = efficient_predict_batch(test, models, ensemble_weights_global)\n",
    "    \n",
    "    submission = pd.DataFrame({'date_id': test['date_id'].values})\n",
    "    for i in range(424):\n",
    "        submission[f'target_{i}'] = predictions[:, i]\n",
    "    \n",
    "    if 'is_scored' in test.columns:\n",
    "        submission = submission[test['is_scored'] == True].reset_index(drop=True)\n",
    "    \n",
    "    submission = submission.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    submission.to_parquet('submission.parquet', index=False, engine='pyarrow')\n",
    "    \n",
    "    print(f\"Готово: {submission.shape}\")\n",
    "    \n",
    "    # Очищаем GPU память\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     global is_initialized\n",
    "    \n",
    "#     print(\"ЗАПУСК УЛУЧШЕННОГО ПАЙПЛАЙНА С OPTUNA\")\n",
    "#     print(\"=\"*50)\n",
    "    \n",
    "#     # Шаг 1: Инициализация и обучение моделей с Optuna\n",
    "#     if not is_initialized:\n",
    "#         print(\"Начало обучения моделей с оптимизацией гиперпараметров...\")\n",
    "#         initialize_models()\n",
    "#     else:\n",
    "#         print(\"Модели уже инициализированы, пропускаем обучение\")\n",
    "    \n",
    "#     # Шаг 2: Создание submission файла\n",
    "#     print(\"Создание submission файла...\")\n",
    "#     create_submission_file()\n",
    "    \n",
    "#     print(\"\\nПАЙПЛАЙН УСПЕШНО ЗАВЕРШЕН!\")\n",
    "\n",
    "# # Запускаем основной пайплайн\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c646d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kaggle_score_correct(predictions, targets):\n",
    "    \"\"\"\n",
    "    ПРАВИЛЬНАЯ метрика соревнования: Modified Sharpe Ratio\n",
    "    \n",
    "    Score = (Mean Spearman Correlation / Std Spearman Correlation) * 100,000\n",
    "    \"\"\"\n",
    "    from scipy.stats import spearmanr\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ВЫЧИСЛЕНИЕ KAGGLE SCORE (Modified Sharpe Ratio)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    correlations = []\n",
    "    failed_targets = 0\n",
    "    \n",
    "    # Для каждого target вычисляем Spearman correlation\n",
    "    for i in range(targets.shape[1]):  # 424 targets\n",
    "        pred_col = predictions[:, i]\n",
    "        true_col = targets[:, i]\n",
    "        \n",
    "        # Проверяем что есть вариация в данных\n",
    "        if len(np.unique(pred_col)) < 2 or len(np.unique(true_col)) < 2:\n",
    "            failed_targets += 1\n",
    "            continue\n",
    "        \n",
    "        # Spearman rank correlation\n",
    "        try:\n",
    "            corr, p_value = spearmanr(pred_col, true_col)\n",
    "            if not np.isnan(corr) and not np.isinf(corr):\n",
    "                correlations.append(corr)\n",
    "            else:\n",
    "                failed_targets += 1\n",
    "        except Exception as e:\n",
    "            failed_targets += 1\n",
    "            continue\n",
    "    \n",
    "    if len(correlations) == 0:\n",
    "        print(\"\\n⚠️ КРИТИЧЕСКАЯ ОШИБКА: Не удалось вычислить ни одной корреляции!\")\n",
    "        print(f\"   Провалено targets: {failed_targets}/{targets.shape[1]}\")\n",
    "        print(\"\\n   Проверка данных:\")\n",
    "        print(f\"   Predictions shape: {predictions.shape}\")\n",
    "        print(f\"   Predictions range: [{predictions.min():.6f}, {predictions.max():.6f}]\")\n",
    "        print(f\"   Predictions unique values: {len(np.unique(predictions))}\")\n",
    "        print(f\"   Targets shape: {targets.shape}\")\n",
    "        print(f\"   Targets range: [{targets.min():.6f}, {targets.max():.6f}]\")\n",
    "        \n",
    "        return {\n",
    "            'kaggle_score': 0.0,\n",
    "            'sharpe_ratio': 0.0,\n",
    "            'mean_correlation': 0.0,\n",
    "            'std_correlation': 0.0,\n",
    "            'median_correlation': 0.0,\n",
    "            'correlations': np.array([])\n",
    "        }\n",
    "    \n",
    "    correlations = np.array(correlations)\n",
    "    \n",
    "    # Статистика\n",
    "    mean_corr = np.mean(correlations)\n",
    "    std_corr = np.std(correlations)\n",
    "    median_corr = np.median(correlations)\n",
    "    min_corr = np.min(correlations)\n",
    "    max_corr = np.max(correlations)\n",
    "    \n",
    "    # Kaggle Score (масштабированный Modified Sharpe Ratio)\n",
    "    if std_corr > 1e-8:  # Защита от деления на 0\n",
    "        sharpe_ratio = mean_corr / std_corr\n",
    "        kaggle_score = sharpe_ratio * 100000  # Масштабирование\n",
    "    else:\n",
    "        sharpe_ratio = 0\n",
    "        kaggle_score = 0\n",
    "    \n",
    "    print(f\"\\nСтатистика Spearman Correlations:\")\n",
    "    print(f\"  Успешно: {len(correlations)}/{targets.shape[1]} targets\")\n",
    "    if failed_targets > 0:\n",
    "        print(f\"  Провалено: {failed_targets} targets (константные значения)\")\n",
    "    print(f\"  Mean:    {mean_corr:.6f}\")\n",
    "    print(f\"  Median:  {median_corr:.6f}\")\n",
    "    print(f\"  Std:     {std_corr:.6f}\")\n",
    "    print(f\"  Min:     {min_corr:.6f}\")\n",
    "    print(f\"  Max:     {max_corr:.6f}\")\n",
    "    \n",
    "    # Распределение\n",
    "    positive = (correlations > 0).sum()\n",
    "    negative = (correlations < 0).sum()\n",
    "    near_zero = (np.abs(correlations) < 0.01).sum()\n",
    "    \n",
    "    print(f\"\\n  Positive correlations: {positive}/{len(correlations)} ({100*positive/len(correlations):.1f}%)\")\n",
    "    print(f\"  Negative correlations: {negative}/{len(correlations)} ({100*negative/len(correlations):.1f}%)\")\n",
    "    print(f\"  Near zero (|r| < 0.01): {near_zero}/{len(correlations)} ({100*near_zero/len(correlations):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"KAGGLE SCORE (Modified Sharpe Ratio):\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Sharpe Ratio:        {sharpe_ratio:.6f}\")\n",
    "    print(f\"  KAGGLE SCORE:        {kaggle_score:.2f}\")\n",
    "    \n",
    "    # Интерпретация\n",
    "    if kaggle_score > 100000:\n",
    "        print(f\"\\n  ✅ ОТЛИЧНЫЙ результат! (> 100,000)\")\n",
    "    elif kaggle_score > 50000:\n",
    "        print(f\"\\n  ✓ Хороший результат (> 50,000)\")\n",
    "    elif kaggle_score > 0:\n",
    "        print(f\"\\n  ⚠ Слабый результат (> 0, но < 50,000)\")\n",
    "    else:\n",
    "        print(f\"\\n  ❌ ПЛОХОЙ результат (отрицательный score)\")\n",
    "        print(f\"     Модель предсказывает в противоположном направлении!\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'kaggle_score': kaggle_score,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'mean_correlation': mean_corr,\n",
    "        'std_correlation': std_corr,\n",
    "        'median_correlation': median_corr,\n",
    "        'min_correlation': min_corr,\n",
    "        'max_correlation': max_corr,\n",
    "        'correlations': correlations,\n",
    "        'successful_targets': len(correlations),\n",
    "        'failed_targets': failed_targets\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6d876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_predictions(predictions, targets):\n",
    "    \"\"\"Диагностика предсказаний для поиска проблем\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ДИАГНОСТИКА ПРЕДСКАЗАНИЙ\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nФорма данных:\")\n",
    "    print(f\"  Predictions: {predictions.shape}\")\n",
    "    print(f\"  Targets: {targets.shape}\")\n",
    "    \n",
    "    print(f\"\\nСтатистика predictions:\")\n",
    "    print(f\"  Mean:   {predictions.mean():.8f}\")\n",
    "    print(f\"  Std:    {predictions.std():.8f}\")\n",
    "    print(f\"  Min:    {predictions.min():.8f}\")\n",
    "    print(f\"  Max:    {predictions.max():.8f}\")\n",
    "    print(f\"  Median: {np.median(predictions):.8f}\")\n",
    "    \n",
    "    print(f\"\\nСтатистика targets:\")\n",
    "    print(f\"  Mean:   {targets.mean():.8f}\")\n",
    "    print(f\"  Std:    {targets.std():.8f}\")\n",
    "    print(f\"  Min:    {targets.min():.8f}\")\n",
    "    print(f\"  Max:    {targets.max():.8f}\")\n",
    "    print(f\"  Median: {np.median(targets):.8f}\")\n",
    "    \n",
    "    # Проверка на константные колонки\n",
    "    const_pred_cols = []\n",
    "    const_target_cols = []\n",
    "    \n",
    "    for i in range(predictions.shape[1]):\n",
    "        if len(np.unique(predictions[:, i])) < 2:\n",
    "            const_pred_cols.append(i)\n",
    "        if len(np.unique(targets[:, i])) < 2:\n",
    "            const_target_cols.append(i)\n",
    "    \n",
    "    if len(const_pred_cols) > 0:\n",
    "        print(f\"\\n⚠️ Константные predictions колонки: {len(const_pred_cols)}/{predictions.shape[1]}\")\n",
    "        print(f\"   Примеры индексов: {const_pred_cols[:10]}\")\n",
    "    else:\n",
    "        print(f\"\\n✅ Нет константных predictions колонок\")\n",
    "    \n",
    "    if len(const_target_cols) > 0:\n",
    "        print(f\"\\n⚠️ Константные target колонки: {len(const_target_cols)}/{targets.shape[1]}\")\n",
    "        print(f\"   Примеры индексов: {const_target_cols[:10]}\")\n",
    "    else:\n",
    "        print(f\"\\n✅ Нет константных target колонок\")\n",
    "    \n",
    "    # Проверка на NaN и Inf\n",
    "    nan_preds = np.isnan(predictions).sum()\n",
    "    inf_preds = np.isinf(predictions).sum()\n",
    "    nan_targets = np.isnan(targets).sum()\n",
    "    inf_targets = np.isinf(targets).sum()\n",
    "    \n",
    "    if nan_preds > 0 or inf_preds > 0:\n",
    "        print(f\"\\n⚠️ Predictions: NaN={nan_preds}, Inf={inf_preds}\")\n",
    "    else:\n",
    "        print(f\"\\n✅ Predictions: Нет NaN/Inf\")\n",
    "    \n",
    "    if nan_targets > 0 or inf_targets > 0:\n",
    "        print(f\"\\n⚠️ Targets: NaN={nan_targets}, Inf={inf_targets}\")\n",
    "    else:\n",
    "        print(f\"\\n✅ Targets: Нет NaN/Inf\")\n",
    "    \n",
    "    # Распределение знаков\n",
    "    pos_preds = (predictions > 0).sum()\n",
    "    neg_preds = (predictions < 0).sum()\n",
    "    zero_preds = (predictions == 0).sum()\n",
    "    \n",
    "    pos_targets = (targets > 0).sum()\n",
    "    neg_targets = (targets < 0).sum()\n",
    "    zero_targets = (targets == 0).sum()\n",
    "    \n",
    "    total = predictions.size\n",
    "    print(f\"\\nРаспределение знаков predictions:\")\n",
    "    print(f\"  Positive: {pos_preds}/{total} ({100*pos_preds/total:.1f}%)\")\n",
    "    print(f\"  Negative: {neg_preds}/{total} ({100*neg_preds/total:.1f}%)\")\n",
    "    print(f\"  Zero:     {zero_preds}/{total} ({100*zero_preds/total:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nРаспределение знаков targets:\")\n",
    "    print(f\"  Positive: {pos_targets}/{total} ({100*pos_targets/total:.1f}%)\")\n",
    "    print(f\"  Negative: {neg_targets}/{total} ({100*neg_targets/total:.1f}%)\")\n",
    "    print(f\"  Zero:     {zero_targets}/{total} ({100*zero_targets/total:.1f}%)\")\n",
    "    \n",
    "    # Проверка вариации по targets\n",
    "    low_variance_preds = 0\n",
    "    low_variance_targets = 0\n",
    "    \n",
    "    for i in range(predictions.shape[1]):\n",
    "        if predictions[:, i].std() < 1e-6:\n",
    "            low_variance_preds += 1\n",
    "        if targets[:, i].std() < 1e-6:\n",
    "            low_variance_targets += 1\n",
    "    \n",
    "    if low_variance_preds > 0:\n",
    "        print(f\"\\n⚠️ Predictions с низкой дисперсией (std < 1e-6): {low_variance_preds}/{predictions.shape[1]}\")\n",
    "    \n",
    "    if low_variance_targets > 0:\n",
    "        print(f\"\\n⚠️ Targets с низкой дисперсией (std < 1e-6): {low_variance_targets}/{targets.shape[1]}\")\n",
    "    \n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "def calculate_kaggle_score_correct(predictions, targets):\n",
    "    \"\"\"\n",
    "    ПРАВИЛЬНАЯ метрика соревнования: Modified Sharpe Ratio\n",
    "    \n",
    "    Score = (Mean Spearman Correlation / Std Spearman Correlation) * 100,000\n",
    "    \"\"\"\n",
    "    from scipy.stats import spearmanr\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ВЫЧИСЛЕНИЕ KAGGLE SCORE (Modified Sharpe Ratio)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    correlations = []\n",
    "    failed_targets = 0\n",
    "    \n",
    "    # Для каждого target вычисляем Spearman correlation\n",
    "    for i in range(targets.shape[1]):  # 424 targets\n",
    "        pred_col = predictions[:, i]\n",
    "        true_col = targets[:, i]\n",
    "        \n",
    "        # Проверяем что есть вариация в данных\n",
    "        if len(np.unique(pred_col)) < 2 or len(np.unique(true_col)) < 2:\n",
    "            failed_targets += 1\n",
    "            continue\n",
    "        \n",
    "        # Spearman rank correlation\n",
    "        try:\n",
    "            corr, p_value = spearmanr(pred_col, true_col)\n",
    "            if not np.isnan(corr) and not np.isinf(corr):\n",
    "                correlations.append(corr)\n",
    "            else:\n",
    "                failed_targets += 1\n",
    "        except Exception as e:\n",
    "            failed_targets += 1\n",
    "            continue\n",
    "    \n",
    "    if len(correlations) == 0:\n",
    "        print(\"\\n⚠️ КРИТИЧЕСКАЯ ОШИБКА: Не удалось вычислить ни одной корреляции!\")\n",
    "        print(f\"   Провалено targets: {failed_targets}/{targets.shape[1]}\")\n",
    "        print(\"\\n   Причины:\")\n",
    "        print(\"   - Все predictions константные для каждого target\")\n",
    "        print(\"   - Все targets константные\")\n",
    "        print(\"   - Недостаточно вариации в данных\")\n",
    "        \n",
    "        return {\n",
    "            'kaggle_score': 0.0,\n",
    "            'sharpe_ratio': 0.0,\n",
    "            'mean_correlation': 0.0,\n",
    "            'std_correlation': 0.0,\n",
    "            'median_correlation': 0.0,\n",
    "            'correlations': np.array([]),\n",
    "            'successful_targets': 0,\n",
    "            'failed_targets': failed_targets\n",
    "        }\n",
    "    \n",
    "    correlations = np.array(correlations)\n",
    "    \n",
    "    # Статистика\n",
    "    mean_corr = np.mean(correlations)\n",
    "    std_corr = np.std(correlations)\n",
    "    median_corr = np.median(correlations)\n",
    "    min_corr = np.min(correlations)\n",
    "    max_corr = np.max(correlations)\n",
    "    \n",
    "    # Kaggle Score (масштабированный Modified Sharpe Ratio)\n",
    "    if std_corr > 1e-8:  # Защита от деления на 0\n",
    "        sharpe_ratio = mean_corr / std_corr\n",
    "        kaggle_score = sharpe_ratio * 100000  # Масштабирование\n",
    "    else:\n",
    "        sharpe_ratio = 0\n",
    "        kaggle_score = 0\n",
    "    \n",
    "    print(f\"\\nСтатистика Spearman Correlations:\")\n",
    "    print(f\"  Успешно: {len(correlations)}/{targets.shape[1]} targets\")\n",
    "    if failed_targets > 0:\n",
    "        print(f\"  Провалено: {failed_targets} targets (константные значения)\")\n",
    "    print(f\"  Mean:    {mean_corr:.6f}\")\n",
    "    print(f\"  Median:  {median_corr:.6f}\")\n",
    "    print(f\"  Std:     {std_corr:.6f}\")\n",
    "    print(f\"  Min:     {min_corr:.6f}\")\n",
    "    print(f\"  Max:     {max_corr:.6f}\")\n",
    "    \n",
    "    # Распределение\n",
    "    positive = (correlations > 0).sum()\n",
    "    negative = (correlations < 0).sum()\n",
    "    near_zero = (np.abs(correlations) < 0.01).sum()\n",
    "    \n",
    "    print(f\"\\n  Positive correlations: {positive}/{len(correlations)} ({100*positive/len(correlations):.1f}%)\")\n",
    "    print(f\"  Negative correlations: {negative}/{len(correlations)} ({100*negative/len(correlations):.1f}%)\")\n",
    "    print(f\"  Near zero (|r| < 0.01): {near_zero}/{len(correlations)} ({100*near_zero/len(correlations):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"KAGGLE SCORE (Modified Sharpe Ratio):\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Sharpe Ratio:        {sharpe_ratio:.6f}\")\n",
    "    print(f\"  KAGGLE SCORE:        {kaggle_score:.2f}\")\n",
    "    \n",
    "    # Интерпретация\n",
    "    if kaggle_score > 100000:\n",
    "        print(f\"\\n  ✅ ОТЛИЧНЫЙ результат! (> 100,000)\")\n",
    "    elif kaggle_score > 50000:\n",
    "        print(f\"\\n  ✓ Хороший результат (> 50,000)\")\n",
    "    elif kaggle_score > 0:\n",
    "        print(f\"\\n  ⚠ Слабый результат (> 0, но < 50,000)\")\n",
    "    else:\n",
    "        print(f\"\\n  ❌ ПЛОХОЙ результат (отрицательный score)\")\n",
    "        print(f\"     Модель предсказывает в противоположном направлении!\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'kaggle_score': kaggle_score,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'mean_correlation': mean_corr,\n",
    "        'std_correlation': std_corr,\n",
    "        'median_correlation': median_corr,\n",
    "        'min_correlation': min_corr,\n",
    "        'max_correlation': max_corr,\n",
    "        'correlations': correlations,\n",
    "        'successful_targets': len(correlations),\n",
    "        'failed_targets': failed_targets\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c035b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_evaluation_corrected():\n",
    "    \"\"\"\n",
    "    Полная оценка с ПРАВИЛЬНОЙ метрикой\n",
    "    \"\"\"\n",
    "    global models, scaler, feature_cols, base_cols, model_val_losses, is_initialized, device\n",
    "    \n",
    "    if not is_initialized:\n",
    "        print(\"Модели не инициализированы!\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"🎯\"*35)\n",
    "    print(\"ПОЛНАЯ ОЦЕНКА МОДЕЛИ (ПРАВИЛЬНАЯ МЕТРИКА)\")\n",
    "    print(\"🎯\"*35)\n",
    "    \n",
    "    # Загружаем данные\n",
    "    train = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/train.csv')\n",
    "    train_labels = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/train_labels.csv')\n",
    "    \n",
    "    # Создаем фичи\n",
    "    train_features = create_enhanced_features(train, base_cols_ref=base_cols)\n",
    "    \n",
    "    try:\n",
    "        target_pairs = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/target_pairs.csv')\n",
    "        train_features = add_spread_features(train, train_features, target_pairs)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    target_cols = [f'target_{i}' for i in range(424)]\n",
    "    \n",
    "    X_train = train_features[feature_cols].fillna(0).values\n",
    "    y_train = train_labels[target_cols].fillna(0).values\n",
    "    \n",
    "    X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    y_train = np.nan_to_num(y_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Validation set (последние 10%)\n",
    "    split_idx = int(len(X_train) * 0.9)\n",
    "    X_val = X_train[split_idx:]\n",
    "    y_val = y_train[split_idx:]\n",
    "    \n",
    "    print(f\"\\n1️⃣  VALIDATION SCORE:\")\n",
    "    print(f\"Validation set size: {len(X_val)} samples\")\n",
    "    \n",
    "    # Предсказания\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "    \n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            pred = model(X_val_tensor)\n",
    "            pred_cpu = pred.cpu().numpy()\n",
    "            all_preds.append(pred_cpu)\n",
    "    \n",
    "    # Взвешенный ансамбль\n",
    "    weights = 1.0 / np.array(model_val_losses)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    val_preds = np.average(all_preds, axis=0, weights=weights)\n",
    "    val_preds = np.clip(val_preds, -0.1, 0.1)\n",
    "    val_preds = np.nan_to_num(val_preds, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # ДИАГНОСТИКА VALIDATION PREDICTIONS\n",
    "    diagnose_predictions(val_preds, y_val)\n",
    "    \n",
    "    # Вычисляем ПРАВИЛЬНЫЙ Kaggle Score\n",
    "    val_results = calculate_kaggle_score_correct(val_preds, y_val)\n",
    "    \n",
    "    # Test set (последние 90 дней)\n",
    "    print(f\"\\n2️⃣  TEST SCORE:\")\n",
    "    test_train = train.tail(90).reset_index(drop=True)\n",
    "    test_labels = train_labels.tail(90).reset_index(drop=True)\n",
    "    \n",
    "    X_test = prepare_features(test_train)\n",
    "    y_test = test_labels[target_cols].fillna(0).values\n",
    "    y_test = np.nan_to_num(y_test, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    \n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            pred = model(X_test_tensor)\n",
    "            pred_cpu = pred.cpu().numpy()\n",
    "            all_preds.append(pred_cpu)\n",
    "    \n",
    "    test_preds = np.average(all_preds, axis=0, weights=weights)\n",
    "    test_preds = np.clip(test_preds, -0.1, 0.1)\n",
    "    test_preds = np.nan_to_num(test_preds, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # ДИАГНОСТИКА TEST PREDICTIONS\n",
    "    diagnose_predictions(test_preds, y_test)\n",
    "    \n",
    "    test_results = calculate_kaggle_score_correct(test_preds, y_test)\n",
    "    \n",
    "    # Сравнение\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"📊 СРАВНЕНИЕ РЕЗУЛЬТАТОВ:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Метрика':<30} {'Validation':<20} {'Test':<20}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'KAGGLE SCORE':<30} {val_results['kaggle_score']:<20.2f} {test_results['kaggle_score']:<20.2f}\")\n",
    "    print(f\"{'Sharpe Ratio':<30} {val_results['sharpe_ratio']:<20.6f} {test_results['sharpe_ratio']:<20.6f}\")\n",
    "    print(f\"{'Mean Correlation':<30} {val_results['mean_correlation']:<20.6f} {test_results['mean_correlation']:<20.6f}\")\n",
    "    print(f\"{'Std Correlation':<30} {val_results['std_correlation']:<20.6f} {test_results['std_correlation']:<20.6f}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if val_results['kaggle_score'] > 100000:\n",
    "        print(\"✅ ОТЛИЧНЫЙ РЕЗУЛЬТАТ! Score > 100,000\")\n",
    "    elif val_results['kaggle_score'] > 50000:\n",
    "        print(\"✓ Хороший результат. Score > 50,000\")\n",
    "    elif val_results['kaggle_score'] > 0:\n",
    "        print(\"⚠ Слабый результат. Score > 0, но < 50,000\")\n",
    "    else:\n",
    "        print(\"❌ ПЛОХОЙ результат. Отрицательный score!\")\n",
    "    \n",
    "    print(\"\\n\" + \"🎯\"*35 + \"\\n\")\n",
    "    \n",
    "    return {\n",
    "        'validation': val_results,\n",
    "        'test': test_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4282c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавьте в main():\n",
    "def main():\n",
    "    global is_initialized\n",
    "    \n",
    "    print(\"ЗАПУСК ПАЙПЛАЙНА\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if not is_initialized:\n",
    "        print(\"Начало обучения моделей...\")\n",
    "        initialize_models()\n",
    "    \n",
    "    # ПРАВИЛЬНАЯ ОЦЕНКА\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ВЫЧИСЛЕНИЕ KAGGLE SCORE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    results = full_evaluation_corrected()\n",
    "    \n",
    "    # Создание submission\n",
    "    print(\"\\nСоздание submission файла...\")\n",
    "    create_submission_file()\n",
    "    \n",
    "    print(\"\\n✅ ВСЕ ГОТОВО!\")\n",
    "    print(f\"🏆 Ваш ожидаемый Kaggle Score: {results['validation']['kaggle_score']:.2f}\")\n",
    "    print(f\"📊 Sharpe Ratio: {results['validation']['sharpe_ratio']:.6f}\")\n",
    "    print(f\"📁 Submission файл: submission.parquet\")\n",
    "    print(f\"\\n🚀 ОТПРАВЬТЕ НА KAGGLE!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e52eb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd_kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
