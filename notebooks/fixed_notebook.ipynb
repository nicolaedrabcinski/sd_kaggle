{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41694760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется устройство: cuda\n",
      "Доступно GPU: 1\n",
      "Название GPU: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "# Ячейка 1: Импорты и настройка\n",
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import warnings\n",
    "import gc\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.stats import spearmanr\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Используется устройство: {device}\")\n",
    "print(f\"Доступно GPU: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Название GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "NUM_TARGET_COLUMNS = 424\n",
    "\n",
    "# Глобальные переменные\n",
    "models = []\n",
    "scaler = None\n",
    "feature_selector = None\n",
    "feature_cols = None\n",
    "base_cols = None\n",
    "is_initialized = False\n",
    "model_val_scores = []\n",
    "ensemble_weights_global = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dceca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesAugmentation:\n",
    "    \"\"\"Продвинутая аугментация для финансовых временных рядов\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_gaussian_noise(features, noise_level=0.01):\n",
    "        \"\"\"Добавление гауссовского шума\"\"\"\n",
    "        noise = torch.randn_like(features) * noise_level\n",
    "        return features + noise\n",
    "    \n",
    "    @staticmethod\n",
    "    def time_warp(features, warp_factor=0.1):\n",
    "        \"\"\"Временное искажение - сдвиг признаков во времени\"\"\"\n",
    "        batch_size, seq_len = features.shape\n",
    "        warp_strength = torch.randn(batch_size, 1, device=features.device) * warp_factor\n",
    "        warped_indices = torch.arange(seq_len, device=features.device).float() + warp_strength\n",
    "        warped_indices = torch.clamp(warped_indices, 0, seq_len-1)\n",
    "        \n",
    "        # Интерполяция\n",
    "        warped_features = torch.zeros_like(features)\n",
    "        for i in range(batch_size):\n",
    "            warped_features[i] = torch.interp(\n",
    "                torch.arange(seq_len, device=features.device).float(),\n",
    "                warped_indices[i],\n",
    "                features[i]\n",
    "            )\n",
    "        return warped_features\n",
    "    \n",
    "    @staticmethod\n",
    "    def feature_dropout(features, drop_prob=0.1):\n",
    "        \"\"\"Случайное обнуление признаков (как dropout но для фичей)\"\"\"\n",
    "        mask = torch.rand_like(features) > drop_prob\n",
    "        return features * mask.float()\n",
    "    \n",
    "    @staticmethod\n",
    "    def scaling_augmentation(features, scale_range=(0.9, 1.1)):\n",
    "        \"\"\"Случайное масштабирование признаков\"\"\"\n",
    "        scale_factors = torch.rand(features.shape[0], 1, device=features.device) * (scale_range[1] - scale_range[0]) + scale_range[0]\n",
    "        return features * scale_factors\n",
    "    \n",
    "    @staticmethod\n",
    "    def jittering(features, jitter_strength=0.02):\n",
    "        \"\"\"Дрожание - случайные небольшие изменения\"\"\"\n",
    "        jitter = (torch.rand_like(features) - 0.5) * 2 * jitter_strength\n",
    "        return features + jitter\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_permutation(features, permute_fraction=0.1):\n",
    "        \"\"\"Случайная перестановка небольшой части признаков\"\"\"\n",
    "        batch_size, num_features = features.shape\n",
    "        num_to_permute = int(num_features * permute_fraction)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            perm_indices = torch.randperm(num_features)[:num_to_permute]\n",
    "            features[i, perm_indices] = features[i, perm_indices[torch.randperm(num_to_permute)]]\n",
    "        \n",
    "        return features\n",
    "\n",
    "def advanced_augmentation(features, augmentation_methods=None, augmentation_prob=0.7):\n",
    "    \"\"\"Продвинутая аугментация с комбинацией методов\"\"\"\n",
    "    if augmentation_methods is None:\n",
    "        augmentation_methods = [\n",
    "            ('noise', 0.3),\n",
    "            ('scaling', 0.2), \n",
    "            ('jitter', 0.2),\n",
    "            ('dropout', 0.2),\n",
    "            ('permutation', 0.1)\n",
    "        ]\n",
    "    \n",
    "    augmented_features = features.clone()\n",
    "    \n",
    "    # Применяем аугментации с вероятностью\n",
    "    if torch.rand(1).item() < augmentation_prob:\n",
    "        for method, prob in augmentation_methods:\n",
    "            if torch.rand(1).item() < prob:\n",
    "                if method == 'noise':\n",
    "                    augmented_features = TimeSeriesAugmentation.add_gaussian_noise(augmented_features, 0.01)\n",
    "                elif method == 'scaling':\n",
    "                    augmented_features = TimeSeriesAugmentation.scaling_augmentation(augmented_features)\n",
    "                elif method == 'jitter':\n",
    "                    augmented_features = TimeSeriesAugmentation.jittering(augmented_features)\n",
    "                elif method == 'dropout':\n",
    "                    augmented_features = TimeSeriesAugmentation.feature_dropout(augmented_features, 0.05)\n",
    "                elif method == 'permutation':\n",
    "                    augmented_features = TimeSeriesAugmentation.random_permutation(augmented_features, 0.05)\n",
    "    \n",
    "    return augmented_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f7425c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 2: Функции потерь\n",
    "class PearsonCorrelationLoss(nn.Module):\n",
    "    \"\"\"Loss функция для оптимизации Pearson Correlation\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Для каждого target отдельно\n",
    "        batch_size = pred.shape[0]\n",
    "        num_targets = pred.shape[1]\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        for i in range(num_targets):\n",
    "            pred_col = pred[:, i]\n",
    "            target_col = target[:, i]\n",
    "            \n",
    "            # Центрируем\n",
    "            pred_centered = pred_col - pred_col.mean()\n",
    "            target_centered = target_col - target_col.mean()\n",
    "            \n",
    "            # Ковариация\n",
    "            covariance = (pred_centered * target_centered).mean()\n",
    "            \n",
    "            # Стандартные отклонения\n",
    "            pred_std = pred_centered.std()\n",
    "            target_std = target_centered.std()\n",
    "            \n",
    "            # Pearson correlation\n",
    "            if pred_std > 1e-8 and target_std > 1e-8:\n",
    "                correlation = covariance / (pred_std * target_std)\n",
    "                # Минимизируем отрицательную корреляцию\n",
    "                total_loss += -correlation\n",
    "            else:\n",
    "                # Штраф за константные предсказания\n",
    "                total_loss += 1.0\n",
    "        \n",
    "        return total_loss / num_targets\n",
    "\n",
    "class SpearmanLoss(nn.Module):\n",
    "    \"\"\"Approximation of Spearman correlation loss\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Используем ранжирование через сортировку\n",
    "        pred_rank = torch.argsort(torch.argsort(pred, dim=0), dim=0).float()\n",
    "        target_rank = torch.argsort(torch.argsort(target, dim=0), dim=0).float()\n",
    "        \n",
    "        # Нормализуем ранги\n",
    "        pred_rank = (pred_rank - pred_rank.mean(dim=0)) / (pred_rank.std(dim=0) + 1e-8)\n",
    "        target_rank = (target_rank - target_rank.mean(dim=0)) / (target_rank.std(dim=0) + 1e-8)\n",
    "        \n",
    "        # Correlation (минимизируем отрицательную)\n",
    "        correlation = (pred_rank * target_rank).mean(dim=0).mean()\n",
    "        return -correlation\n",
    "\n",
    "class CombinedCorrelationLoss(nn.Module):\n",
    "    \"\"\"Комбинация Pearson + Spearman + MSE\"\"\"\n",
    "    def __init__(self, pearson_weight=0.4, spearman_weight=0.4, mse_weight=0.2):\n",
    "        super().__init__()\n",
    "        self.pearson_weight = pearson_weight\n",
    "        self.spearman_weight = spearman_weight\n",
    "        self.mse_weight = mse_weight\n",
    "        \n",
    "        self.pearson_loss = PearsonCorrelationLoss()\n",
    "        self.spearman_loss = SpearmanLoss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pearson_l = self.pearson_loss(pred, target)\n",
    "        spearman_l = self.spearman_loss(pred, target)\n",
    "        mse_l = self.mse_loss(pred, target)\n",
    "        \n",
    "        total_loss = (self.pearson_weight * pearson_l + \n",
    "                     self.spearman_weight * spearman_l + \n",
    "                     self.mse_weight * mse_l)\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "189246a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 3: Архитектуры моделей\n",
    "class SimpleEffectiveModel(nn.Module):\n",
    "    \"\"\"Простая но эффективная архитектура\"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_size=512, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.BatchNorm1d(hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.8),\n",
    "            \n",
    "            nn.Linear(hidden_size // 2, hidden_size // 4),\n",
    "            nn.BatchNorm1d(hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.6),\n",
    "            \n",
    "            nn.Linear(hidden_size // 4, output_size)\n",
    "        )\n",
    "        \n",
    "        # Инициализация весов\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59a0dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEffectiveModel_Improved(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=512, dropout_rate=0.4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.BatchNorm1d(hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.8),\n",
    "            \n",
    "            nn.Linear(hidden_size // 2, hidden_size // 4),\n",
    "            nn.BatchNorm1d(hidden_size // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.6),\n",
    "            \n",
    "            nn.Linear(hidden_size // 4, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d05f330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResidualModel(nn.Module):\n",
    "    \"\"\"Модель с residual connections\"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_size=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size)\n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size)\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size // 2, output_size)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # Residual blocks\n",
    "        residual = x\n",
    "        x = self.block1(x)\n",
    "        x = x + residual\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        residual = x\n",
    "        x = self.block2(x)\n",
    "        x = x + residual\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return self.output(x)\n",
    "\n",
    "class WideModel(nn.Module):\n",
    "    \"\"\"Широкая модель с большим количеством нейронов\"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f8fc64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Новая: Глубокая модель с skip connections\n",
    "class DeepSkipModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=512, num_blocks=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(num_blocks):\n",
    "            self.blocks.append(nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size)\n",
    "            ))\n",
    "        \n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            residual = x\n",
    "            x = block(x)\n",
    "            x = x + residual\n",
    "            x = nn.ReLU()(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c9711ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=512, num_heads=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Self-attention\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size * 2, hidden_size)\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # Add sequence dimension for attention\n",
    "        x_seq = x.unsqueeze(1)\n",
    "        \n",
    "        # Self-attention with residual\n",
    "        attn_out, _ = self.attention(x_seq, x_seq, x_seq)\n",
    "        x = x + attn_out.squeeze(1)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = x + ffn_out\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d27794d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleInsideModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_experts=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_size, 384),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(384, 256)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_experts),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gates = self.gate(x)\n",
    "        expert_outputs = [expert(x) for expert in self.experts]\n",
    "        \n",
    "        # Weighted combination\n",
    "        combined = torch.zeros_like(expert_outputs[0])\n",
    "        for i, expert_out in enumerate(expert_outputs):\n",
    "            combined += gates[:, i:i+1] * expert_out\n",
    "        \n",
    "        return self.output(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d41aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 4: Создание признаков\n",
    "def create_optimized_features(df, base_cols_ref=None, max_features=800):\n",
    "    \"\"\"Создает оптимизированный набор признаков\"\"\"\n",
    "    features = df.copy()\n",
    "    \n",
    "    if base_cols_ref is not None:\n",
    "        numeric_cols = [c for c in base_cols_ref if c in df.columns]\n",
    "    else:\n",
    "        numeric_cols = [c for c in df.columns \n",
    "                       if c not in ['date_id', 'is_scored'] \n",
    "                       and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    \n",
    "    # Ограничиваем количество базовых колонок\n",
    "    numeric_cols = numeric_cols[:min(100, len(numeric_cols))]\n",
    "    \n",
    "    print(f\"Обрабатывается {len(numeric_cols)} базовых колонок...\")\n",
    "    \n",
    "    feature_count = 0\n",
    "    for col in numeric_cols:\n",
    "        try:\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                # Базовые returns\n",
    "                features[f'{col}_return_1d'] = df[col].pct_change(1)\n",
    "                features[f'{col}_return_5d'] = df[col].pct_change(5)\n",
    "                \n",
    "                # Скользящие статистики\n",
    "                for window in [5, 10, 20]:\n",
    "                    features[f'{col}_ma_{window}'] = df[col].rolling(window, min_periods=1).mean()\n",
    "                    features[f'{col}_std_{window}'] = df[col].rolling(window, min_periods=1).std()\n",
    "                \n",
    "                # Z-score\n",
    "                rolling_mean = df[col].rolling(20, min_periods=1).mean()\n",
    "                rolling_std = df[col].rolling(20, min_periods=1).std()\n",
    "                features[f'{col}_zscore'] = (df[col] - rolling_mean) / (rolling_std + 1e-8)\n",
    "                \n",
    "                # Lag features\n",
    "                for lag in [1, 2, 3]:\n",
    "                    features[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "                \n",
    "                # Volatility\n",
    "                features[f'{col}_volatility_20'] = df[col].rolling(20).std() / (df[col].rolling(20).mean() + 1e-8)\n",
    "                \n",
    "                feature_count += 11  # 11 features per column\n",
    "                \n",
    "                # Останавливаемся если достигли лимита\n",
    "                if feature_count >= max_features:\n",
    "                    print(f\"Достигнут лимит в {max_features} признаков\")\n",
    "                    break\n",
    "                    \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Заполняем NaN\n",
    "    features = features.fillna(0)\n",
    "    \n",
    "    # Удаляем колонки с нулевой дисперсией\n",
    "    selector = VarianceThreshold()\n",
    "    non_constant_features = selector.fit(features.select_dtypes(include=[np.number])).get_support()\n",
    "    feature_columns = features.select_dtypes(include=[np.number]).columns[non_constant_features]\n",
    "    features = features[feature_columns]\n",
    "    \n",
    "    print(f\"Создано {len(features.columns)} признаков после фильтрации\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dca409b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_features_vectorized_fixed(features, targets, n_features=500):\n",
    "    \"\"\"Исправленная векторизованная версия с обработкой NaN\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    print(\"Векторизованный отбор признаков с GPU...\")\n",
    "    \n",
    "    # Фильтруем колонки\n",
    "    feature_cols = [c for c in features.columns if c not in ['date_id', 'is_scored']]\n",
    "    features_filtered = features[feature_cols]\n",
    "    \n",
    "    # Перенос на GPU\n",
    "    X = torch.tensor(features_filtered.values, dtype=torch.float32, device=device)\n",
    "    y = torch.tensor(targets.values, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Заменяем NaN и Inf на 0\n",
    "    X = torch.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    y = torch.nan_to_num(y, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Центрируем данные\n",
    "    X_centered = X - X.mean(dim=0)\n",
    "    y_centered = y - y.mean(dim=0)\n",
    "    \n",
    "    # Вычисляем ковариационную матрицу\n",
    "    covariance = torch.mm(X_centered.T, y_centered) / (X.shape[0] - 1)\n",
    "    \n",
    "    # Вычисляем стандартные отклонения с защитой от деления на 0\n",
    "    x_std = X_centered.std(dim=0, unbiased=True)\n",
    "    y_std = y_centered.std(dim=0, unbiased=True)\n",
    "    \n",
    "    # Защита от нулевых стандартных отклонений\n",
    "    x_std = torch.where(x_std < 1e-8, torch.ones_like(x_std), x_std)\n",
    "    y_std = torch.where(y_std < 1e-8, torch.ones_like(y_std), y_std)\n",
    "    \n",
    "    # Вычисляем матрицу корреляций Пирсона\n",
    "    correlation_matrix = covariance / (x_std.unsqueeze(1) * y_std.unsqueeze(0))\n",
    "    \n",
    "    # Заменяем NaN в корреляциях на 0\n",
    "    correlation_matrix = torch.nan_to_num(correlation_matrix, nan=0.0)\n",
    "    \n",
    "    # Усредняем абсолютные корреляции по targets\n",
    "    feature_importances = correlation_matrix.abs().mean(dim=1)\n",
    "    \n",
    "    # Сортируем признаки по важности\n",
    "    sorted_indices = torch.argsort(feature_importances, descending=True)\n",
    "    selected_features = [feature_cols[i] for i in sorted_indices[:n_features].cpu().numpy()]\n",
    "    \n",
    "    print(f\"Отобрано {len(selected_features)} лучших признаков\")\n",
    "    if len(feature_importances) > 0:\n",
    "        best_score = feature_importances[sorted_indices[0]].item()\n",
    "        print(f\"Лучший признак: {selected_features[0]} (корреляция: {best_score:.4f})\")\n",
    "    \n",
    "    # Очистка памяти\n",
    "    del X, y, X_centered, y_centered, covariance, correlation_matrix\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b668692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 5: Функции обучения\n",
    "def calculate_spearman_score(predictions, targets):\n",
    "    \"\"\"Вычисляет средний Spearman correlation\"\"\"\n",
    "    correlations = []\n",
    "    \n",
    "    for i in range(targets.shape[1]):\n",
    "        pred_col = predictions[:, i]\n",
    "        target_col = targets[:, i]\n",
    "        \n",
    "        if len(np.unique(pred_col)) > 1 and len(np.unique(target_col)) > 1:\n",
    "            try:\n",
    "                corr, _ = spearmanr(pred_col, target_col)\n",
    "                if not np.isnan(corr):\n",
    "                    correlations.append(corr)\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return np.mean(correlations) if correlations else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e8fd8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_features(features, noise_level=0.01):\n",
    "    \"\"\"Добавляет шум к признакам для аугментации\"\"\"\n",
    "    noise = torch.randn_like(features) * noise_level\n",
    "    return features + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dc8808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_augmentation(model, X_train_t, y_train_t, X_val_t, y_val_t, \n",
    "                                epochs=500, patience=50, lr=0.001, model_name=\"\", \n",
    "                                use_augmentation=True, augmentation_strength=0.7):\n",
    "    \"\"\"Функция обучения с продвинутой аугментацией\"\"\"\n",
    "    \n",
    "    criterion = CombinedCorrelationLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=8, factor=0.5)\n",
    "    \n",
    "    best_val_score = -float('inf')\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    print(f\"Начало обучения {model_name} с аугментацией: {use_augmentation}\")\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping на эпохе {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Обучение с аугментацией\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_augmentation:\n",
    "            # Применяем аугментацию только к тренировочным данным\n",
    "            augmented_features = advanced_augmentation(X_train_t, augmentation_prob=augmentation_strength)\n",
    "            train_outputs = model(augmented_features)\n",
    "        else:\n",
    "            train_outputs = model(X_train_t)\n",
    "            \n",
    "        train_loss = criterion(train_outputs, y_train_t)\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # Gradient clipping для стабильности\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Валидация (БЕЗ аугментации)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_t)\n",
    "            val_score = calculate_spearman_score(val_outputs.cpu().numpy(), y_val_t.cpu().numpy())\n",
    "            \n",
    "            train_preds = train_outputs.detach().cpu().numpy()\n",
    "            train_true = y_train_t.cpu().numpy()\n",
    "            train_score = calculate_spearman_score(train_preds, train_true)\n",
    "        \n",
    "        # Обновление лучших результатов\n",
    "        if val_score > best_val_score:\n",
    "            best_val_score = val_score\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            epochs_no_improve = 0\n",
    "            improvement_msg = \"✓ УЛУЧШЕНИЕ\"\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            improvement_msg = f\"NO IMPROVE ({epochs_no_improve}/{patience})\"\n",
    "        \n",
    "        model.train()\n",
    "        scheduler.step(train_loss)\n",
    "        \n",
    "        # Логирование\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0 or epochs_no_improve >= patience:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            aug_status = \"AUG\" if use_augmentation else \"NO_AUG\"\n",
    "            print(f\"{model_name} | {aug_status} | Ep {epoch+1:3d}/{epochs} | \"\n",
    "                  f\"TrL: {train_loss.item():.6f} | \"\n",
    "                  f\"TrScore: {train_score:.4f} | \"\n",
    "                  f\"ValScore: {val_score:.4f} | \"\n",
    "                  f\"LR: {current_lr:.6f} | {improvement_msg}\")\n",
    "    \n",
    "    # Загрузка лучших весов\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    return model, best_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89211a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models_with_augmentation():\n",
    "    global models, scaler, feature_cols, base_cols, model_val_scores, is_initialized, device\n",
    "    \n",
    "    if is_initialized:\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"ИНИЦИАЛИЗАЦИЯ С АУГМЕНТАЦИЕЙ НА {device}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Загрузка данных\n",
    "    train = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/train.csv')\n",
    "    train_labels = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/train_labels.csv')\n",
    "    \n",
    "    # Базовые колонки\n",
    "    base_cols = [c for c in train.columns \n",
    "                 if c not in ['date_id'] \n",
    "                 and pd.api.types.is_numeric_dtype(train[c])]\n",
    "    \n",
    "    print(f\"Базовых колонок: {len(base_cols)}\")\n",
    "    print(\"Создание оптимизированных признаков...\")\n",
    "    \n",
    "    # Создание признаков\n",
    "    train_features = create_optimized_features(train, base_cols_ref=base_cols, max_features=800)\n",
    "    \n",
    "    # Выбор лучших признаков\n",
    "    target_cols = [f'target_{i}' for i in range(424)]\n",
    "    selected_features = select_best_features_vectorized_fixed(train_features, train_labels[target_cols], n_features=500)\n",
    "    \n",
    "    feature_cols = selected_features\n",
    "    \n",
    "    # Подготовка данных\n",
    "    X_train = train_features[feature_cols].fillna(0).values\n",
    "    y_train = train_labels[target_cols].fillna(0).values\n",
    "    \n",
    "    X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    y_train = np.nan_to_num(y_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Масштабирование\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Перенос на GPU\n",
    "    X_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    y_tensor = torch.FloatTensor(y_train).to(device)\n",
    "    \n",
    "    # Разделение на train/validation\n",
    "    split_idx = int(len(X_train_scaled) * 0.85)\n",
    "    X_train_t, X_val_t = X_tensor[:split_idx], X_tensor[split_idx:]\n",
    "    y_train_t, y_val_t = y_tensor[:split_idx], y_tensor[split_idx:]\n",
    "    \n",
    "    print(f\"Train: {len(X_train_t)}, Validation: {len(X_val_t)}\")\n",
    "    \n",
    "    # Конфигурация моделей с разной силой аугментации\n",
    "    model_configs = [\n",
    "        (SimpleEffectiveModel_Improved, \"SimpleEffective_Improved\", 0.001, True, 0.7),\n",
    "        (ResidualModel, \"Residual\", 0.001, True, 0.8),\n",
    "        (WideModel, \"Wide\", 0.0005, True, 0.6),\n",
    "        (DeepSkipModel, \"DeepSkip\", 0.001, True, 0.7),\n",
    "        (AttentionModel, \"Attention\", 0.0008, True, 0.5),  # Меньше аугментации для attention\n",
    "        (EnsembleInsideModel, \"EnsembleInside\", 0.001, True, 0.7),\n",
    "    ]\n",
    "    \n",
    "    # Обучение всех моделей с аугментацией\n",
    "    for i, (ModelClass, name, lr, use_aug, aug_strength) in enumerate(model_configs):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"МОДЕЛЬ {i+1}/{len(model_configs)}: {name} (Аугментация: {use_aug})\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        try:\n",
    "            # Создание модели\n",
    "            if name == \"Wide\":\n",
    "                model = ModelClass(X_train_scaled.shape[1], 424).to(device)\n",
    "            elif name == \"Attention\":\n",
    "                model = ModelClass(X_train_scaled.shape[1], 424, hidden_size=384).to(device)\n",
    "            elif name == \"DeepSkip\":\n",
    "                model = ModelClass(X_train_scaled.shape[1], 424, hidden_size=512, num_blocks=5).to(device)\n",
    "            elif name == \"EnsembleInside\":\n",
    "                model = ModelClass(X_train_scaled.shape[1], 424, num_experts=4).to(device)\n",
    "            else:\n",
    "                model = ModelClass(X_train_scaled.shape[1], 424, hidden_size=512).to(device)\n",
    "            \n",
    "            # Обучение с аугментацией\n",
    "            model, val_score = train_model_with_augmentation(\n",
    "                model, X_train_t, y_train_t, X_val_t, y_val_t, \n",
    "                epochs=200, patience=20, lr=lr, model_name=name,\n",
    "                use_augmentation=use_aug, augmentation_strength=aug_strength\n",
    "            )\n",
    "            \n",
    "            models.append(model)\n",
    "            model_val_scores.append(val_score)\n",
    "            \n",
    "            print(f\"✅ {name} завершена. Best Val Score: {val_score:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ ОШИБКА при обучении модели {name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "        \n",
    "        # Очистка GPU памяти\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Проверка что хоть что-то обучилось\n",
    "    if not models:\n",
    "        raise Exception(\"❌ НИ ОДНА МОДЕЛЬ НЕ БЫЛА УСПЕШНО ОБУЧЕНА!\")\n",
    "    \n",
    "    # Вычисление весов для ансамбля\n",
    "    weights = np.array(model_val_scores)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ВЕСА МОДЕЛЕЙ С АУГМЕНТАЦИЕЙ:\")\n",
    "    for i, (config, weight, val_score) in enumerate(zip(model_configs, weights, model_val_scores)):\n",
    "        if i < len(models):\n",
    "            aug_status = \"AUG\" if config[3] else \"NO_AUG\"\n",
    "            print(f\"  Модель {i+1} ({config[1]}): вес={weight:.4f}, val_score={val_score:.4f} [{aug_status}]\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    is_initialized = True\n",
    "    \n",
    "    # Сохранение scaler и feature_cols\n",
    "    joblib.dump(scaler, 'scaler.joblib')\n",
    "    joblib.dump(feature_cols, 'feature_cols.joblib')\n",
    "    print(\"Scaler и feature_cols сохранены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "141dd808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df):\n",
    "    global feature_cols, scaler\n",
    "    \n",
    "    # Создаем фичи тем же способом что и при обучении\n",
    "    test_features = create_optimized_features(df, base_cols_ref=base_cols, max_features=800)\n",
    "    \n",
    "    # Используем только те фичи, которые были отобраны при обучении\n",
    "    available_features = [f for f in feature_cols if f in test_features.columns]\n",
    "    \n",
    "    X_test = np.zeros((len(df), len(feature_cols)))\n",
    "    \n",
    "    for i, col in enumerate(feature_cols):\n",
    "        if col in test_features.columns:\n",
    "            X_test[:, i] = test_features[col].fillna(0).values\n",
    "    \n",
    "    X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c5f1c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 7: Функции предсказания\n",
    "def predict(test, label_lags_1_batch, label_lags_2_batch, label_lags_3_batch, label_lags_4_batch):\n",
    "    global models, scaler, feature_cols, model_val_scores, is_initialized, device\n",
    "    \n",
    "    if not is_initialized:\n",
    "        initialize_models_fixed()\n",
    "    \n",
    "    try:\n",
    "        test_pd = test.to_pandas()\n",
    "        X_test = prepare_features(test_pd)\n",
    "        X_test_scaled = scaler.transform(X_test[-1:])\n",
    "        \n",
    "        X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "        \n",
    "        # Взвешенный ансамбль\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for model in models:\n",
    "                pred = model(X_test_tensor)\n",
    "                pred_cpu = pred.cpu().numpy()[0]\n",
    "                all_preds.append(pred_cpu)\n",
    "        \n",
    "        # Вычисляем веса (пропорционально val score)\n",
    "        weights = np.array(model_val_scores)\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        # Взвешенное усреднение\n",
    "        predictions = np.average(all_preds, axis=0, weights=weights)\n",
    "        \n",
    "        # Мягкое ограничение значений\n",
    "        predictions = np.tanh(predictions * 10) * 0.1  # Мягкое ограничение до [-0.1, 0.1]\n",
    "        predictions = np.nan_to_num(predictions, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        return pl.DataFrame({f'target_{i}': [float(predictions[i])] for i in range(NUM_TARGET_COLUMNS)})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка predict: {e}\")\n",
    "        return pl.DataFrame({f'target_{i}': [0.0] for i in range(NUM_TARGET_COLUMNS)})\n",
    "\n",
    "@torch.no_grad()\n",
    "def efficient_predict_batch(test_data, models, weights=None, batch_size=1024):\n",
    "    \"\"\"Эффективное предсказание с батчингом\"\"\"\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(models)) / len(models)\n",
    "    \n",
    "    # Если test_data уже pandas DataFrame, используем как есть\n",
    "    if hasattr(test_data, 'to_pandas'):\n",
    "        test_pd = test_data.to_pandas()\n",
    "    else:\n",
    "        test_pd = test_data\n",
    "    \n",
    "    X_test = prepare_features(test_pd)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Батчинг для больших данных\n",
    "    all_predictions = []\n",
    "    \n",
    "    for i in range(0, len(X_test_scaled), batch_size):\n",
    "        batch = X_test_scaled[i:i+batch_size]\n",
    "        X_batch_tensor = torch.FloatTensor(batch).to(device)\n",
    "        \n",
    "        batch_preds = []\n",
    "        for model, weight in zip(models, weights):\n",
    "            pred = model(X_batch_tensor).cpu().numpy()\n",
    "            batch_preds.append(pred * weight)\n",
    "        \n",
    "        batch_ensemble = np.sum(batch_preds, axis=0)\n",
    "        \n",
    "        # Мягкое ограничение\n",
    "        batch_ensemble = np.tanh(batch_ensemble * 10) * 0.1\n",
    "        all_predictions.append(batch_ensemble)\n",
    "    \n",
    "    predictions = np.vstack(all_predictions)\n",
    "    predictions = np.nan_to_num(predictions, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def create_submission_file():\n",
    "    global models, scaler, feature_cols, is_initialized, device, model_val_scores\n",
    "    \n",
    "    print(\"\\nСоздание submission.parquet...\")\n",
    "    \n",
    "    if not is_initialized:\n",
    "        raise Exception(\"Модели не инициализированы!\")\n",
    "    \n",
    "    test = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/test.csv')\n",
    "    \n",
    "    print(\"Генерация предсказаний...\")\n",
    "    \n",
    "    # Вычисляем веса\n",
    "    weights = np.array(model_val_scores)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    # Используем батчинговое предсказание\n",
    "    predictions = efficient_predict_batch(test, models, weights)\n",
    "    \n",
    "    submission = pd.DataFrame({'date_id': test['date_id'].values})\n",
    "    for i in range(424):\n",
    "        submission[f'target_{i}'] = predictions[:, i]\n",
    "    \n",
    "    if 'is_scored' in test.columns:\n",
    "        submission = submission[test['is_scored'] == True].reset_index(drop=True)\n",
    "    \n",
    "    submission = submission.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    submission.to_parquet('submission.parquet', index=False, engine='pyarrow')\n",
    "    \n",
    "    print(f\"Готово: {submission.shape}\")\n",
    "    \n",
    "    # Очищаем GPU память\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a96f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 8: Оценка модели\n",
    "def calculate_kaggle_score(predictions, targets):\n",
    "    \"\"\"\n",
    "    ПРАВИЛЬНАЯ метрика соревнования: Modified Sharpe Ratio\n",
    "    Score = (Mean Spearman Correlation / Std Spearman Correlation) * 100,000\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ВЫЧИСЛЕНИЕ KAGGLE SCORE (Modified Sharpe Ratio)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    correlations = []\n",
    "    failed_targets = 0\n",
    "    \n",
    "    # Для каждого target вычисляем Spearman correlation\n",
    "    for i in range(targets.shape[1]):\n",
    "        pred_col = predictions[:, i]\n",
    "        true_col = targets[:, i]\n",
    "        \n",
    "        # Проверяем что есть вариация в данных\n",
    "        if len(np.unique(pred_col)) < 2 or len(np.unique(true_col)) < 2:\n",
    "            failed_targets += 1\n",
    "            continue\n",
    "        \n",
    "        # Spearman rank correlation\n",
    "        try:\n",
    "            corr, p_value = spearmanr(pred_col, true_col)\n",
    "            if not np.isnan(corr) and not np.isinf(corr):\n",
    "                correlations.append(corr)\n",
    "            else:\n",
    "                failed_targets += 1\n",
    "        except Exception as e:\n",
    "            failed_targets += 1\n",
    "            continue\n",
    "    \n",
    "    if len(correlations) == 0:\n",
    "        print(\"\\n⚠️ Не удалось вычислить корреляции!\")\n",
    "        return {\n",
    "            'kaggle_score': 0.0,\n",
    "            'sharpe_ratio': 0.0,\n",
    "            'mean_correlation': 0.0,\n",
    "            'std_correlation': 0.0,\n",
    "            'correlations': np.array([])\n",
    "        }\n",
    "    \n",
    "    correlations = np.array(correlations)\n",
    "    \n",
    "    # Статистика\n",
    "    mean_corr = np.mean(correlations)\n",
    "    std_corr = np.std(correlations)\n",
    "    \n",
    "    # Kaggle Score (масштабированный Modified Sharpe Ratio)\n",
    "    if std_corr > 1e-8:\n",
    "        sharpe_ratio = mean_corr / std_corr\n",
    "        kaggle_score = sharpe_ratio * 100000\n",
    "    else:\n",
    "        sharpe_ratio = 0\n",
    "        kaggle_score = 0\n",
    "    \n",
    "    print(f\"\\nСтатистика Spearman Correlations:\")\n",
    "    print(f\"  Успешно: {len(correlations)}/{targets.shape[1]} targets\")\n",
    "    print(f\"  Mean:    {mean_corr:.6f}\")\n",
    "    print(f\"  Std:     {std_corr:.6f}\")\n",
    "    print(f\"  Min:     {np.min(correlations):.6f}\")\n",
    "    print(f\"  Max:     {np.max(correlations):.6f}\")\n",
    "    \n",
    "    positive = (correlations > 0).sum()\n",
    "    negative = (correlations < 0).sum()\n",
    "    \n",
    "    print(f\"\\n  Positive correlations: {positive}/{len(correlations)} ({100*positive/len(correlations):.1f}%)\")\n",
    "    print(f\"  Negative correlations: {negative}/{len(correlations)} ({100*negative/len(correlations):.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"KAGGLE SCORE (Modified Sharpe Ratio):\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Sharpe Ratio:        {sharpe_ratio:.6f}\")\n",
    "    print(f\"  KAGGLE SCORE:        {kaggle_score:.2f}\")\n",
    "    \n",
    "    # Интерпретация\n",
    "    if kaggle_score > 100000:\n",
    "        print(f\"\\n  ✅ ОТЛИЧНЫЙ результат! (> 100,000)\")\n",
    "    elif kaggle_score > 50000:\n",
    "        print(f\"\\n  ✓ Хороший результат (> 50,000)\")\n",
    "    elif kaggle_score > 0:\n",
    "        print(f\"\\n  ⚠ Слабый результат (> 0, но < 50,000)\")\n",
    "    else:\n",
    "        print(f\"\\n  ❌ ПЛОХОЙ результат (отрицательный score)\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'kaggle_score': kaggle_score,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'mean_correlation': mean_corr,\n",
    "        'std_correlation': std_corr,\n",
    "        'correlations': correlations\n",
    "    }\n",
    "\n",
    "def full_evaluation():\n",
    "    \"\"\"\n",
    "    Полная оценка модели\n",
    "    \"\"\"\n",
    "    global models, scaler, feature_cols, base_cols, model_val_scores, is_initialized, device\n",
    "    \n",
    "    if not is_initialized:\n",
    "        print(\"Модели не инициализированы!\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"🎯\"*35)\n",
    "    print(\"ПОЛНАЯ ОЦЕНКА МОДЕЛИ\")\n",
    "    print(\"🎯\"*35)\n",
    "    \n",
    "    # Загружаем данные\n",
    "    train = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/train.csv')\n",
    "    train_labels = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/train_labels.csv')\n",
    "    \n",
    "    # Создаем фичи\n",
    "    train_features = create_optimized_features(train, base_cols_ref=base_cols, max_features=800)\n",
    "    \n",
    "    target_cols = [f'target_{i}' for i in range(424)]\n",
    "    \n",
    "    # Используем только отобранные фичи\n",
    "    available_features = [f for f in feature_cols if f in train_features.columns]\n",
    "    X_train = train_features[available_features].fillna(0).values\n",
    "    y_train = train_labels[target_cols].fillna(0).values\n",
    "    \n",
    "    X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    y_train = np.nan_to_num(y_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Validation set (последние 15%)\n",
    "    split_idx = int(len(X_train) * 0.85)\n",
    "    X_val = X_train[split_idx:]\n",
    "    y_val = y_train[split_idx:]\n",
    "    \n",
    "    print(f\"\\n1️⃣  VALIDATION SCORE:\")\n",
    "    print(f\"Validation set size: {len(X_val)} samples\")\n",
    "    \n",
    "    # Предсказания\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "    \n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            pred = model(X_val_tensor)\n",
    "            pred_cpu = pred.cpu().numpy()\n",
    "            all_preds.append(pred_cpu)\n",
    "    \n",
    "    # Взвешенный ансамбль\n",
    "    weights = np.array(model_val_scores)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    val_preds = np.average(all_preds, axis=0, weights=weights)\n",
    "    val_preds = np.tanh(val_preds * 10) * 0.1\n",
    "    val_preds = np.nan_to_num(val_preds, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Вычисляем Kaggle Score\n",
    "    val_results = calculate_kaggle_score(val_preds, y_val)\n",
    "    \n",
    "    # Test set (последние 90 дней)\n",
    "    print(f\"\\n2️⃣  TEST SCORE:\")\n",
    "    test_train = train.tail(90).reset_index(drop=True)\n",
    "    test_labels = train_labels.tail(90).reset_index(drop=True)\n",
    "    \n",
    "    X_test = prepare_features(test_train)\n",
    "    y_test = test_labels[target_cols].fillna(0).values\n",
    "    y_test = np.nan_to_num(y_test, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    \n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            pred = model(X_test_tensor)\n",
    "            pred_cpu = pred.cpu().numpy()\n",
    "            all_preds.append(pred_cpu)\n",
    "    \n",
    "    test_preds = np.average(all_preds, axis=0, weights=weights)\n",
    "    test_preds = np.tanh(test_preds * 10) * 0.1\n",
    "    test_preds = np.nan_to_num(test_preds, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    test_results = calculate_kaggle_score(test_preds, y_test)\n",
    "    \n",
    "    # Сравнение\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"📊 СРАВНЕНИЕ РЕЗУЛЬТАТОВ:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Метрика':<30} {'Validation':<20} {'Test':<20}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"{'KAGGLE SCORE':<30} {val_results['kaggle_score']:<20.2f} {test_results['kaggle_score']:<20.2f}\")\n",
    "    print(f\"{'Sharpe Ratio':<30} {val_results['sharpe_ratio']:<20.6f} {test_results['sharpe_ratio']:<20.6f}\")\n",
    "    print(f\"{'Mean Correlation':<30} {val_results['mean_correlation']:<20.6f} {test_results['mean_correlation']:<20.6f}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if val_results['kaggle_score'] > 0:\n",
    "        print(\"✅ ХОРОШО: Модель показывает положительную корреляцию!\")\n",
    "    else:\n",
    "        print(\"❌ ПЛОХО: Модель показывает отрицательную корреляцию!\")\n",
    "    \n",
    "    print(\"\\n\" + \"🎯\"*35 + \"\\n\")\n",
    "    \n",
    "    return {\n",
    "        'validation': val_results,\n",
    "        'test': test_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e1108ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЗАПУСК ПАЙПЛАЙНА С ПРОДВИНУТОЙ АУГМЕНТАЦИЕЙ\n",
      "============================================================\n",
      "Начало обучения со сложной аугментацией...\n",
      "======================================================================\n",
      "ИНИЦИАЛИЗАЦИЯ С АУГМЕНТАЦИЕЙ НА cuda\n",
      "======================================================================\n",
      "Базовых колонок: 557\n",
      "Создание оптимизированных признаков...\n",
      "Обрабатывается 100 базовых колонок...\n",
      "Достигнут лимит в 800 признаков\n",
      "Создано 1507 признаков после фильтрации\n",
      "Векторизованный отбор признаков с GPU...\n",
      "Отобрано 500 лучших признаков\n",
      "Лучший признак: US_Stock_EMB_adj_open_volatility_20 (корреляция: 0.0607)\n",
      "Train: 1666, Validation: 295\n",
      "\n",
      "======================================================================\n",
      "МОДЕЛЬ 1/6: SimpleEffective_Improved (Аугментация: True)\n",
      "======================================================================\n",
      "Начало обучения SimpleEffective_Improved с аугментацией: True\n",
      "SimpleEffective_Improved | AUG | Ep   1/200 | TrL: 0.045389 | TrScore: -0.0007 | ValScore: 0.0014 | LR: 0.001000 | ✓ УЛУЧШЕНИЕ\n",
      "SimpleEffective_Improved | AUG | Ep  10/200 | TrL: -0.031458 | TrScore: 0.0451 | ValScore: 0.0199 | LR: 0.001000 | ✓ УЛУЧШЕНИЕ\n",
      "SimpleEffective_Improved | AUG | Ep  20/200 | TrL: -0.114963 | TrScore: 0.1310 | ValScore: 0.0520 | LR: 0.001000 | ✓ УЛУЧШЕНИЕ\n",
      "SimpleEffective_Improved | AUG | Ep  30/200 | TrL: -0.217989 | TrScore: 0.2579 | ValScore: 0.0534 | LR: 0.001000 | NO IMPROVE (5/20)\n",
      "SimpleEffective_Improved | AUG | Ep  40/200 | TrL: -0.288943 | TrScore: 0.3441 | ValScore: 0.0531 | LR: 0.001000 | NO IMPROVE (5/20)\n",
      "SimpleEffective_Improved | AUG | Ep  50/200 | TrL: -0.329458 | TrScore: 0.3926 | ValScore: 0.0494 | LR: 0.001000 | NO IMPROVE (15/20)\n",
      "SimpleEffective_Improved | AUG | Ep  60/200 | TrL: -0.356171 | TrScore: 0.4226 | ValScore: 0.0403 | LR: 0.001000 | NO IMPROVE (7/20)\n",
      "SimpleEffective_Improved | AUG | Ep  70/200 | TrL: -0.382528 | TrScore: 0.4549 | ValScore: 0.0575 | LR: 0.001000 | NO IMPROVE (17/20)\n",
      "SimpleEffective_Improved | AUG | Ep  73/200 | TrL: -0.386407 | TrScore: 0.4587 | ValScore: 0.0528 | LR: 0.001000 | NO IMPROVE (20/20)\n",
      "Early stopping на эпохе 74\n",
      "✅ SimpleEffective_Improved завершена. Best Val Score: 0.0590\n",
      "\n",
      "======================================================================\n",
      "МОДЕЛЬ 2/6: Residual (Аугментация: True)\n",
      "======================================================================\n",
      "Начало обучения Residual с аугментацией: True\n",
      "Residual | AUG | Ep   1/200 | TrL: 0.018268 | TrScore: 0.0015 | ValScore: 0.0046 | LR: 0.001000 | ✓ УЛУЧШЕНИЕ\n",
      "Residual | AUG | Ep  10/200 | TrL: -0.146514 | TrScore: 0.1508 | ValScore: 0.0234 | LR: 0.001000 | ✓ УЛУЧШЕНИЕ\n",
      "Residual | AUG | Ep  20/200 | TrL: -0.302969 | TrScore: 0.3433 | ValScore: 0.0210 | LR: 0.001000 | NO IMPROVE (3/20)\n",
      "Residual | AUG | Ep  30/200 | TrL: -0.379555 | TrScore: 0.4397 | ValScore: 0.0349 | LR: 0.001000 | NO IMPROVE (6/20)\n",
      "Residual | AUG | Ep  40/200 | TrL: -0.430555 | TrScore: 0.5036 | ValScore: 0.0320 | LR: 0.001000 | NO IMPROVE (16/20)\n",
      "Residual | AUG | Ep  44/200 | TrL: -0.438924 | TrScore: 0.5122 | ValScore: 0.0343 | LR: 0.001000 | NO IMPROVE (20/20)\n",
      "Early stopping на эпохе 45\n",
      "✅ Residual завершена. Best Val Score: 0.0367\n",
      "\n",
      "======================================================================\n",
      "МОДЕЛЬ 3/6: Wide (Аугментация: True)\n",
      "======================================================================\n",
      "Начало обучения Wide с аугментацией: True\n",
      "Wide | AUG | Ep   1/200 | TrL: 0.040766 | TrScore: -0.0011 | ValScore: -0.0028 | LR: 0.000500 | ✓ УЛУЧШЕНИЕ\n",
      "Wide | AUG | Ep  10/200 | TrL: -0.047491 | TrScore: 0.0648 | ValScore: 0.0152 | LR: 0.000500 | ✓ УЛУЧШЕНИЕ\n",
      "Wide | AUG | Ep  20/200 | TrL: -0.135703 | TrScore: 0.1548 | ValScore: 0.0431 | LR: 0.000500 | ✓ УЛУЧШЕНИЕ\n",
      "Wide | AUG | Ep  30/200 | TrL: -0.234404 | TrScore: 0.2747 | ValScore: 0.0560 | LR: 0.000500 | NO IMPROVE (1/20)\n",
      "Wide | AUG | Ep  40/200 | TrL: -0.304367 | TrScore: 0.3582 | ValScore: 0.0406 | LR: 0.000500 | NO IMPROVE (11/20)\n",
      "Wide | AUG | Ep  49/200 | TrL: -0.347630 | TrScore: 0.4136 | ValScore: 0.0510 | LR: 0.000500 | NO IMPROVE (20/20)\n",
      "Early stopping на эпохе 50\n",
      "✅ Wide завершена. Best Val Score: 0.0561\n",
      "\n",
      "======================================================================\n",
      "МОДЕЛЬ 4/6: DeepSkip (Аугментация: True)\n",
      "======================================================================\n",
      "Начало обучения DeepSkip с аугментацией: True\n",
      "DeepSkip | AUG | Ep   1/200 | TrL: 0.449801 | TrScore: -0.0009 | ValScore: -0.0007 | LR: 0.001000 | ✓ УЛУЧШЕНИЕ\n",
      "DeepSkip | AUG | Ep  10/200 | TrL: 0.029244 | TrScore: 0.0074 | ValScore: -0.0031 | LR: 0.001000 | NO IMPROVE (8/20)\n",
      "DeepSkip | AUG | Ep  20/200 | TrL: -0.023203 | TrScore: 0.0319 | ValScore: 0.0129 | LR: 0.001000 | ✓ УЛУЧШЕНИЕ\n",
      "DeepSkip | AUG | Ep  30/200 | TrL: -0.091373 | TrScore: 0.0989 | ValScore: 0.0313 | LR: 0.001000 | ✓ УЛУЧШЕНИЕ\n",
      "DeepSkip | AUG | Ep  40/200 | TrL: -0.199764 | TrScore: 0.2414 | ValScore: 0.0347 | LR: 0.001000 | NO IMPROVE (2/20)\n",
      "DeepSkip | AUG | Ep  50/200 | TrL: -0.296186 | TrScore: 0.3677 | ValScore: 0.0063 | LR: 0.001000 | NO IMPROVE (12/20)\n",
      "DeepSkip | AUG | Ep  58/200 | TrL: -0.349749 | TrScore: 0.4320 | ValScore: 0.0351 | LR: 0.001000 | NO IMPROVE (20/20)\n",
      "Early stopping на эпохе 59\n",
      "✅ DeepSkip завершена. Best Val Score: 0.0389\n",
      "\n",
      "======================================================================\n",
      "МОДЕЛЬ 5/6: Attention (Аугментация: True)\n",
      "======================================================================\n",
      "Начало обучения Attention с аугментацией: True\n",
      "Attention | AUG | Ep   1/200 | TrL: 0.070270 | TrScore: -0.0029 | ValScore: -0.0006 | LR: 0.000800 | ✓ УЛУЧШЕНИЕ\n",
      "Attention | AUG | Ep  10/200 | TrL: -0.168363 | TrScore: 0.2046 | ValScore: 0.0407 | LR: 0.000800 | ✓ УЛУЧШЕНИЕ\n",
      "Attention | AUG | Ep  20/200 | TrL: -0.288113 | TrScore: 0.3209 | ValScore: 0.0704 | LR: 0.000800 | ✓ УЛУЧШЕНИЕ\n",
      "Attention | AUG | Ep  30/200 | TrL: -0.350735 | TrScore: 0.3865 | ValScore: 0.0559 | LR: 0.000800 | NO IMPROVE (10/20)\n",
      "Attention | AUG | Ep  40/200 | TrL: -0.389079 | TrScore: 0.4309 | ValScore: 0.0508 | LR: 0.000800 | NO IMPROVE (20/20)\n",
      "Early stopping на эпохе 41\n",
      "✅ Attention завершена. Best Val Score: 0.0704\n",
      "\n",
      "======================================================================\n",
      "МОДЕЛЬ 6/6: EnsembleInside (Аугментация: True)\n",
      "======================================================================\n",
      "Начало обучения EnsembleInside с аугментацией: True\n",
      "EnsembleInside | AUG | Ep   1/200 | TrL: -0.000483 | TrScore: 0.0006 | ValScore: 0.0021 | LR: 0.001000 | ✓ УЛУЧШЕНИЕ\n",
      "EnsembleInside | AUG | Ep  10/200 | TrL: -0.149841 | TrScore: 0.1574 | ValScore: 0.0271 | LR: 0.001000 | ✓ УЛУЧШЕНИЕ\n",
      "EnsembleInside | AUG | Ep  20/200 | TrL: -0.289626 | TrScore: 0.3260 | ValScore: 0.0361 | LR: 0.001000 | NO IMPROVE (3/20)\n",
      "EnsembleInside | AUG | Ep  30/200 | TrL: -0.392663 | TrScore: 0.4579 | ValScore: 0.0418 | LR: 0.001000 | NO IMPROVE (6/20)\n",
      "EnsembleInside | AUG | Ep  40/200 | TrL: -0.434460 | TrScore: 0.5112 | ValScore: 0.0370 | LR: 0.001000 | NO IMPROVE (4/20)\n",
      "EnsembleInside | AUG | Ep  50/200 | TrL: -0.480798 | TrScore: 0.5697 | ValScore: 0.0345 | LR: 0.001000 | NO IMPROVE (14/20)\n",
      "EnsembleInside | AUG | Ep  56/200 | TrL: -0.497630 | TrScore: 0.5909 | ValScore: 0.0375 | LR: 0.001000 | NO IMPROVE (20/20)\n",
      "Early stopping на эпохе 57\n",
      "✅ EnsembleInside завершена. Best Val Score: 0.0463\n",
      "\n",
      "======================================================================\n",
      "ВЕСА МОДЕЛЕЙ С АУГМЕНТАЦИЕЙ:\n",
      "  Модель 1 (SimpleEffective_Improved): вес=0.1919, val_score=0.0590 [AUG]\n",
      "  Модель 2 (Residual): вес=0.1193, val_score=0.0367 [AUG]\n",
      "  Модель 3 (Wide): вес=0.1825, val_score=0.0561 [AUG]\n",
      "  Модель 4 (DeepSkip): вес=0.1266, val_score=0.0389 [AUG]\n",
      "  Модель 5 (Attention): вес=0.2290, val_score=0.0704 [AUG]\n",
      "  Модель 6 (EnsembleInside): вес=0.1507, val_score=0.0463 [AUG]\n",
      "======================================================================\n",
      "\n",
      "Scaler и feature_cols сохранены\n",
      "\n",
      "============================================================\n",
      "ОЦЕНКА МОДЕЛЕЙ С АУГМЕНТАЦИЕЙ\n",
      "============================================================\n",
      "\n",
      "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
      "ПОЛНАЯ ОЦЕНКА МОДЕЛИ\n",
      "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
      "Обрабатывается 100 базовых колонок...\n",
      "Достигнут лимит в 800 признаков\n",
      "Создано 1507 признаков после фильтрации\n",
      "\n",
      "1️⃣  VALIDATION SCORE:\n",
      "Validation set size: 295 samples\n",
      "\n",
      "======================================================================\n",
      "ВЫЧИСЛЕНИЕ KAGGLE SCORE (Modified Sharpe Ratio)\n",
      "======================================================================\n",
      "\n",
      "Статистика Spearman Correlations:\n",
      "  Успешно: 424/424 targets\n",
      "  Mean:    0.057219\n",
      "  Std:     0.058726\n",
      "  Min:     -0.118412\n",
      "  Max:     0.232985\n",
      "\n",
      "  Positive correlations: 359/424 (84.7%)\n",
      "  Negative correlations: 65/424 (15.3%)\n",
      "\n",
      "======================================================================\n",
      "KAGGLE SCORE (Modified Sharpe Ratio):\n",
      "======================================================================\n",
      "  Sharpe Ratio:        0.974347\n",
      "  KAGGLE SCORE:        97434.67\n",
      "\n",
      "  ✓ Хороший результат (> 50,000)\n",
      "======================================================================\n",
      "\n",
      "\n",
      "2️⃣  TEST SCORE:\n",
      "Обрабатывается 100 базовых колонок...\n",
      "Достигнут лимит в 800 признаков\n",
      "Создано 1502 признаков после фильтрации\n",
      "\n",
      "======================================================================\n",
      "ВЫЧИСЛЕНИЕ KAGGLE SCORE (Modified Sharpe Ratio)\n",
      "======================================================================\n",
      "\n",
      "Статистика Spearman Correlations:\n",
      "  Успешно: 424/424 targets\n",
      "  Mean:    0.017133\n",
      "  Std:     0.094447\n",
      "  Min:     -0.244093\n",
      "  Max:     0.298635\n",
      "\n",
      "  Positive correlations: 248/424 (58.5%)\n",
      "  Negative correlations: 176/424 (41.5%)\n",
      "\n",
      "======================================================================\n",
      "KAGGLE SCORE (Modified Sharpe Ratio):\n",
      "======================================================================\n",
      "  Sharpe Ratio:        0.181401\n",
      "  KAGGLE SCORE:        18140.06\n",
      "\n",
      "  ⚠ Слабый результат (> 0, но < 50,000)\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "📊 СРАВНЕНИЕ РЕЗУЛЬТАТОВ:\n",
      "======================================================================\n",
      "Метрика                        Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "KAGGLE SCORE                   97434.67             18140.06            \n",
      "Sharpe Ratio                   0.974347             0.181401            \n",
      "Mean Correlation               0.057219             0.017133            \n",
      "======================================================================\n",
      "✅ ХОРОШО: Модель показывает положительную корреляцию!\n",
      "\n",
      "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
      "\n",
      "\n",
      "Создание submission файла...\n",
      "\n",
      "Создание submission.parquet...\n",
      "Генерация предсказаний...\n",
      "Обрабатывается 100 базовых колонок...\n",
      "Достигнут лимит в 800 признаков\n",
      "Создано 1502 признаков после фильтрации\n",
      "Готово: (90, 425)\n",
      "\n",
      "✅ АУГМЕНТИРОВАННЫЙ АНСАМБЛЬ ГОТОВ!\n",
      "🏆 Ваш ожидаемый Kaggle Score: 97434.67\n",
      "📊 Sharpe Ratio: 0.974347\n",
      "🔢 Количество моделей: 6\n",
      "🎯 Использована продвинутая аугментация\n",
      "📁 Submission файл: submission.parquet\n",
      "\n",
      "🚀 ОТПРАВЛЯЙТЕ НА KAGGLE!\n"
     ]
    }
   ],
   "source": [
    "def main_with_augmentation():\n",
    "    global is_initialized\n",
    "    \n",
    "    print(\"ЗАПУСК ПАЙПЛАЙНА С ПРОДВИНУТОЙ АУГМЕНТАЦИЕЙ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not is_initialized:\n",
    "        print(\"Начало обучения со сложной аугментацией...\")\n",
    "        initialize_models_with_augmentation()\n",
    "    \n",
    "    # Оценка\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ОЦЕНКА МОДЕЛЕЙ С АУГМЕНТАЦИЕЙ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = full_evaluation()\n",
    "    \n",
    "    # Создание submission\n",
    "    print(\"\\nСоздание submission файла...\")\n",
    "    create_submission_file()\n",
    "    \n",
    "    print(\"\\n✅ АУГМЕНТИРОВАННЫЙ АНСАМБЛЬ ГОТОВ!\")\n",
    "    print(f\"🏆 Ваш ожидаемый Kaggle Score: {results['validation']['kaggle_score']:.2f}\")\n",
    "    print(f\"📊 Sharpe Ratio: {results['validation']['sharpe_ratio']:.6f}\")\n",
    "    print(f\"🔢 Количество моделей: {len(models)}\")\n",
    "    print(f\"🎯 Использована продвинутая аугментация\")\n",
    "    print(f\"📁 Submission файл: submission.parquet\")\n",
    "    print(f\"\\n🚀 ОТПРАВЛЯЙТЕ НА KAGGLE!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_with_augmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1fdd8e",
   "metadata": {},
   "source": [
    "#### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dddc08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавьте эти импорты в начало файла\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Расширенный класс для оптимизации ВСЕХ параметров\n",
    "class ComprehensiveOptunaOptimizer:\n",
    "    \"\"\"Комплексная оптимизация ВСЕХ гиперпараметров\"\"\"\n",
    "    \n",
    "    def __init__(self, X_train_t, y_train_t, X_val_t, y_val_t, feature_cols):\n",
    "        self.X_train_t = X_train_t\n",
    "        self.y_train_t = y_train_t\n",
    "        self.X_val_t = X_val_t\n",
    "        self.y_val_t = y_val_t\n",
    "        self.feature_cols = feature_cols\n",
    "        self.best_params = {}\n",
    "        self.studies = {}\n",
    "    \n",
    "    def objective_comprehensive(self, trial):\n",
    "        \"\"\"Комплексная objective функция, оптимизирующая ВСЕ параметры\"\"\"\n",
    "        \n",
    "        # 1. Выбор архитектуры модели\n",
    "        model_type = trial.suggest_categorical('model_type', [\n",
    "            'simple_effective', 'residual', 'wide', 'deep_skip', \n",
    "            'attention', 'ensemble_inside'\n",
    "        ])\n",
    "        \n",
    "        # 2. Общие гиперпараметры\n",
    "        lr = trial.suggest_float('lr', 1e-6, 1e-2, log=True)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-8, 1e-2, log=True)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128, 256])\n",
    "        \n",
    "        # 3. Параметры аугментации\n",
    "        augmentation_strength = trial.suggest_float('augmentation_strength', 0.1, 0.9)\n",
    "        use_noise = trial.suggest_categorical('use_noise', [True, False])\n",
    "        use_scaling = trial.suggest_categorical('use_scaling', [True, False])\n",
    "        use_jitter = trial.suggest_categorical('use_jitter', [True, False])\n",
    "        use_dropout_aug = trial.suggest_categorical('use_dropout_aug', [True, False])\n",
    "        use_permutation = trial.suggest_categorical('use_permutation', [True, False])\n",
    "        \n",
    "        # 4. Параметры loss функции\n",
    "        pearson_weight = trial.suggest_float('pearson_weight', 0.1, 0.8)\n",
    "        spearman_weight = trial.suggest_float('spearman_weight', 0.1, 0.8)\n",
    "        mse_weight = trial.suggest_float('mse_weight', 0.05, 0.4)\n",
    "        \n",
    "        # Нормализация весов\n",
    "        total_weight = pearson_weight + spearman_weight + mse_weight\n",
    "        pearson_weight /= total_weight\n",
    "        spearman_weight /= total_weight\n",
    "        mse_weight /= total_weight\n",
    "        \n",
    "        # 5. Архитектурные параметры в зависимости от типа модели\n",
    "        if model_type == 'simple_effective':\n",
    "            hidden_size = trial.suggest_categorical('hidden_size', [256, 512, 768, 1024, 1536])\n",
    "            dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.7)\n",
    "            num_layers = trial.suggest_int('num_layers', 2, 6)\n",
    "            \n",
    "            model = self._create_simple_effective_model(\n",
    "                hidden_size, dropout_rate, num_layers\n",
    "            )\n",
    "            \n",
    "        elif model_type == 'residual':\n",
    "            hidden_size = trial.suggest_categorical('res_hidden_size', [128, 256, 512, 768])\n",
    "            num_blocks = trial.suggest_int('num_blocks', 2, 6)\n",
    "            block_dropout = trial.suggest_float('block_dropout', 0.1, 0.5)\n",
    "            \n",
    "            model = self._create_residual_model(\n",
    "                hidden_size, num_blocks, block_dropout\n",
    "            )\n",
    "            \n",
    "        elif model_type == 'wide':\n",
    "            layer1_size = trial.suggest_categorical('layer1_size', [512, 768, 1024, 1536, 2048])\n",
    "            layer2_size = trial.suggest_categorical('layer2_size', [256, 512, 768, 1024])\n",
    "            layer3_size = trial.suggest_categorical('layer3_size', [128, 256, 512])\n",
    "            dropout1 = trial.suggest_float('dropout1', 0.2, 0.7)\n",
    "            dropout2 = trial.suggest_float('dropout2', 0.1, 0.6)\n",
    "            dropout3 = trial.suggest_float('dropout3', 0.1, 0.5)\n",
    "            \n",
    "            model = self._create_wide_model(\n",
    "                layer1_size, layer2_size, layer3_size, dropout1, dropout2, dropout3\n",
    "            )\n",
    "            \n",
    "        elif model_type == 'deep_skip':\n",
    "            hidden_size = trial.suggest_categorical('deep_hidden_size', [256, 512, 768, 1024])\n",
    "            num_blocks = trial.suggest_int('deep_num_blocks', 3, 8)\n",
    "            block_dropout = trial.suggest_float('deep_dropout', 0.1, 0.5)\n",
    "            \n",
    "            model = self._create_deep_skip_model(\n",
    "                hidden_size, num_blocks, block_dropout\n",
    "            )\n",
    "            \n",
    "        elif model_type == 'attention':\n",
    "            hidden_size = trial.suggest_categorical('attn_hidden_size', [256, 384, 512, 768])\n",
    "            num_heads = trial.suggest_categorical('num_heads', [4, 8, 16])\n",
    "            ffn_multiplier = trial.suggest_float('ffn_multiplier', 1.0, 4.0)\n",
    "            attn_dropout = trial.suggest_float('attn_dropout', 0.05, 0.3)\n",
    "            \n",
    "            model = self._create_attention_model(\n",
    "                hidden_size, num_heads, ffn_multiplier, attn_dropout\n",
    "            )\n",
    "            \n",
    "        elif model_type == 'ensemble_inside':\n",
    "            num_experts = trial.suggest_int('num_experts', 2, 6)\n",
    "            expert_size = trial.suggest_categorical('expert_size', [128, 256, 384, 512])\n",
    "            gate_size = trial.suggest_categorical('gate_size', [64, 128, 256])\n",
    "            expert_dropout = trial.suggest_float('expert_dropout', 0.1, 0.5)\n",
    "            \n",
    "            model = self._create_ensemble_model(\n",
    "                num_experts, expert_size, gate_size, expert_dropout\n",
    "            )\n",
    "        \n",
    "        # 6. Параметры оптимизатора\n",
    "        optimizer_type = trial.suggest_categorical('optimizer', ['adam', 'adamw', 'sgd', 'rmsprop'])\n",
    "        use_scheduler = trial.suggest_categorical('use_scheduler', [True, False])\n",
    "        grad_clip = trial.suggest_float('grad_clip', 0.1, 2.0)\n",
    "        \n",
    "        # Создание оптимизатора\n",
    "        if optimizer_type == 'adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'adamw':\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'sgd':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.9)\n",
    "        else:  # rmsprop\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        # Scheduler\n",
    "        if use_scheduler:\n",
    "            scheduler_type = trial.suggest_categorical('scheduler_type', ['plateau', 'cosine', 'step'])\n",
    "            if scheduler_type == 'plateau':\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "            elif scheduler_type == 'cosine':\n",
    "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "            else:  # step\n",
    "                scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "        else:\n",
    "            scheduler = None\n",
    "        \n",
    "        # 7. Custom loss функция\n",
    "        criterion = CombinedCorrelationLoss(\n",
    "            pearson_weight=pearson_weight,\n",
    "            spearman_weight=spearman_weight,\n",
    "            mse_weight=mse_weight\n",
    "        )\n",
    "        \n",
    "        # 8. Конфигурация аугментации\n",
    "        augmentation_methods = []\n",
    "        if use_noise:\n",
    "            augmentation_methods.append(('noise', trial.suggest_float('noise_prob', 0.1, 0.8)))\n",
    "        if use_scaling:\n",
    "            augmentation_methods.append(('scaling', trial.suggest_float('scaling_prob', 0.1, 0.8)))\n",
    "        if use_jitter:\n",
    "            augmentation_methods.append(('jitter', trial.suggest_float('jitter_prob', 0.1, 0.8)))\n",
    "        if use_dropout_aug:\n",
    "            augmentation_methods.append(('dropout', trial.suggest_float('dropout_aug_prob', 0.05, 0.4)))\n",
    "        if use_permutation:\n",
    "            augmentation_methods.append(('permutation', trial.suggest_float('permutation_prob', 0.05, 0.3)))\n",
    "        \n",
    "        # Обучение с комплексными параметрами\n",
    "        best_val_score = self._train_with_comprehensive_params(\n",
    "            trial, model, optimizer, scheduler, criterion, \n",
    "            augmentation_strength, augmentation_methods, batch_size, grad_clip\n",
    "        )\n",
    "        \n",
    "        return best_val_score\n",
    "    \n",
    "    def _create_simple_effective_model(self, hidden_size, dropout_rate, num_layers):\n",
    "        \"\"\"Создание SimpleEffectiveModel с оптимизированными параметрами\"\"\"\n",
    "        class DynamicSimpleEffectiveModel(nn.Module):\n",
    "            def __init__(self, input_size, output_size, hidden_size, dropout_rate, num_layers):\n",
    "                super().__init__()\n",
    "                \n",
    "                layers = []\n",
    "                current_size = input_size\n",
    "                \n",
    "                # Входной слой\n",
    "                layers.extend([\n",
    "                    nn.Linear(current_size, hidden_size),\n",
    "                    nn.BatchNorm1d(hidden_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout_rate)\n",
    "                ])\n",
    "                current_size = hidden_size\n",
    "                \n",
    "                # Скрытые слои\n",
    "                for i in range(num_layers - 2):\n",
    "                    next_size = max(hidden_size // (2 ** (i + 1)), 64)\n",
    "                    layers.extend([\n",
    "                        nn.Linear(current_size, next_size),\n",
    "                        nn.BatchNorm1d(next_size),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(dropout_rate * (0.8 ** (i + 1)))\n",
    "                    ])\n",
    "                    current_size = next_size\n",
    "                \n",
    "                # Выходной слой\n",
    "                layers.append(nn.Linear(current_size, output_size))\n",
    "                \n",
    "                self.net = nn.Sequential(*layers)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                return self.net(x)\n",
    "        \n",
    "        return DynamicSimpleEffectiveModel(\n",
    "            len(self.feature_cols), 424, hidden_size, dropout_rate, num_layers\n",
    "        ).to(device)\n",
    "    \n",
    "    def _create_residual_model(self, hidden_size, num_blocks, block_dropout):\n",
    "        \"\"\"Создание ResidualModel с оптимизированными параметрами\"\"\"\n",
    "        class DynamicResidualModel(nn.Module):\n",
    "            def __init__(self, input_size, output_size, hidden_size, num_blocks, block_dropout):\n",
    "                super().__init__()\n",
    "                \n",
    "                self.input_proj = nn.Linear(input_size, hidden_size)\n",
    "                self.blocks = nn.ModuleList()\n",
    "                \n",
    "                for _ in range(num_blocks):\n",
    "                    self.blocks.append(nn.Sequential(\n",
    "                        nn.Linear(hidden_size, hidden_size),\n",
    "                        nn.BatchNorm1d(hidden_size),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(block_dropout),\n",
    "                        nn.Linear(hidden_size, hidden_size),\n",
    "                        nn.BatchNorm1d(hidden_size)\n",
    "                    ))\n",
    "                \n",
    "                self.output = nn.Sequential(\n",
    "                    nn.Linear(hidden_size, hidden_size // 2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(block_dropout * 0.5),\n",
    "                    nn.Linear(hidden_size // 2, output_size)\n",
    "                )\n",
    "                self.dropout = nn.Dropout(block_dropout * 0.3)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                x = self.input_proj(x)\n",
    "                \n",
    "                for block in self.blocks:\n",
    "                    residual = x\n",
    "                    x = block(x)\n",
    "                    x = x + residual\n",
    "                    x = nn.ReLU()(x)\n",
    "                    x = self.dropout(x)\n",
    "                \n",
    "                return self.output(x)\n",
    "        \n",
    "        return DynamicResidualModel(\n",
    "            len(self.feature_cols), 424, hidden_size, num_blocks, block_dropout\n",
    "        ).to(device)\n",
    "    \n",
    "    def _create_wide_model(self, layer1_size, layer2_size, layer3_size, dropout1, dropout2, dropout3):\n",
    "        \"\"\"Создание WideModel с оптимизированными параметрами\"\"\"\n",
    "        class DynamicWideModel(nn.Module):\n",
    "            def __init__(self, input_size, output_size, layer1_size, layer2_size, layer3_size, dropout1, dropout2, dropout3):\n",
    "                super().__init__()\n",
    "                \n",
    "                self.net = nn.Sequential(\n",
    "                    nn.Linear(input_size, layer1_size),\n",
    "                    nn.BatchNorm1d(layer1_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout1),\n",
    "                    \n",
    "                    nn.Linear(layer1_size, layer2_size),\n",
    "                    nn.BatchNorm1d(layer2_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout2),\n",
    "                    \n",
    "                    nn.Linear(layer2_size, layer3_size),\n",
    "                    nn.BatchNorm1d(layer3_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout3),\n",
    "                    \n",
    "                    nn.Linear(layer3_size, output_size)\n",
    "                )\n",
    "            \n",
    "            def forward(self, x):\n",
    "                return self.net(x)\n",
    "        \n",
    "        return DynamicWideModel(\n",
    "            len(self.feature_cols), 424, layer1_size, layer2_size, layer3_size, dropout1, dropout2, dropout3\n",
    "        ).to(device)\n",
    "    \n",
    "    def _create_deep_skip_model(self, hidden_size, num_blocks, block_dropout):\n",
    "        \"\"\"Создание DeepSkipModel с оптимизированными параметрами\"\"\"\n",
    "        class DynamicDeepSkipModel(nn.Module):\n",
    "            def __init__(self, input_size, output_size, hidden_size, num_blocks, block_dropout):\n",
    "                super().__init__()\n",
    "                \n",
    "                self.input_proj = nn.Linear(input_size, hidden_size)\n",
    "                self.blocks = nn.ModuleList()\n",
    "                \n",
    "                for i in range(num_blocks):\n",
    "                    self.blocks.append(nn.Sequential(\n",
    "                        nn.Linear(hidden_size, hidden_size),\n",
    "                        nn.BatchNorm1d(hidden_size),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(block_dropout),\n",
    "                        nn.Linear(hidden_size, hidden_size),\n",
    "                        nn.BatchNorm1d(hidden_size)\n",
    "                    ))\n",
    "                \n",
    "                self.output = nn.Linear(hidden_size, output_size)\n",
    "                self.dropout = nn.Dropout(block_dropout * 0.7)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                x = self.input_proj(x)\n",
    "                \n",
    "                for block in self.blocks:\n",
    "                    residual = x\n",
    "                    x = block(x)\n",
    "                    x = x + residual\n",
    "                    x = nn.ReLU()(x)\n",
    "                    x = self.dropout(x)\n",
    "                \n",
    "                return self.output(x)\n",
    "        \n",
    "        return DynamicDeepSkipModel(\n",
    "            len(self.feature_cols), 424, hidden_size, num_blocks, block_dropout\n",
    "        ).to(device)\n",
    "    \n",
    "    def _create_attention_model(self, hidden_size, num_heads, ffn_multiplier, attn_dropout):\n",
    "        \"\"\"Создание AttentionModel с оптимизированными параметрами\"\"\"\n",
    "        class DynamicAttentionModel(nn.Module):\n",
    "            def __init__(self, input_size, output_size, hidden_size, num_heads, ffn_multiplier, attn_dropout):\n",
    "                super().__init__()\n",
    "                \n",
    "                self.input_proj = nn.Linear(input_size, hidden_size)\n",
    "                \n",
    "                self.attention = nn.MultiheadAttention(\n",
    "                    embed_dim=hidden_size,\n",
    "                    num_heads=num_heads,\n",
    "                    dropout=attn_dropout,\n",
    "                    batch_first=True\n",
    "                )\n",
    "                \n",
    "                ffn_hidden = int(hidden_size * ffn_multiplier)\n",
    "                self.ffn = nn.Sequential(\n",
    "                    nn.Linear(hidden_size, ffn_hidden),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(attn_dropout * 2),\n",
    "                    nn.Linear(ffn_hidden, hidden_size)\n",
    "                )\n",
    "                \n",
    "                self.norm1 = nn.LayerNorm(hidden_size)\n",
    "                self.norm2 = nn.LayerNorm(hidden_size)\n",
    "                self.output = nn.Linear(hidden_size, output_size)\n",
    "                self.dropout = nn.Dropout(attn_dropout)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                x = self.input_proj(x)\n",
    "                residual1 = x\n",
    "                \n",
    "                # Attention\n",
    "                x_seq = x.unsqueeze(1)\n",
    "                attn_out, _ = self.attention(x_seq, x_seq, x_seq)\n",
    "                x = residual1 + attn_out.squeeze(1)\n",
    "                x = self.norm1(x)\n",
    "                x = self.dropout(x)\n",
    "                \n",
    "                # FFN\n",
    "                residual2 = x\n",
    "                ffn_out = self.ffn(x)\n",
    "                x = residual2 + ffn_out\n",
    "                x = self.norm2(x)\n",
    "                x = self.dropout(x)\n",
    "                \n",
    "                return self.output(x)\n",
    "        \n",
    "        return DynamicAttentionModel(\n",
    "            len(self.feature_cols), 424, hidden_size, num_heads, ffn_multiplier, attn_dropout\n",
    "        ).to(device)\n",
    "    \n",
    "    def _create_ensemble_model(self, num_experts, expert_size, gate_size, expert_dropout):\n",
    "        \"\"\"Создание EnsembleInsideModel с оптимизированными параметрами\"\"\"\n",
    "        class DynamicEnsembleModel(nn.Module):\n",
    "            def __init__(self, input_size, output_size, num_experts, expert_size, gate_size, expert_dropout):\n",
    "                super().__init__()\n",
    "                \n",
    "                self.experts = nn.ModuleList([\n",
    "                    nn.Sequential(\n",
    "                        nn.Linear(input_size, expert_size),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(expert_dropout),\n",
    "                        nn.Linear(expert_size, expert_size // 2)\n",
    "                    ) for _ in range(num_experts)\n",
    "                ])\n",
    "                \n",
    "                self.gate = nn.Sequential(\n",
    "                    nn.Linear(input_size, gate_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(gate_size, num_experts),\n",
    "                    nn.Softmax(dim=1)\n",
    "                )\n",
    "                \n",
    "                self.output = nn.Sequential(\n",
    "                    nn.Linear(expert_size // 2, expert_size // 4),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(expert_dropout * 0.7),\n",
    "                    nn.Linear(expert_size // 4, output_size)\n",
    "                )\n",
    "            \n",
    "            def forward(self, x):\n",
    "                gates = self.gate(x)\n",
    "                expert_outputs = [expert(x) for expert in self.experts]\n",
    "                \n",
    "                combined = torch.zeros_like(expert_outputs[0])\n",
    "                for i, expert_out in enumerate(expert_outputs):\n",
    "                    combined += gates[:, i:i+1] * expert_out\n",
    "                \n",
    "                return self.output(combined)\n",
    "        \n",
    "        return DynamicEnsembleModel(\n",
    "            len(self.feature_cols), 424, num_experts, expert_size, gate_size, expert_dropout\n",
    "        ).to(device)\n",
    "    \n",
    "    def _train_with_comprehensive_params(self, trial, model, optimizer, scheduler, criterion, \n",
    "                                       augmentation_strength, augmentation_methods, batch_size, grad_clip):\n",
    "        \"\"\"Обучение с комплексными параметрами\"\"\"\n",
    "        \n",
    "        # DataLoader с батчингом\n",
    "        dataset = TensorDataset(self.X_train_t, self.y_train_t)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        best_val_score = -float('inf')\n",
    "        patience = 20\n",
    "        epochs_no_improve = 0\n",
    "        best_epoch = 0\n",
    "        \n",
    "        for epoch in range(150):  # Увеличенное количество эпох\n",
    "            # Training\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch_X, batch_y in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Аугментация с оптимизированными методами\n",
    "                if augmentation_strength > 0 and augmentation_methods:\n",
    "                    batch_X = advanced_augmentation(\n",
    "                        batch_X, \n",
    "                        augmentation_methods=augmentation_methods,\n",
    "                        augmentation_prob=augmentation_strength\n",
    "                    )\n",
    "                \n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(self.X_val_t)\n",
    "                val_score = calculate_spearman_score(\n",
    "                    val_outputs.cpu().numpy(), \n",
    "                    self.y_val_t.cpu().numpy()\n",
    "                )\n",
    "            \n",
    "            # Scheduler step\n",
    "            if scheduler:\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(epoch_loss / num_batches)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_score > best_val_score:\n",
    "                best_val_score = val_score\n",
    "                epochs_no_improve = 0\n",
    "                best_epoch = epoch\n",
    "                # Сохраняем лучшие веса\n",
    "                best_weights = model.state_dict().copy()\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "            \n",
    "            # Report intermediate value for pruning\n",
    "            trial.report(val_score, epoch)\n",
    "            \n",
    "            # Prune trial if not promising\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "            if epochs_no_improve >= patience:\n",
    "                break\n",
    "        \n",
    "        # Загружаем лучшие веса\n",
    "        if 'best_weights' in locals():\n",
    "            model.load_state_dict(best_weights)\n",
    "        \n",
    "        # Очистка памяти\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return best_val_score\n",
    "    \n",
    "    def optimize_comprehensive(self, n_trials=200, timeout=None):\n",
    "        \"\"\"Запуск комплексной оптимизации\"\"\"\n",
    "        \n",
    "        print(\"🚀 ЗАПУСК КОМПЛЕКСНОЙ OPTUNA ОПТИМИЗАЦИИ...\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Количество trials: {n_trials}\")\n",
    "        print(f\"Таймаут: {timeout} секунд\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42),\n",
    "            pruner=optuna.pruners.HyperbandPruner()\n",
    "        )\n",
    "        \n",
    "        study.optimize(\n",
    "            self.objective_comprehensive, \n",
    "            n_trials=n_trials,\n",
    "            timeout=timeout,\n",
    "            gc_after_trial=True\n",
    "        )\n",
    "        \n",
    "        self.studies['comprehensive'] = study\n",
    "        self.best_params = study.best_params\n",
    "        \n",
    "        print(\"🎯 КОМПЛЕКСНАЯ ОПТИМИЗАЦИЯ ЗАВЕРШЕНА!\")\n",
    "        print(f\"Лучший score: {study.best_value:.4f}\")\n",
    "        print(f\"Лучшие параметры: {study.best_params}\")\n",
    "        \n",
    "        return study\n",
    "\n",
    "# Функция для создания лучшей модели на основе оптимизированных параметров\n",
    "def create_best_comprehensive_model(optuna_optimizer):\n",
    "    \"\"\"Создает лучшую модель на основе комплексной оптимизации\"\"\"\n",
    "    \n",
    "    best_params = optuna_optimizer.best_params\n",
    "    model_type = best_params['model_type']\n",
    "    \n",
    "    print(f\"🎯 СОЗДАНИЕ ЛУЧШЕЙ МОДЕЛИ: {model_type}\")\n",
    "    \n",
    "    if model_type == 'simple_effective':\n",
    "        model = optuna_optimizer._create_simple_effective_model(\n",
    "            best_params['hidden_size'],\n",
    "            best_params['dropout_rate'],\n",
    "            best_params['num_layers']\n",
    "        )\n",
    "        \n",
    "    elif model_type == 'residual':\n",
    "        model = optuna_optimizer._create_residual_model(\n",
    "            best_params['res_hidden_size'],\n",
    "            best_params['num_blocks'],\n",
    "            best_params['block_dropout']\n",
    "        )\n",
    "        \n",
    "    elif model_type == 'wide':\n",
    "        model = optuna_optimizer._create_wide_model(\n",
    "            best_params['layer1_size'],\n",
    "            best_params['layer2_size'],\n",
    "            best_params['layer3_size'],\n",
    "            best_params['dropout1'],\n",
    "            best_params['dropout2'],\n",
    "            best_params['dropout3']\n",
    "        )\n",
    "        \n",
    "    elif model_type == 'deep_skip':\n",
    "        model = optuna_optimizer._create_deep_skip_model(\n",
    "            best_params['deep_hidden_size'],\n",
    "            best_params['deep_num_blocks'],\n",
    "            best_params['deep_dropout']\n",
    "        )\n",
    "        \n",
    "    elif model_type == 'attention':\n",
    "        model = optuna_optimizer._create_attention_model(\n",
    "            best_params['attn_hidden_size'],\n",
    "            best_params['num_heads'],\n",
    "            best_params['ffn_multiplier'],\n",
    "            best_params['attn_dropout']\n",
    "        )\n",
    "        \n",
    "    elif model_type == 'ensemble_inside':\n",
    "        model = optuna_optimizer._create_ensemble_model(\n",
    "            best_params['num_experts'],\n",
    "            best_params['expert_size'],\n",
    "            best_params['gate_size'],\n",
    "            best_params['expert_dropout']\n",
    "        )\n",
    "    \n",
    "    return model, best_params\n",
    "\n",
    "# Функция для визуализации результатов Optuna\n",
    "def visualize_optuna_results(studies):\n",
    "    \"\"\"Визуализация результатов Optuna\"\"\"\n",
    "    \n",
    "    for study_name, study in studies.items():\n",
    "        print(f\"\\n📊 ВИЗУАЛИЗАЦИЯ РЕЗУЛЬТАТОВ: {study_name}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # История оптимизации\n",
    "        try:\n",
    "            fig = optuna.visualization.plot_optimization_history(study)\n",
    "            fig.show()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Не удалось создать историю оптимизации: {e}\")\n",
    "        \n",
    "        # Важность параметров\n",
    "        try:\n",
    "            fig = optuna.visualization.plot_param_importances(study)\n",
    "            fig.show()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Не удалось создать график важности параметров: {e}\")\n",
    "        \n",
    "        # Параллельные координаты\n",
    "        try:\n",
    "            fig = optuna.visualization.plot_parallel_coordinate(study)\n",
    "            fig.show()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Не удалось создать параллельные координаты: {e}\")\n",
    "        \n",
    "        # Распределение параметров\n",
    "        try:\n",
    "            fig = optuna.visualization.plot_contour(study)\n",
    "            fig.show()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Не удалось создать контурный график: {e}\")\n",
    "\n",
    "# Обновленная функция инициализации с комплексной оптимизацией\n",
    "def initialize_models_with_comprehensive_optuna(n_trials=200, timeout=None):\n",
    "    global models, scaler, feature_cols, base_cols, model_val_scores, is_initialized, device\n",
    "    \n",
    "    if is_initialized:\n",
    "        return\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"КОМПЛЕКСНАЯ OPTUNA ОПТИМИЗАЦИЯ НА {device}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Загрузка данных\n",
    "    train = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/train.csv')\n",
    "    train_labels = pd.read_csv('/home/nicolaedrabcinski/sd_kaggle/data/raw/train_labels.csv')\n",
    "    \n",
    "    # Базовые колонки\n",
    "    base_cols = [c for c in train.columns \n",
    "                 if c not in ['date_id'] \n",
    "                 and pd.api.types.is_numeric_dtype(train[c])]\n",
    "    \n",
    "    print(f\"Базовых колонок: {len(base_cols)}\")\n",
    "    print(\"Создание оптимизированных признаков...\")\n",
    "    \n",
    "    # Создание признаков\n",
    "    train_features = create_optimized_features(train, base_cols_ref=base_cols, max_features=800)\n",
    "    \n",
    "    # Выбор лучших признаков\n",
    "    target_cols = [f'target_{i}' for i in range(424)]\n",
    "    selected_features = select_best_features_vectorized_fixed(train_features, train_labels[target_cols], n_features=500)\n",
    "    \n",
    "    feature_cols = selected_features\n",
    "    \n",
    "    # Подготовка данных\n",
    "    X_train = train_features[feature_cols].fillna(0).values\n",
    "    y_train = train_labels[target_cols].fillna(0).values\n",
    "    \n",
    "    X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    y_train = np.nan_to_num(y_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Масштабирование\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Перенос на GPU\n",
    "    X_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "    y_tensor = torch.FloatTensor(y_train).to(device)\n",
    "    \n",
    "    # Разделение на train/validation\n",
    "    split_idx = int(len(X_train_scaled) * 0.85)\n",
    "    X_train_t, X_val_t = X_tensor[:split_idx], X_tensor[split_idx:]\n",
    "    y_train_t, y_val_t = y_tensor[:split_idx], y_tensor[split_idx:]\n",
    "    \n",
    "    print(f\"Train: {len(X_train_t)}, Validation: {len(X_val_t)}\")\n",
    "    \n",
    "    # Комплексная оптимизация Optuna\n",
    "    optuna_optimizer = ComprehensiveOptunaOptimizer(\n",
    "        X_train_t, y_train_t, X_val_t, y_val_t, feature_cols\n",
    "    )\n",
    "    \n",
    "    study = optuna_optimizer.optimize_comprehensive(\n",
    "        n_trials=n_trials, \n",
    "        timeout=timeout\n",
    "    )\n",
    "    \n",
    "    # Создание и обучение лучшей модели\n",
    "    best_model, best_params = create_best_comprehensive_model(optuna_optimizer)\n",
    "    \n",
    "    # Извлекаем оптимизированные параметры для обучения\n",
    "    lr = best_params['lr']\n",
    "    augmentation_strength = best_params['augmentation_strength']\n",
    "    \n",
    "    # Конфигурация аугментации\n",
    "    augmentation_methods = []\n",
    "    if best_params.get('use_noise', False):\n",
    "        augmentation_methods.append(('noise', best_params.get('noise_prob', 0.3)))\n",
    "    if best_params.get('use_scaling', False):\n",
    "        augmentation_methods.append(('scaling', best_params.get('scaling_prob', 0.2)))\n",
    "    if best_params.get('use_jitter', False):\n",
    "        augmentation_methods.append(('jitter', best_params.get('jitter_prob', 0.2)))\n",
    "    if best_params.get('use_dropout_aug', False):\n",
    "        augmentation_methods.append(('dropout', best_params.get('dropout_aug_prob', 0.1)))\n",
    "    if best_params.get('use_permutation', False):\n",
    "        augmentation_methods.append(('permutation', best_params.get('permutation_prob', 0.1)))\n",
    "    \n",
    "    print(f\"\\n🎯 ОБУЧЕНИЕ ЛУЧШЕЙ МОДЕЛИ С ОПТИМИЗИРОВАННЫМИ ПАРАМЕТРАМИ:\")\n",
    "    print(f\"   Архитектура: {best_params['model_type']}\")\n",
    "    print(f\"   Learning rate: {lr}\")\n",
    "    print(f\"   Augmentation strength: {augmentation_strength}\")\n",
    "    print(f\"   Augmentation methods: {[m[0] for m in augmentation_methods]}\")\n",
    "    \n",
    "    # Обучение лучшей модели\n",
    "    best_model, val_score = train_model_with_augmentation(\n",
    "        best_model, X_train_t, y_train_t, X_val_t, y_val_t,\n",
    "        epochs=300,  # Увеличиваем эпохи для лучшей модели\n",
    "        patience=25,\n",
    "        lr=lr,\n",
    "        model_name=f\"BEST_{best_params['model_type']}\",\n",
    "        use_augmentation=True,\n",
    "        augmentation_strength=augmentation_strength\n",
    "    )\n",
    "    \n",
    "    models.append(best_model)\n",
    "    model_val_scores.append(val_score)\n",
    "    \n",
    "    print(f\"✅ ЛУЧШАЯ МОДЕЛЬ завершена. Best Val Score: {val_score:.4f}\")\n",
    "    \n",
    "    # Визуализация результатов\n",
    "    visualize_optuna_results(optuna_optimizer.studies)\n",
    "    \n",
    "    is_initialized = True\n",
    "    \n",
    "    # Сохранение scaler и feature_cols\n",
    "    joblib.dump(scaler, 'scaler_optimized.joblib')\n",
    "    joblib.dump(feature_cols, 'feature_cols_optimized.joblib')\n",
    "    joblib.dump(best_params, 'best_params.joblib')\n",
    "    print(\"Scaler, feature_cols и best_params сохранены\")\n",
    "\n",
    "# Новая main функция с комплексной оптимизацией\n",
    "def main_with_comprehensive_optuna(n_trials=200, timeout=None):\n",
    "    global is_initialized\n",
    "    \n",
    "    print(\"🚀 ЗАПУСК ПАЙПЛАЙНА С КОМПЛЕКСНОЙ OPTUNA ОПТИМИЗАЦИЕЙ\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Количество trials: {n_trials}\")\n",
    "    print(f\"Таймаут: {timeout} секунд\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not is_initialized:\n",
    "        print(\"Начало комплексной оптимизации...\")\n",
    "        initialize_models_with_comprehensive_optuna(\n",
    "            n_trials=n_trials, \n",
    "            timeout=timeout\n",
    "        )\n",
    "    \n",
    "    # Оценка\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ОЦЕНКА ОПТИМИЗИРОВАННОЙ МОДЕЛИ\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = full_evaluation()\n",
    "    \n",
    "    # Создание submission\n",
    "    print(\"\\nСоздание submission файла...\")\n",
    "    create_submission_file()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = (end_time - start_time) / 3600  # в часах\n",
    "    \n",
    "    print(\"\\n✅ КОМПЛЕКСНАЯ OPTUNA ОПТИМИЗАЦИЯ ЗАВЕРШЕНА!\")\n",
    "    print(f\"🏆 Ваш ожидаемый Kaggle Score: {results['validation']['kaggle_score']:.2f}\")\n",
    "    print(f\"📊 Sharpe Ratio: {results['validation']['sharpe_ratio']:.6f}\")\n",
    "    print(f\"🔢 Количество моделей: {len(models)}\")\n",
    "    print(f\"⏱️ Общее время выполнения: {total_time:.2f} часов\")\n",
    "    print(f\"🎯 Использована комплексная Optuna оптимизация\")\n",
    "    print(f\"📁 Submission файл: submission.parquet\")\n",
    "    print(f\"\\n🚀 ОТПРАВЛЯЙТЕ НА KAGGLE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1551b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_augmentation(features, augmentation_methods=None, augmentation_prob=0.7):\n",
    "    \"\"\"Продвинутая аугментация с кастомными методами\"\"\"\n",
    "    if augmentation_methods is None:\n",
    "        augmentation_methods = [\n",
    "            ('noise', 0.3),\n",
    "            ('scaling', 0.2), \n",
    "            ('jitter', 0.2),\n",
    "            ('dropout', 0.2),\n",
    "            ('permutation', 0.1)\n",
    "        ]\n",
    "    \n",
    "    augmented_features = features.clone()\n",
    "    \n",
    "    # Применяем аугментации с вероятностью\n",
    "    if torch.rand(1).item() < augmentation_prob:\n",
    "        for method, prob in augmentation_methods:\n",
    "            if torch.rand(1).item() < prob:\n",
    "                if method == 'noise':\n",
    "                    augmented_features = TimeSeriesAugmentation.add_gaussian_noise(augmented_features, 0.01)\n",
    "                elif method == 'scaling':\n",
    "                    augmented_features = TimeSeriesAugmentation.scaling_augmentation(augmented_features)\n",
    "                elif method == 'jitter':\n",
    "                    augmented_features = TimeSeriesAugmentation.jittering(augmented_features)\n",
    "                elif method == 'dropout':\n",
    "                    augmented_features = TimeSeriesAugmentation.feature_dropout(augmented_features, 0.05)\n",
    "                elif method == 'permutation':\n",
    "                    augmented_features = TimeSeriesAugmentation.random_permutation(augmented_features, 0.05)\n",
    "    \n",
    "    return augmented_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83053765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 ЗАПУСК ПАЙПЛАЙНА С КОМПЛЕКСНОЙ OPTUNA ОПТИМИЗАЦИЕЙ\n",
      "======================================================================\n",
      "Количество trials: 500\n",
      "Таймаут: 172800 секунд\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ОЦЕНКА ОПТИМИЗИРОВАННОЙ МОДЕЛИ\n",
      "======================================================================\n",
      "\n",
      "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
      "ПОЛНАЯ ОЦЕНКА МОДЕЛИ\n",
      "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
      "Обрабатывается 100 базовых колонок...\n",
      "Достигнут лимит в 800 признаков\n",
      "Создано 1507 признаков после фильтрации\n",
      "\n",
      "1️⃣  VALIDATION SCORE:\n",
      "Validation set size: 295 samples\n",
      "\n",
      "======================================================================\n",
      "ВЫЧИСЛЕНИЕ KAGGLE SCORE (Modified Sharpe Ratio)\n",
      "======================================================================\n",
      "\n",
      "Статистика Spearman Correlations:\n",
      "  Успешно: 424/424 targets\n",
      "  Mean:    0.057219\n",
      "  Std:     0.058726\n",
      "  Min:     -0.118412\n",
      "  Max:     0.232985\n",
      "\n",
      "  Positive correlations: 359/424 (84.7%)\n",
      "  Negative correlations: 65/424 (15.3%)\n",
      "\n",
      "======================================================================\n",
      "KAGGLE SCORE (Modified Sharpe Ratio):\n",
      "======================================================================\n",
      "  Sharpe Ratio:        0.974347\n",
      "  KAGGLE SCORE:        97434.67\n",
      "\n",
      "  ✓ Хороший результат (> 50,000)\n",
      "======================================================================\n",
      "\n",
      "\n",
      "2️⃣  TEST SCORE:\n",
      "Обрабатывается 100 базовых колонок...\n",
      "Достигнут лимит в 800 признаков\n",
      "Создано 1502 признаков после фильтрации\n",
      "\n",
      "======================================================================\n",
      "ВЫЧИСЛЕНИЕ KAGGLE SCORE (Modified Sharpe Ratio)\n",
      "======================================================================\n",
      "\n",
      "Статистика Spearman Correlations:\n",
      "  Успешно: 424/424 targets\n",
      "  Mean:    0.017133\n",
      "  Std:     0.094447\n",
      "  Min:     -0.244093\n",
      "  Max:     0.298635\n",
      "\n",
      "  Positive correlations: 248/424 (58.5%)\n",
      "  Negative correlations: 176/424 (41.5%)\n",
      "\n",
      "======================================================================\n",
      "KAGGLE SCORE (Modified Sharpe Ratio):\n",
      "======================================================================\n",
      "  Sharpe Ratio:        0.181401\n",
      "  KAGGLE SCORE:        18140.06\n",
      "\n",
      "  ⚠ Слабый результат (> 0, но < 50,000)\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "📊 СРАВНЕНИЕ РЕЗУЛЬТАТОВ:\n",
      "======================================================================\n",
      "Метрика                        Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "KAGGLE SCORE                   97434.67             18140.06            \n",
      "Sharpe Ratio                   0.974347             0.181401            \n",
      "Mean Correlation               0.057219             0.017133            \n",
      "======================================================================\n",
      "✅ ХОРОШО: Модель показывает положительную корреляцию!\n",
      "\n",
      "🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯🎯\n",
      "\n",
      "\n",
      "Создание submission файла...\n",
      "\n",
      "Создание submission.parquet...\n",
      "Генерация предсказаний...\n",
      "Обрабатывается 100 базовых колонок...\n",
      "Достигнут лимит в 800 признаков\n",
      "Создано 1502 признаков после фильтрации\n",
      "Готово: (90, 425)\n",
      "\n",
      "✅ КОМПЛЕКСНАЯ OPTUNA ОПТИМИЗАЦИЯ ЗАВЕРШЕНА!\n",
      "🏆 Ваш ожидаемый Kaggle Score: 97434.67\n",
      "📊 Sharpe Ratio: 0.974347\n",
      "🔢 Количество моделей: 6\n",
      "⏱️ Общее время выполнения: 0.00 часов\n",
      "🎯 Использована комплексная Optuna оптимизация\n",
      "📁 Submission файл: submission.parquet\n",
      "\n",
      "🚀 ОТПРАВЛЯЙТЕ НА KAGGLE!\n"
     ]
    }
   ],
   "source": [
    "# Добавьте этот импорт в начало\n",
    "import time\n",
    "\n",
    "# Запуск комплексной оптимизации\n",
    "if __name__ == \"__main__\":\n",
    "    # Для серьезной оптимизации (рекомендуется если есть время)\n",
    "    # n_trials=200-500, timeout=24-48 часов\n",
    "    \n",
    "    main_with_comprehensive_optuna(\n",
    "        n_trials=500,           # Количество trials\n",
    "        timeout=48*3600         # 48 часов таймаут\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a90817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd_kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
